-- t=000 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.1000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.1000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.1000) => 0.0000
activation[4] = self(0.0000) * weight[4,4](0.1000) => 0.0000
activation[5] = self(0.0000) * weight[5,5](0.1000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.1000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.1000) => 0.0000
activation[8] = self(0.0000) * weight[8,8](0.1000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.1000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(0.0000) + output[1](0.0000) * 1.0000 => 0.0000
activation[3] = self(0.0000) + output[7](0.0000) * -0.5000 => 0.0000
activation[3] = self(0.0000) + output[8](0.0000) * -0.5000 => 0.0000
activation[4] = self(0.0000) + output[1](0.0000) * 0.5000 => 0.0000
activation[4] = self(0.0000) + output[2](0.0000) * 0.5000 => 0.0000
activation[4] = self(0.0000) + output[6](0.0000) * -0.5000 => 0.0000
activation[4] = self(0.0000) + output[8](0.0000) * -0.5000 => 0.0000
activation[5] = self(0.0000) + output[2](0.0000) * 1.0000 => 0.0000
activation[5] = self(0.0000) + output[6](0.0000) * -0.5000 => 0.0000
activation[5] = self(0.0000) + output[7](0.0000) * -0.5000 => 0.0000
activation[6] = self(0.0000) + output[3](0.0000) * 1.0000 => 0.0000
activation[7] = self(0.0000) + output[4](0.0000) * 1.0000 => 0.0000
activation[8] = self(0.0000) + output[5](0.0000) * 1.0000 => 0.0000
activation[9] = self(0.0000) + output[3](0.0000) * 0.7500 => 0.0000
activation[9] = self(0.0000) + output[4](0.0000) * 0.2500 => 0.0000
activation[10] = self(0.0000) + output[4](0.0000) * 0.2500 => 0.0000
activation[10] = self(0.0000) + output[5](0.0000) * 0.7500 => 0.0000
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0000)
networkOutput[2] := neuronOutput[10](0.0000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.0000) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.0000) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.0000) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.0000) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=001 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.1000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.1000) => 0.0000
activation[3] = self(0.0000) * weight[3,3](0.1000) => 0.0000
activation[4] = self(0.0000) * weight[4,4](0.1000) => 0.0000
activation[5] = self(0.0000) * weight[5,5](0.1000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.1000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.1000) => 0.0000
activation[8] = self(0.0000) * weight[8,8](0.1000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.1000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(0.0000) + output[1](0.0000) * 1.0000 => 0.0000
activation[3] = self(0.0000) + output[7](0.0000) * -0.5000 => 0.0000
activation[3] = self(0.0000) + output[8](0.0000) * -0.5000 => 0.0000
activation[4] = self(0.0000) + output[1](0.0000) * 0.5000 => 0.0000
activation[4] = self(0.0000) + output[2](0.0000) * 0.5000 => 0.0000
activation[4] = self(0.0000) + output[6](0.0000) * -0.5000 => 0.0000
activation[4] = self(0.0000) + output[8](0.0000) * -0.5000 => 0.0000
activation[5] = self(0.0000) + output[2](0.0000) * 1.0000 => 0.0000
activation[5] = self(0.0000) + output[6](0.0000) * -0.5000 => 0.0000
activation[5] = self(0.0000) + output[7](0.0000) * -0.5000 => 0.0000
activation[6] = self(0.0000) + output[3](0.0000) * 1.0000 => 0.0000
activation[7] = self(0.0000) + output[4](0.0000) * 1.0000 => 0.0000
activation[8] = self(0.0000) + output[5](0.0000) * 1.0000 => 0.0000
activation[9] = self(0.0000) + output[3](0.0000) * 0.7500 => 0.0000
activation[9] = self(0.0000) + output[4](0.0000) * 0.2500 => 0.0000
activation[10] = self(0.0000) + output[4](0.0000) * 0.2500 => 0.0000
activation[10] = self(0.0000) + output[5](0.0000) * 0.7500 => 0.0000
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0000)
networkOutput[2] := neuronOutput[10](0.0000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.0000) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.0000) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.0000) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.0000) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=002 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.1000) => 0.1000
activation[2] = self(1.0000) * weight[2,2](0.1000) => 0.1000
activation[3] = self(0.0000) * weight[3,3](0.1000) => 0.0000
activation[4] = self(0.0000) * weight[4,4](0.1000) => 0.0000
activation[5] = self(0.0000) * weight[5,5](0.1000) => 0.0000
activation[6] = self(0.0000) * weight[6,6](0.1000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.1000) => 0.0000
activation[8] = self(0.0000) * weight[8,8](0.1000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.1000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.1000) + network_input[1]=0.0000 => 0.1000
activation[2] = self(0.1000) + network_input[2]=0.0000 => 0.1000
activation[3] = self(0.0000) + output[1](1.0000) * 1.0000 => 1.0000
activation[3] = self(1.0000) + output[7](0.0000) * -0.5000 => 1.0000
activation[3] = self(1.0000) + output[8](0.0000) * -0.5000 => 1.0000
activation[4] = self(0.0000) + output[1](1.0000) * 0.5000 => 0.5000
activation[4] = self(0.5000) + output[2](1.0000) * 0.5000 => 1.0000
activation[4] = self(1.0000) + output[6](0.0000) * -0.5000 => 1.0000
activation[4] = self(1.0000) + output[8](0.0000) * -0.5000 => 1.0000
activation[5] = self(0.0000) + output[2](1.0000) * 1.0000 => 1.0000
activation[5] = self(1.0000) + output[6](0.0000) * -0.5000 => 1.0000
activation[5] = self(1.0000) + output[7](0.0000) * -0.5000 => 1.0000
activation[6] = self(0.0000) + output[3](0.0000) * 1.0000 => 0.0000
activation[7] = self(0.0000) + output[4](0.0000) * 1.0000 => 0.0000
activation[8] = self(0.0000) + output[5](0.0000) * 1.0000 => 0.0000
activation[9] = self(0.0000) + output[3](0.0000) * 0.7500 => 0.0000
activation[9] = self(0.0000) + output[4](0.0000) * 0.2500 => 0.0000
activation[10] = self(0.0000) + output[4](0.0000) * 0.2500 => 0.0000
activation[10] = self(0.0000) + output[5](0.0000) * 0.7500 => 0.0000
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.1000) > threshold(0.0000)? ==> 0.1000
neuronOutput[2]: activation(0.1000) > threshold(0.0000)? ==> 0.1000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[5]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0000)
networkOutput[2] := neuronOutput[10](0.0000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.1000) * output[3](1.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](1.0000) * output[3](1.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.1000) * output[4](1.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.1000) * output[4](1.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](1.0000) * output[4](1.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.1000) * output[5](1.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](1.0000) * output[5](1.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=003 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.1000) * weight[1,1](0.1000) => 0.0100
activation[2] = self(0.1000) * weight[2,2](0.1000) => 0.0100
activation[3] = self(1.0000) * weight[3,3](0.1000) => 0.1000
activation[4] = self(1.0000) * weight[4,4](0.1000) => 0.1000
activation[5] = self(1.0000) * weight[5,5](0.1000) => 0.1000
activation[6] = self(0.0000) * weight[6,6](0.1000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.1000) => 0.0000
activation[8] = self(0.0000) * weight[8,8](0.1000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.1000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0100) + network_input[1]=1.0000 => 1.0100
activation[2] = self(0.0100) + network_input[2]=1.0000 => 1.0100
activation[3] = self(0.1000) + output[1](0.1000) * 1.0000 => 0.2000
activation[3] = self(0.2000) + output[7](0.0000) * -0.5000 => 0.2000
activation[3] = self(0.2000) + output[8](0.0000) * -0.5000 => 0.2000
activation[4] = self(0.1000) + output[1](0.1000) * 0.5000 => 0.1500
activation[4] = self(0.1500) + output[2](0.1000) * 0.5000 => 0.2000
activation[4] = self(0.2000) + output[6](0.0000) * -0.5000 => 0.2000
activation[4] = self(0.2000) + output[8](0.0000) * -0.5000 => 0.2000
activation[5] = self(0.1000) + output[2](0.1000) * 1.0000 => 0.2000
activation[5] = self(0.2000) + output[6](0.0000) * -0.5000 => 0.2000
activation[5] = self(0.2000) + output[7](0.0000) * -0.5000 => 0.2000
activation[6] = self(0.0000) + output[3](1.0000) * 1.0000 => 1.0000
activation[7] = self(0.0000) + output[4](1.0000) * 1.0000 => 1.0000
activation[8] = self(0.0000) + output[5](1.0000) * 1.0000 => 1.0000
activation[9] = self(0.0000) + output[3](1.0000) * 0.7500 => 0.7500
activation[9] = self(0.7500) + output[4](1.0000) * 0.2500 => 1.0000
activation[10] = self(0.0000) + output[4](1.0000) * 0.2500 => 0.2500
activation[10] = self(0.2500) + output[5](1.0000) * 0.7500 => 1.0000
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0100) > threshold(0.0000)? ==> 1.0100
neuronOutput[2]: activation(1.0100) > threshold(0.0000)? ==> 1.0100
neuronOutput[3]: activation(0.2000) > threshold(0.0000)? ==> 0.2000
neuronOutput[4]: activation(0.2000) > threshold(0.0000)? ==> 0.2000
neuronOutput[5]: activation(0.2000) > threshold(0.0000)? ==> 0.2000
neuronOutput[6]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[7]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[8]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[9]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[10]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](1.0000)
networkOutput[2] := neuronOutput[10](1.0000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.0100) * output[3](0.2000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.2000) * output[3](0.2000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.0100) * output[4](0.2000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.0100) * output[4](0.2000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.2000) * output[4](0.2000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.0100) * output[5](0.2000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.2000) * output[5](0.2000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=004 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0100) * weight[1,1](0.1000) => 0.1010
activation[2] = self(1.0100) * weight[2,2](0.1000) => 0.1010
activation[3] = self(0.2000) * weight[3,3](0.1000) => 0.0200
activation[4] = self(0.2000) * weight[4,4](0.1000) => 0.0200
activation[5] = self(0.2000) * weight[5,5](0.1000) => 0.0200
activation[6] = self(1.0000) * weight[6,6](0.1000) => 0.1000
activation[7] = self(1.0000) * weight[7,7](0.1000) => 0.1000
activation[8] = self(1.0000) * weight[8,8](0.1000) => 0.1000
activation[9] = self(1.0000) * weight[9,9](0.1000) => 0.1000
activation[10] = self(1.0000) * weight[10,10](0.1000) => 0.1000
  b. Update inputs from other neurons
activation[1] = self(0.1010) + network_input[1]=0.0000 => 0.1010
activation[2] = self(0.1010) + network_input[2]=0.0000 => 0.1010
activation[3] = self(0.0200) + output[1](1.0100) * 1.0000 => 1.0300
activation[3] = self(1.0300) + output[7](1.0000) * -0.5000 => 0.5300
activation[3] = self(0.5300) + output[8](1.0000) * -0.5000 => 0.0300
activation[4] = self(0.0200) + output[1](1.0100) * 0.5000 => 0.5250
activation[4] = self(0.5250) + output[2](1.0100) * 0.5000 => 1.0300
activation[4] = self(1.0300) + output[6](1.0000) * -0.5000 => 0.5300
activation[4] = self(0.5300) + output[8](1.0000) * -0.5000 => 0.0300
activation[5] = self(0.0200) + output[2](1.0100) * 1.0000 => 1.0300
activation[5] = self(1.0300) + output[6](1.0000) * -0.5000 => 0.5300
activation[5] = self(0.5300) + output[7](1.0000) * -0.5000 => 0.0300
activation[6] = self(0.1000) + output[3](0.2000) * 1.0000 => 0.3000
activation[7] = self(0.1000) + output[4](0.2000) * 1.0000 => 0.3000
activation[8] = self(0.1000) + output[5](0.2000) * 1.0000 => 0.3000
activation[9] = self(0.1000) + output[3](0.2000) * 0.7500 => 0.2500
activation[9] = self(0.2500) + output[4](0.2000) * 0.2500 => 0.3000
activation[10] = self(0.1000) + output[4](0.2000) * 0.2500 => 0.1500
activation[10] = self(0.1500) + output[5](0.2000) * 0.7500 => 0.3000
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.1010) > threshold(0.0000)? ==> 0.1010
neuronOutput[2]: activation(0.1010) > threshold(0.0000)? ==> 0.1010
neuronOutput[3]: activation(0.0300) > threshold(0.0000)? ==> 0.0300
neuronOutput[4]: activation(0.0300) > threshold(0.0000)? ==> 0.0300
neuronOutput[5]: activation(0.0300) > threshold(0.0000)? ==> 0.0300
neuronOutput[6]: activation(0.3000) > threshold(0.0000)? ==> 0.3000
neuronOutput[7]: activation(0.3000) > threshold(0.0000)? ==> 0.3000
neuronOutput[8]: activation(0.3000) > threshold(0.0000)? ==> 0.3000
neuronOutput[9]: activation(0.3000) > threshold(0.0000)? ==> 0.3000
neuronOutput[10]: activation(0.3000) > threshold(0.0000)? ==> 0.3000
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.3000)
networkOutput[2] := neuronOutput[10](0.3000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.1010) * output[3](0.0300) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0300) * output[3](0.0300) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.1010) * output[4](0.0300) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.1010) * output[4](0.0300) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0300) * output[4](0.0300) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.1010) * output[5](0.0300) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0300) * output[5](0.0300) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=005 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.1010) * weight[1,1](0.1000) => 0.0101
activation[2] = self(0.1010) * weight[2,2](0.1000) => 0.0101
activation[3] = self(0.0300) * weight[3,3](0.1000) => 0.0030
activation[4] = self(0.0300) * weight[4,4](0.1000) => 0.0030
activation[5] = self(0.0300) * weight[5,5](0.1000) => 0.0030
activation[6] = self(0.3000) * weight[6,6](0.1000) => 0.0300
activation[7] = self(0.3000) * weight[7,7](0.1000) => 0.0300
activation[8] = self(0.3000) * weight[8,8](0.1000) => 0.0300
activation[9] = self(0.3000) * weight[9,9](0.1000) => 0.0300
activation[10] = self(0.3000) * weight[10,10](0.1000) => 0.0300
  b. Update inputs from other neurons
activation[1] = self(0.0101) + network_input[1]=0.0000 => 0.0101
activation[2] = self(0.0101) + network_input[2]=0.0000 => 0.0101
activation[3] = self(0.0030) + output[1](0.1010) * 1.0000 => 0.1040
activation[3] = self(0.1040) + output[7](0.3000) * -0.5000 => -0.0460
activation[3] = self(-0.0460) + output[8](0.3000) * -0.5000 => -0.1960
activation[4] = self(0.0030) + output[1](0.1010) * 0.5000 => 0.0535
activation[4] = self(0.0535) + output[2](0.1010) * 0.5000 => 0.1040
activation[4] = self(0.1040) + output[6](0.3000) * -0.5000 => -0.0460
activation[4] = self(-0.0460) + output[8](0.3000) * -0.5000 => -0.1960
activation[5] = self(0.0030) + output[2](0.1010) * 1.0000 => 0.1040
activation[5] = self(0.1040) + output[6](0.3000) * -0.5000 => -0.0460
activation[5] = self(-0.0460) + output[7](0.3000) * -0.5000 => -0.1960
activation[6] = self(0.0300) + output[3](0.0300) * 1.0000 => 0.0600
activation[7] = self(0.0300) + output[4](0.0300) * 1.0000 => 0.0600
activation[8] = self(0.0300) + output[5](0.0300) * 1.0000 => 0.0600
activation[9] = self(0.0300) + output[3](0.0300) * 0.7500 => 0.0525
activation[9] = self(0.0525) + output[4](0.0300) * 0.2500 => 0.0600
activation[10] = self(0.0300) + output[4](0.0300) * 0.2500 => 0.0375
activation[10] = self(0.0375) + output[5](0.0300) * 0.7500 => 0.0600
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0101) > threshold(0.0000)? ==> 0.0101
neuronOutput[2]: activation(0.0101) > threshold(0.0000)? ==> 0.0101
neuronOutput[3]: activation(-0.1960) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-0.1960) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-0.1960) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.0600) > threshold(0.0000)? ==> 0.0600
neuronOutput[7]: activation(0.0600) > threshold(0.0000)? ==> 0.0600
neuronOutput[8]: activation(0.0600) > threshold(0.0000)? ==> 0.0600
neuronOutput[9]: activation(0.0600) > threshold(0.0000)? ==> 0.0600
neuronOutput[10]: activation(0.0600) > threshold(0.0000)? ==> 0.0600
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0600)
networkOutput[2] := neuronOutput[10](0.0600)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.0101) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.0101) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.0101) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.0101) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=006 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0101) * weight[1,1](0.1000) => 0.0010
activation[2] = self(0.0101) * weight[2,2](0.1000) => 0.0010
activation[3] = self(-0.1960) * weight[3,3](0.1000) => -0.0196
activation[4] = self(-0.1960) * weight[4,4](0.1000) => -0.0196
activation[5] = self(-0.1960) * weight[5,5](0.1000) => -0.0196
activation[6] = self(0.0600) * weight[6,6](0.1000) => 0.0060
activation[7] = self(0.0600) * weight[7,7](0.1000) => 0.0060
activation[8] = self(0.0600) * weight[8,8](0.1000) => 0.0060
activation[9] = self(0.0600) * weight[9,9](0.1000) => 0.0060
activation[10] = self(0.0600) * weight[10,10](0.1000) => 0.0060
  b. Update inputs from other neurons
activation[1] = self(0.0010) + network_input[1]=0.0000 => 0.0010
activation[2] = self(0.0010) + network_input[2]=0.0000 => 0.0010
activation[3] = self(-0.0196) + output[1](0.0101) * 1.0000 => -0.0095
activation[3] = self(-0.0095) + output[7](0.0600) * -0.5000 => -0.0395
activation[3] = self(-0.0395) + output[8](0.0600) * -0.5000 => -0.0695
activation[4] = self(-0.0196) + output[1](0.0101) * 0.5000 => -0.0146
activation[4] = self(-0.0146) + output[2](0.0101) * 0.5000 => -0.0095
activation[4] = self(-0.0095) + output[6](0.0600) * -0.5000 => -0.0395
activation[4] = self(-0.0395) + output[8](0.0600) * -0.5000 => -0.0695
activation[5] = self(-0.0196) + output[2](0.0101) * 1.0000 => -0.0095
activation[5] = self(-0.0095) + output[6](0.0600) * -0.5000 => -0.0395
activation[5] = self(-0.0395) + output[7](0.0600) * -0.5000 => -0.0695
activation[6] = self(0.0060) + output[3](0.0000) * 1.0000 => 0.0060
activation[7] = self(0.0060) + output[4](0.0000) * 1.0000 => 0.0060
activation[8] = self(0.0060) + output[5](0.0000) * 1.0000 => 0.0060
activation[9] = self(0.0060) + output[3](0.0000) * 0.7500 => 0.0060
activation[9] = self(0.0060) + output[4](0.0000) * 0.2500 => 0.0060
activation[10] = self(0.0060) + output[4](0.0000) * 0.2500 => 0.0060
activation[10] = self(0.0060) + output[5](0.0000) * 0.7500 => 0.0060
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0010) > threshold(0.0000)? ==> 0.0010
neuronOutput[2]: activation(0.0010) > threshold(0.0000)? ==> 0.0010
neuronOutput[3]: activation(-0.0695) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-0.0695) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-0.0695) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.0060) > threshold(0.0000)? ==> 0.0060
neuronOutput[7]: activation(0.0060) > threshold(0.0000)? ==> 0.0060
neuronOutput[8]: activation(0.0060) > threshold(0.0000)? ==> 0.0060
neuronOutput[9]: activation(0.0060) > threshold(0.0000)? ==> 0.0060
neuronOutput[10]: activation(0.0060) > threshold(0.0000)? ==> 0.0060
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0060)
networkOutput[2] := neuronOutput[10](0.0060)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.0010) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.0010) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.0010) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.0010) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=007 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0010) * weight[1,1](0.1000) => 0.0001
activation[2] = self(0.0010) * weight[2,2](0.1000) => 0.0001
activation[3] = self(-0.0695) * weight[3,3](0.1000) => -0.0070
activation[4] = self(-0.0695) * weight[4,4](0.1000) => -0.0070
activation[5] = self(-0.0695) * weight[5,5](0.1000) => -0.0070
activation[6] = self(0.0060) * weight[6,6](0.1000) => 0.0006
activation[7] = self(0.0060) * weight[7,7](0.1000) => 0.0006
activation[8] = self(0.0060) * weight[8,8](0.1000) => 0.0006
activation[9] = self(0.0060) * weight[9,9](0.1000) => 0.0006
activation[10] = self(0.0060) * weight[10,10](0.1000) => 0.0006
  b. Update inputs from other neurons
activation[1] = self(0.0001) + network_input[1]=1.0000 => 1.0001
activation[2] = self(0.0001) + network_input[2]=1.0000 => 1.0001
activation[3] = self(-0.0070) + output[1](0.0010) * 1.0000 => -0.0059
activation[3] = self(-0.0059) + output[7](0.0060) * -0.5000 => -0.0089
activation[3] = self(-0.0089) + output[8](0.0060) * -0.5000 => -0.0119
activation[4] = self(-0.0070) + output[1](0.0010) * 0.5000 => -0.0064
activation[4] = self(-0.0064) + output[2](0.0010) * 0.5000 => -0.0059
activation[4] = self(-0.0059) + output[6](0.0060) * -0.5000 => -0.0089
activation[4] = self(-0.0089) + output[8](0.0060) * -0.5000 => -0.0119
activation[5] = self(-0.0070) + output[2](0.0010) * 1.0000 => -0.0059
activation[5] = self(-0.0059) + output[6](0.0060) * -0.5000 => -0.0089
activation[5] = self(-0.0089) + output[7](0.0060) * -0.5000 => -0.0119
activation[6] = self(0.0006) + output[3](0.0000) * 1.0000 => 0.0006
activation[7] = self(0.0006) + output[4](0.0000) * 1.0000 => 0.0006
activation[8] = self(0.0006) + output[5](0.0000) * 1.0000 => 0.0006
activation[9] = self(0.0006) + output[3](0.0000) * 0.7500 => 0.0006
activation[9] = self(0.0006) + output[4](0.0000) * 0.2500 => 0.0006
activation[10] = self(0.0006) + output[4](0.0000) * 0.2500 => 0.0006
activation[10] = self(0.0006) + output[5](0.0000) * 0.7500 => 0.0006
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0001) > threshold(0.0000)? ==> 1.0001
neuronOutput[2]: activation(1.0001) > threshold(0.0000)? ==> 1.0001
neuronOutput[3]: activation(-0.0119) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-0.0119) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-0.0119) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.0006) > threshold(0.0000)? ==> 0.0006
neuronOutput[7]: activation(0.0006) > threshold(0.0000)? ==> 0.0006
neuronOutput[8]: activation(0.0006) > threshold(0.0000)? ==> 0.0006
neuronOutput[9]: activation(0.0006) > threshold(0.0000)? ==> 0.0006
neuronOutput[10]: activation(0.0006) > threshold(0.0000)? ==> 0.0006
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0006)
networkOutput[2] := neuronOutput[10](0.0006)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.0001) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.0001) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.0001) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.0001) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=008 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0001) * weight[1,1](0.1000) => 0.1000
activation[2] = self(1.0001) * weight[2,2](0.1000) => 0.1000
activation[3] = self(-0.0119) * weight[3,3](0.1000) => -0.0012
activation[4] = self(-0.0119) * weight[4,4](0.1000) => -0.0012
activation[5] = self(-0.0119) * weight[5,5](0.1000) => -0.0012
activation[6] = self(0.0006) * weight[6,6](0.1000) => 0.0001
activation[7] = self(0.0006) * weight[7,7](0.1000) => 0.0001
activation[8] = self(0.0006) * weight[8,8](0.1000) => 0.0001
activation[9] = self(0.0006) * weight[9,9](0.1000) => 0.0001
activation[10] = self(0.0006) * weight[10,10](0.1000) => 0.0001
  b. Update inputs from other neurons
activation[1] = self(0.1000) + network_input[1]=1.0000 => 1.1000
activation[2] = self(0.1000) + network_input[2]=1.0000 => 1.1000
activation[3] = self(-0.0012) + output[1](1.0001) * 1.0000 => 0.9989
activation[3] = self(0.9989) + output[7](0.0006) * -0.5000 => 0.9986
activation[3] = self(0.9986) + output[8](0.0006) * -0.5000 => 0.9983
activation[4] = self(-0.0012) + output[1](1.0001) * 0.5000 => 0.4989
activation[4] = self(0.4989) + output[2](1.0001) * 0.5000 => 0.9989
activation[4] = self(0.9989) + output[6](0.0006) * -0.5000 => 0.9986
activation[4] = self(0.9986) + output[8](0.0006) * -0.5000 => 0.9983
activation[5] = self(-0.0012) + output[2](1.0001) * 1.0000 => 0.9989
activation[5] = self(0.9989) + output[6](0.0006) * -0.5000 => 0.9986
activation[5] = self(0.9986) + output[7](0.0006) * -0.5000 => 0.9983
activation[6] = self(0.0001) + output[3](0.0000) * 1.0000 => 0.0001
activation[7] = self(0.0001) + output[4](0.0000) * 1.0000 => 0.0001
activation[8] = self(0.0001) + output[5](0.0000) * 1.0000 => 0.0001
activation[9] = self(0.0001) + output[3](0.0000) * 0.7500 => 0.0001
activation[9] = self(0.0001) + output[4](0.0000) * 0.2500 => 0.0001
activation[10] = self(0.0001) + output[4](0.0000) * 0.2500 => 0.0001
activation[10] = self(0.0001) + output[5](0.0000) * 0.7500 => 0.0001
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.1000) > threshold(0.0000)? ==> 1.1000
neuronOutput[2]: activation(1.1000) > threshold(0.0000)? ==> 1.1000
neuronOutput[3]: activation(0.9983) > threshold(0.0000)? ==> 0.9983
neuronOutput[4]: activation(0.9983) > threshold(0.0000)? ==> 0.9983
neuronOutput[5]: activation(0.9983) > threshold(0.0000)? ==> 0.9983
neuronOutput[6]: activation(0.0001) > threshold(0.0000)? ==> 0.0001
neuronOutput[7]: activation(0.0001) > threshold(0.0000)? ==> 0.0001
neuronOutput[8]: activation(0.0001) > threshold(0.0000)? ==> 0.0001
neuronOutput[9]: activation(0.0001) > threshold(0.0000)? ==> 0.0001
neuronOutput[10]: activation(0.0001) > threshold(0.0000)? ==> 0.0001
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0001)
networkOutput[2] := neuronOutput[10](0.0001)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.1000) * output[3](0.9983) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.9983) * output[3](0.9983) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.1000) * output[4](0.9983) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.1000) * output[4](0.9983) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.9983) * output[4](0.9983) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.1000) * output[5](0.9983) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.9983) * output[5](0.9983) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=009 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.1000) * weight[1,1](0.1000) => 0.1100
activation[2] = self(1.1000) * weight[2,2](0.1000) => 0.1100
activation[3] = self(0.9983) * weight[3,3](0.1000) => 0.0998
activation[4] = self(0.9983) * weight[4,4](0.1000) => 0.0998
activation[5] = self(0.9983) * weight[5,5](0.1000) => 0.0998
activation[6] = self(0.0001) * weight[6,6](0.1000) => 0.0000
activation[7] = self(0.0001) * weight[7,7](0.1000) => 0.0000
activation[8] = self(0.0001) * weight[8,8](0.1000) => 0.0000
activation[9] = self(0.0001) * weight[9,9](0.1000) => 0.0000
activation[10] = self(0.0001) * weight[10,10](0.1000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.1100) + network_input[1]=1.0000 => 1.1100
activation[2] = self(0.1100) + network_input[2]=1.0000 => 1.1100
activation[3] = self(0.0998) + output[1](1.1000) * 1.0000 => 1.1998
activation[3] = self(1.1998) + output[7](0.0001) * -0.5000 => 1.1998
activation[3] = self(1.1998) + output[8](0.0001) * -0.5000 => 1.1998
activation[4] = self(0.0998) + output[1](1.1000) * 0.5000 => 0.6498
activation[4] = self(0.6498) + output[2](1.1000) * 0.5000 => 1.1998
activation[4] = self(1.1998) + output[6](0.0001) * -0.5000 => 1.1998
activation[4] = self(1.1998) + output[8](0.0001) * -0.5000 => 1.1998
activation[5] = self(0.0998) + output[2](1.1000) * 1.0000 => 1.1998
activation[5] = self(1.1998) + output[6](0.0001) * -0.5000 => 1.1998
activation[5] = self(1.1998) + output[7](0.0001) * -0.5000 => 1.1998
activation[6] = self(0.0000) + output[3](0.9983) * 1.0000 => 0.9983
activation[7] = self(0.0000) + output[4](0.9983) * 1.0000 => 0.9983
activation[8] = self(0.0000) + output[5](0.9983) * 1.0000 => 0.9983
activation[9] = self(0.0000) + output[3](0.9983) * 0.7500 => 0.7487
activation[9] = self(0.7487) + output[4](0.9983) * 0.2500 => 0.9983
activation[10] = self(0.0000) + output[4](0.9983) * 0.2500 => 0.2496
activation[10] = self(0.2496) + output[5](0.9983) * 0.7500 => 0.9983
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.1100) > threshold(0.0000)? ==> 1.1100
neuronOutput[2]: activation(1.1100) > threshold(0.0000)? ==> 1.1100
neuronOutput[3]: activation(1.1998) > threshold(0.0000)? ==> 1.1998
neuronOutput[4]: activation(1.1998) > threshold(0.0000)? ==> 1.1998
neuronOutput[5]: activation(1.1998) > threshold(0.0000)? ==> 1.1998
neuronOutput[6]: activation(0.9983) > threshold(0.0000)? ==> 0.9983
neuronOutput[7]: activation(0.9983) > threshold(0.0000)? ==> 0.9983
neuronOutput[8]: activation(0.9983) > threshold(0.0000)? ==> 0.9983
neuronOutput[9]: activation(0.9983) > threshold(0.0000)? ==> 0.9983
neuronOutput[10]: activation(0.9983) > threshold(0.0000)? ==> 0.9983
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.9983)
networkOutput[2] := neuronOutput[10](0.9983)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.1100) * output[3](1.1998) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](1.1998) * output[3](1.1998) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.1100) * output[4](1.1998) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.1100) * output[4](1.1998) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](1.1998) * output[4](1.1998) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.1100) * output[5](1.1998) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](1.1998) * output[5](1.1998) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=010 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.1100) * weight[1,1](0.1000) => 0.1110
activation[2] = self(1.1100) * weight[2,2](0.1000) => 0.1110
activation[3] = self(1.1998) * weight[3,3](0.1000) => 0.1200
activation[4] = self(1.1998) * weight[4,4](0.1000) => 0.1200
activation[5] = self(1.1998) * weight[5,5](0.1000) => 0.1200
activation[6] = self(0.9983) * weight[6,6](0.1000) => 0.0998
activation[7] = self(0.9983) * weight[7,7](0.1000) => 0.0998
activation[8] = self(0.9983) * weight[8,8](0.1000) => 0.0998
activation[9] = self(0.9983) * weight[9,9](0.1000) => 0.0998
activation[10] = self(0.9983) * weight[10,10](0.1000) => 0.0998
  b. Update inputs from other neurons
activation[1] = self(0.1110) + network_input[1]=0.0000 => 0.1110
activation[2] = self(0.1110) + network_input[2]=0.0000 => 0.1110
activation[3] = self(0.1200) + output[1](1.1100) * 1.0000 => 1.2300
activation[3] = self(1.2300) + output[7](0.9983) * -0.5000 => 0.7308
activation[3] = self(0.7308) + output[8](0.9983) * -0.5000 => 0.2317
activation[4] = self(0.1200) + output[1](1.1100) * 0.5000 => 0.6750
activation[4] = self(0.6750) + output[2](1.1100) * 0.5000 => 1.2300
activation[4] = self(1.2300) + output[6](0.9983) * -0.5000 => 0.7308
activation[4] = self(0.7308) + output[8](0.9983) * -0.5000 => 0.2317
activation[5] = self(0.1200) + output[2](1.1100) * 1.0000 => 1.2300
activation[5] = self(1.2300) + output[6](0.9983) * -0.5000 => 0.7308
activation[5] = self(0.7308) + output[7](0.9983) * -0.5000 => 0.2317
activation[6] = self(0.0998) + output[3](1.1998) * 1.0000 => 1.2996
activation[7] = self(0.0998) + output[4](1.1998) * 1.0000 => 1.2996
activation[8] = self(0.0998) + output[5](1.1998) * 1.0000 => 1.2996
activation[9] = self(0.0998) + output[3](1.1998) * 0.7500 => 0.9997
activation[9] = self(0.9997) + output[4](1.1998) * 0.2500 => 1.2996
activation[10] = self(0.0998) + output[4](1.1998) * 0.2500 => 0.3998
activation[10] = self(0.3998) + output[5](1.1998) * 0.7500 => 1.2996
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.1110) > threshold(0.0000)? ==> 0.1110
neuronOutput[2]: activation(0.1110) > threshold(0.0000)? ==> 0.1110
neuronOutput[3]: activation(0.2317) > threshold(0.0000)? ==> 0.2317
neuronOutput[4]: activation(0.2317) > threshold(0.0000)? ==> 0.2317
neuronOutput[5]: activation(0.2317) > threshold(0.0000)? ==> 0.2317
neuronOutput[6]: activation(1.2996) > threshold(0.0000)? ==> 1.2996
neuronOutput[7]: activation(1.2996) > threshold(0.0000)? ==> 1.2996
neuronOutput[8]: activation(1.2996) > threshold(0.0000)? ==> 1.2996
neuronOutput[9]: activation(1.2996) > threshold(0.0000)? ==> 1.2996
neuronOutput[10]: activation(1.2996) > threshold(0.0000)? ==> 1.2996
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](1.2996)
networkOutput[2] := neuronOutput[10](1.2996)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.1110) * output[3](0.2317) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.2317) * output[3](0.2317) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.1110) * output[4](0.2317) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.1110) * output[4](0.2317) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.2317) * output[4](0.2317) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.1110) * output[5](0.2317) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.2317) * output[5](0.2317) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=011 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.1110) * weight[1,1](0.1000) => 0.0111
activation[2] = self(0.1110) * weight[2,2](0.1000) => 0.0111
activation[3] = self(0.2317) * weight[3,3](0.1000) => 0.0232
activation[4] = self(0.2317) * weight[4,4](0.1000) => 0.0232
activation[5] = self(0.2317) * weight[5,5](0.1000) => 0.0232
activation[6] = self(1.2996) * weight[6,6](0.1000) => 0.1300
activation[7] = self(1.2996) * weight[7,7](0.1000) => 0.1300
activation[8] = self(1.2996) * weight[8,8](0.1000) => 0.1300
activation[9] = self(1.2996) * weight[9,9](0.1000) => 0.1300
activation[10] = self(1.2996) * weight[10,10](0.1000) => 0.1300
  b. Update inputs from other neurons
activation[1] = self(0.0111) + network_input[1]=1.0000 => 1.0111
activation[2] = self(0.0111) + network_input[2]=1.0000 => 1.0111
activation[3] = self(0.0232) + output[1](0.1110) * 1.0000 => 0.1342
activation[3] = self(0.1342) + output[7](1.2996) * -0.5000 => -0.5156
activation[3] = self(-0.5156) + output[8](1.2996) * -0.5000 => -1.1654
activation[4] = self(0.0232) + output[1](0.1110) * 0.5000 => 0.0787
activation[4] = self(0.0787) + output[2](0.1110) * 0.5000 => 0.1342
activation[4] = self(0.1342) + output[6](1.2996) * -0.5000 => -0.5156
activation[4] = self(-0.5156) + output[8](1.2996) * -0.5000 => -1.1654
activation[5] = self(0.0232) + output[2](0.1110) * 1.0000 => 0.1342
activation[5] = self(0.1342) + output[6](1.2996) * -0.5000 => -0.5156
activation[5] = self(-0.5156) + output[7](1.2996) * -0.5000 => -1.1654
activation[6] = self(0.1300) + output[3](0.2317) * 1.0000 => 0.3616
activation[7] = self(0.1300) + output[4](0.2317) * 1.0000 => 0.3616
activation[8] = self(0.1300) + output[5](0.2317) * 1.0000 => 0.3616
activation[9] = self(0.1300) + output[3](0.2317) * 0.7500 => 0.3037
activation[9] = self(0.3037) + output[4](0.2317) * 0.2500 => 0.3616
activation[10] = self(0.1300) + output[4](0.2317) * 0.2500 => 0.1879
activation[10] = self(0.1879) + output[5](0.2317) * 0.7500 => 0.3616
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0111) > threshold(0.0000)? ==> 1.0111
neuronOutput[2]: activation(1.0111) > threshold(0.0000)? ==> 1.0111
neuronOutput[3]: activation(-1.1654) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-1.1654) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-1.1654) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.3616) > threshold(0.0000)? ==> 0.3616
neuronOutput[7]: activation(0.3616) > threshold(0.0000)? ==> 0.3616
neuronOutput[8]: activation(0.3616) > threshold(0.0000)? ==> 0.3616
neuronOutput[9]: activation(0.3616) > threshold(0.0000)? ==> 0.3616
neuronOutput[10]: activation(0.3616) > threshold(0.0000)? ==> 0.3616
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.3616)
networkOutput[2] := neuronOutput[10](0.3616)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.0111) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.0111) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.0111) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.0111) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=012 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0111) * weight[1,1](0.1000) => 0.1011
activation[2] = self(1.0111) * weight[2,2](0.1000) => 0.1011
activation[3] = self(-1.1654) * weight[3,3](0.1000) => -0.1165
activation[4] = self(-1.1654) * weight[4,4](0.1000) => -0.1165
activation[5] = self(-1.1654) * weight[5,5](0.1000) => -0.1165
activation[6] = self(0.3616) * weight[6,6](0.1000) => 0.0362
activation[7] = self(0.3616) * weight[7,7](0.1000) => 0.0362
activation[8] = self(0.3616) * weight[8,8](0.1000) => 0.0362
activation[9] = self(0.3616) * weight[9,9](0.1000) => 0.0362
activation[10] = self(0.3616) * weight[10,10](0.1000) => 0.0362
  b. Update inputs from other neurons
activation[1] = self(0.1011) + network_input[1]=1.0000 => 1.1011
activation[2] = self(0.1011) + network_input[2]=1.0000 => 1.1011
activation[3] = self(-0.1165) + output[1](1.0111) * 1.0000 => 0.8946
activation[3] = self(0.8946) + output[7](0.3616) * -0.5000 => 0.7137
activation[3] = self(0.7137) + output[8](0.3616) * -0.5000 => 0.5329
activation[4] = self(-0.1165) + output[1](1.0111) * 0.5000 => 0.3890
activation[4] = self(0.3890) + output[2](1.0111) * 0.5000 => 0.8946
activation[4] = self(0.8946) + output[6](0.3616) * -0.5000 => 0.7137
activation[4] = self(0.7137) + output[8](0.3616) * -0.5000 => 0.5329
activation[5] = self(-0.1165) + output[2](1.0111) * 1.0000 => 0.8946
activation[5] = self(0.8946) + output[6](0.3616) * -0.5000 => 0.7137
activation[5] = self(0.7137) + output[7](0.3616) * -0.5000 => 0.5329
activation[6] = self(0.0362) + output[3](0.0000) * 1.0000 => 0.0362
activation[7] = self(0.0362) + output[4](0.0000) * 1.0000 => 0.0362
activation[8] = self(0.0362) + output[5](0.0000) * 1.0000 => 0.0362
activation[9] = self(0.0362) + output[3](0.0000) * 0.7500 => 0.0362
activation[9] = self(0.0362) + output[4](0.0000) * 0.2500 => 0.0362
activation[10] = self(0.0362) + output[4](0.0000) * 0.2500 => 0.0362
activation[10] = self(0.0362) + output[5](0.0000) * 0.7500 => 0.0362
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.1011) > threshold(0.0000)? ==> 1.1011
neuronOutput[2]: activation(1.1011) > threshold(0.0000)? ==> 1.1011
neuronOutput[3]: activation(0.5329) > threshold(0.0000)? ==> 0.5329
neuronOutput[4]: activation(0.5329) > threshold(0.0000)? ==> 0.5329
neuronOutput[5]: activation(0.5329) > threshold(0.0000)? ==> 0.5329
neuronOutput[6]: activation(0.0362) > threshold(0.0000)? ==> 0.0362
neuronOutput[7]: activation(0.0362) > threshold(0.0000)? ==> 0.0362
neuronOutput[8]: activation(0.0362) > threshold(0.0000)? ==> 0.0362
neuronOutput[9]: activation(0.0362) > threshold(0.0000)? ==> 0.0362
neuronOutput[10]: activation(0.0362) > threshold(0.0000)? ==> 0.0362
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0362)
networkOutput[2] := neuronOutput[10](0.0362)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.1011) * output[3](0.5329) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.5329) * output[3](0.5329) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.1011) * output[4](0.5329) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.1011) * output[4](0.5329) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.5329) * output[4](0.5329) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.1011) * output[5](0.5329) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.5329) * output[5](0.5329) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=013 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.1011) * weight[1,1](0.1000) => 0.1101
activation[2] = self(1.1011) * weight[2,2](0.1000) => 0.1101
activation[3] = self(0.5329) * weight[3,3](0.1000) => 0.0533
activation[4] = self(0.5329) * weight[4,4](0.1000) => 0.0533
activation[5] = self(0.5329) * weight[5,5](0.1000) => 0.0533
activation[6] = self(0.0362) * weight[6,6](0.1000) => 0.0036
activation[7] = self(0.0362) * weight[7,7](0.1000) => 0.0036
activation[8] = self(0.0362) * weight[8,8](0.1000) => 0.0036
activation[9] = self(0.0362) * weight[9,9](0.1000) => 0.0036
activation[10] = self(0.0362) * weight[10,10](0.1000) => 0.0036
  b. Update inputs from other neurons
activation[1] = self(0.1101) + network_input[1]=0.0000 => 0.1101
activation[2] = self(0.1101) + network_input[2]=0.0000 => 0.1101
activation[3] = self(0.0533) + output[1](1.1011) * 1.0000 => 1.1544
activation[3] = self(1.1544) + output[7](0.0362) * -0.5000 => 1.1363
activation[3] = self(1.1363) + output[8](0.0362) * -0.5000 => 1.1182
activation[4] = self(0.0533) + output[1](1.1011) * 0.5000 => 0.6038
activation[4] = self(0.6038) + output[2](1.1011) * 0.5000 => 1.1544
activation[4] = self(1.1544) + output[6](0.0362) * -0.5000 => 1.1363
activation[4] = self(1.1363) + output[8](0.0362) * -0.5000 => 1.1182
activation[5] = self(0.0533) + output[2](1.1011) * 1.0000 => 1.1544
activation[5] = self(1.1544) + output[6](0.0362) * -0.5000 => 1.1363
activation[5] = self(1.1363) + output[7](0.0362) * -0.5000 => 1.1182
activation[6] = self(0.0036) + output[3](0.5329) * 1.0000 => 0.5365
activation[7] = self(0.0036) + output[4](0.5329) * 1.0000 => 0.5365
activation[8] = self(0.0036) + output[5](0.5329) * 1.0000 => 0.5365
activation[9] = self(0.0036) + output[3](0.5329) * 0.7500 => 0.4033
activation[9] = self(0.4033) + output[4](0.5329) * 0.2500 => 0.5365
activation[10] = self(0.0036) + output[4](0.5329) * 0.2500 => 0.1368
activation[10] = self(0.1368) + output[5](0.5329) * 0.7500 => 0.5365
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.1101) > threshold(0.0000)? ==> 0.1101
neuronOutput[2]: activation(0.1101) > threshold(0.0000)? ==> 0.1101
neuronOutput[3]: activation(1.1182) > threshold(0.0000)? ==> 1.1182
neuronOutput[4]: activation(1.1182) > threshold(0.0000)? ==> 1.1182
neuronOutput[5]: activation(1.1182) > threshold(0.0000)? ==> 1.1182
neuronOutput[6]: activation(0.5365) > threshold(0.0000)? ==> 0.5365
neuronOutput[7]: activation(0.5365) > threshold(0.0000)? ==> 0.5365
neuronOutput[8]: activation(0.5365) > threshold(0.0000)? ==> 0.5365
neuronOutput[9]: activation(0.5365) > threshold(0.0000)? ==> 0.5365
neuronOutput[10]: activation(0.5365) > threshold(0.0000)? ==> 0.5365
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.5365)
networkOutput[2] := neuronOutput[10](0.5365)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.1101) * output[3](1.1182) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](1.1182) * output[3](1.1182) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.1101) * output[4](1.1182) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.1101) * output[4](1.1182) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](1.1182) * output[4](1.1182) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.1101) * output[5](1.1182) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](1.1182) * output[5](1.1182) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=014 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.1101) * weight[1,1](0.1000) => 0.0110
activation[2] = self(0.1101) * weight[2,2](0.1000) => 0.0110
activation[3] = self(1.1182) * weight[3,3](0.1000) => 0.1118
activation[4] = self(1.1182) * weight[4,4](0.1000) => 0.1118
activation[5] = self(1.1182) * weight[5,5](0.1000) => 0.1118
activation[6] = self(0.5365) * weight[6,6](0.1000) => 0.0537
activation[7] = self(0.5365) * weight[7,7](0.1000) => 0.0537
activation[8] = self(0.5365) * weight[8,8](0.1000) => 0.0537
activation[9] = self(0.5365) * weight[9,9](0.1000) => 0.0537
activation[10] = self(0.5365) * weight[10,10](0.1000) => 0.0537
  b. Update inputs from other neurons
activation[1] = self(0.0110) + network_input[1]=1.0000 => 1.0110
activation[2] = self(0.0110) + network_input[2]=1.0000 => 1.0110
activation[3] = self(0.1118) + output[1](0.1101) * 1.0000 => 0.2219
activation[3] = self(0.2219) + output[7](0.5365) * -0.5000 => -0.0463
activation[3] = self(-0.0463) + output[8](0.5365) * -0.5000 => -0.3146
activation[4] = self(0.1118) + output[1](0.1101) * 0.5000 => 0.1669
activation[4] = self(0.1669) + output[2](0.1101) * 0.5000 => 0.2219
activation[4] = self(0.2219) + output[6](0.5365) * -0.5000 => -0.0463
activation[4] = self(-0.0463) + output[8](0.5365) * -0.5000 => -0.3146
activation[5] = self(0.1118) + output[2](0.1101) * 1.0000 => 0.2219
activation[5] = self(0.2219) + output[6](0.5365) * -0.5000 => -0.0463
activation[5] = self(-0.0463) + output[7](0.5365) * -0.5000 => -0.3146
activation[6] = self(0.0537) + output[3](1.1182) * 1.0000 => 1.1719
activation[7] = self(0.0537) + output[4](1.1182) * 1.0000 => 1.1719
activation[8] = self(0.0537) + output[5](1.1182) * 1.0000 => 1.1719
activation[9] = self(0.0537) + output[3](1.1182) * 0.7500 => 0.8923
activation[9] = self(0.8923) + output[4](1.1182) * 0.2500 => 1.1719
activation[10] = self(0.0537) + output[4](1.1182) * 0.2500 => 0.3332
activation[10] = self(0.3332) + output[5](1.1182) * 0.7500 => 1.1719
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0110) > threshold(0.0000)? ==> 1.0110
neuronOutput[2]: activation(1.0110) > threshold(0.0000)? ==> 1.0110
neuronOutput[3]: activation(-0.3146) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-0.3146) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-0.3146) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(1.1719) > threshold(0.0000)? ==> 1.1719
neuronOutput[7]: activation(1.1719) > threshold(0.0000)? ==> 1.1719
neuronOutput[8]: activation(1.1719) > threshold(0.0000)? ==> 1.1719
neuronOutput[9]: activation(1.1719) > threshold(0.0000)? ==> 1.1719
neuronOutput[10]: activation(1.1719) > threshold(0.0000)? ==> 1.1719
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](1.1719)
networkOutput[2] := neuronOutput[10](1.1719)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.0110) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.0110) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.0110) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.0110) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=015 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0110) * weight[1,1](0.1000) => 0.1011
activation[2] = self(1.0110) * weight[2,2](0.1000) => 0.1011
activation[3] = self(-0.3146) * weight[3,3](0.1000) => -0.0315
activation[4] = self(-0.3146) * weight[4,4](0.1000) => -0.0315
activation[5] = self(-0.3146) * weight[5,5](0.1000) => -0.0315
activation[6] = self(1.1719) * weight[6,6](0.1000) => 0.1172
activation[7] = self(1.1719) * weight[7,7](0.1000) => 0.1172
activation[8] = self(1.1719) * weight[8,8](0.1000) => 0.1172
activation[9] = self(1.1719) * weight[9,9](0.1000) => 0.1172
activation[10] = self(1.1719) * weight[10,10](0.1000) => 0.1172
  b. Update inputs from other neurons
activation[1] = self(0.1011) + network_input[1]=0.0000 => 0.1011
activation[2] = self(0.1011) + network_input[2]=0.0000 => 0.1011
activation[3] = self(-0.0315) + output[1](1.0110) * 1.0000 => 0.9796
activation[3] = self(0.9796) + output[7](1.1719) * -0.5000 => 0.3936
activation[3] = self(0.3936) + output[8](1.1719) * -0.5000 => -0.1923
activation[4] = self(-0.0315) + output[1](1.0110) * 0.5000 => 0.4740
activation[4] = self(0.4740) + output[2](1.0110) * 0.5000 => 0.9796
activation[4] = self(0.9796) + output[6](1.1719) * -0.5000 => 0.3936
activation[4] = self(0.3936) + output[8](1.1719) * -0.5000 => -0.1923
activation[5] = self(-0.0315) + output[2](1.0110) * 1.0000 => 0.9796
activation[5] = self(0.9796) + output[6](1.1719) * -0.5000 => 0.3936
activation[5] = self(0.3936) + output[7](1.1719) * -0.5000 => -0.1923
activation[6] = self(0.1172) + output[3](0.0000) * 1.0000 => 0.1172
activation[7] = self(0.1172) + output[4](0.0000) * 1.0000 => 0.1172
activation[8] = self(0.1172) + output[5](0.0000) * 1.0000 => 0.1172
activation[9] = self(0.1172) + output[3](0.0000) * 0.7500 => 0.1172
activation[9] = self(0.1172) + output[4](0.0000) * 0.2500 => 0.1172
activation[10] = self(0.1172) + output[4](0.0000) * 0.2500 => 0.1172
activation[10] = self(0.1172) + output[5](0.0000) * 0.7500 => 0.1172
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.1011) > threshold(0.0000)? ==> 0.1011
neuronOutput[2]: activation(0.1011) > threshold(0.0000)? ==> 0.1011
neuronOutput[3]: activation(-0.1923) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-0.1923) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-0.1923) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.1172) > threshold(0.0000)? ==> 0.1172
neuronOutput[7]: activation(0.1172) > threshold(0.0000)? ==> 0.1172
neuronOutput[8]: activation(0.1172) > threshold(0.0000)? ==> 0.1172
neuronOutput[9]: activation(0.1172) > threshold(0.0000)? ==> 0.1172
neuronOutput[10]: activation(0.1172) > threshold(0.0000)? ==> 0.1172
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.1172)
networkOutput[2] := neuronOutput[10](0.1172)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.1011) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.1011) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.1011) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.1011) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=016 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.1011) * weight[1,1](0.1000) => 0.0101
activation[2] = self(0.1011) * weight[2,2](0.1000) => 0.0101
activation[3] = self(-0.1923) * weight[3,3](0.1000) => -0.0192
activation[4] = self(-0.1923) * weight[4,4](0.1000) => -0.0192
activation[5] = self(-0.1923) * weight[5,5](0.1000) => -0.0192
activation[6] = self(0.1172) * weight[6,6](0.1000) => 0.0117
activation[7] = self(0.1172) * weight[7,7](0.1000) => 0.0117
activation[8] = self(0.1172) * weight[8,8](0.1000) => 0.0117
activation[9] = self(0.1172) * weight[9,9](0.1000) => 0.0117
activation[10] = self(0.1172) * weight[10,10](0.1000) => 0.0117
  b. Update inputs from other neurons
activation[1] = self(0.0101) + network_input[1]=0.0000 => 0.0101
activation[2] = self(0.0101) + network_input[2]=0.0000 => 0.0101
activation[3] = self(-0.0192) + output[1](0.1011) * 1.0000 => 0.0819
activation[3] = self(0.0819) + output[7](0.1172) * -0.5000 => 0.0233
activation[3] = self(0.0233) + output[8](0.1172) * -0.5000 => -0.0353
activation[4] = self(-0.0192) + output[1](0.1011) * 0.5000 => 0.0313
activation[4] = self(0.0313) + output[2](0.1011) * 0.5000 => 0.0819
activation[4] = self(0.0819) + output[6](0.1172) * -0.5000 => 0.0233
activation[4] = self(0.0233) + output[8](0.1172) * -0.5000 => -0.0353
activation[5] = self(-0.0192) + output[2](0.1011) * 1.0000 => 0.0819
activation[5] = self(0.0819) + output[6](0.1172) * -0.5000 => 0.0233
activation[5] = self(0.0233) + output[7](0.1172) * -0.5000 => -0.0353
activation[6] = self(0.0117) + output[3](0.0000) * 1.0000 => 0.0117
activation[7] = self(0.0117) + output[4](0.0000) * 1.0000 => 0.0117
activation[8] = self(0.0117) + output[5](0.0000) * 1.0000 => 0.0117
activation[9] = self(0.0117) + output[3](0.0000) * 0.7500 => 0.0117
activation[9] = self(0.0117) + output[4](0.0000) * 0.2500 => 0.0117
activation[10] = self(0.0117) + output[4](0.0000) * 0.2500 => 0.0117
activation[10] = self(0.0117) + output[5](0.0000) * 0.7500 => 0.0117
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0101) > threshold(0.0000)? ==> 0.0101
neuronOutput[2]: activation(0.0101) > threshold(0.0000)? ==> 0.0101
neuronOutput[3]: activation(-0.0353) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-0.0353) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-0.0353) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.0117) > threshold(0.0000)? ==> 0.0117
neuronOutput[7]: activation(0.0117) > threshold(0.0000)? ==> 0.0117
neuronOutput[8]: activation(0.0117) > threshold(0.0000)? ==> 0.0117
neuronOutput[9]: activation(0.0117) > threshold(0.0000)? ==> 0.0117
neuronOutput[10]: activation(0.0117) > threshold(0.0000)? ==> 0.0117
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0117)
networkOutput[2] := neuronOutput[10](0.0117)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.0101) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.0101) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.0101) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.0101) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=017 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0101) * weight[1,1](0.1000) => 0.0010
activation[2] = self(0.0101) * weight[2,2](0.1000) => 0.0010
activation[3] = self(-0.0353) * weight[3,3](0.1000) => -0.0035
activation[4] = self(-0.0353) * weight[4,4](0.1000) => -0.0035
activation[5] = self(-0.0353) * weight[5,5](0.1000) => -0.0035
activation[6] = self(0.0117) * weight[6,6](0.1000) => 0.0012
activation[7] = self(0.0117) * weight[7,7](0.1000) => 0.0012
activation[8] = self(0.0117) * weight[8,8](0.1000) => 0.0012
activation[9] = self(0.0117) * weight[9,9](0.1000) => 0.0012
activation[10] = self(0.0117) * weight[10,10](0.1000) => 0.0012
  b. Update inputs from other neurons
activation[1] = self(0.0010) + network_input[1]=0.0000 => 0.0010
activation[2] = self(0.0010) + network_input[2]=0.0000 => 0.0010
activation[3] = self(-0.0035) + output[1](0.0101) * 1.0000 => 0.0066
activation[3] = self(0.0066) + output[7](0.0117) * -0.5000 => 0.0007
activation[3] = self(0.0007) + output[8](0.0117) * -0.5000 => -0.0051
activation[4] = self(-0.0035) + output[1](0.0101) * 0.5000 => 0.0015
activation[4] = self(0.0015) + output[2](0.0101) * 0.5000 => 0.0066
activation[4] = self(0.0066) + output[6](0.0117) * -0.5000 => 0.0007
activation[4] = self(0.0007) + output[8](0.0117) * -0.5000 => -0.0051
activation[5] = self(-0.0035) + output[2](0.0101) * 1.0000 => 0.0066
activation[5] = self(0.0066) + output[6](0.0117) * -0.5000 => 0.0007
activation[5] = self(0.0007) + output[7](0.0117) * -0.5000 => -0.0051
activation[6] = self(0.0012) + output[3](0.0000) * 1.0000 => 0.0012
activation[7] = self(0.0012) + output[4](0.0000) * 1.0000 => 0.0012
activation[8] = self(0.0012) + output[5](0.0000) * 1.0000 => 0.0012
activation[9] = self(0.0012) + output[3](0.0000) * 0.7500 => 0.0012
activation[9] = self(0.0012) + output[4](0.0000) * 0.2500 => 0.0012
activation[10] = self(0.0012) + output[4](0.0000) * 0.2500 => 0.0012
activation[10] = self(0.0012) + output[5](0.0000) * 0.7500 => 0.0012
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0010) > threshold(0.0000)? ==> 0.0010
neuronOutput[2]: activation(0.0010) > threshold(0.0000)? ==> 0.0010
neuronOutput[3]: activation(-0.0051) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-0.0051) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-0.0051) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.0012) > threshold(0.0000)? ==> 0.0012
neuronOutput[7]: activation(0.0012) > threshold(0.0000)? ==> 0.0012
neuronOutput[8]: activation(0.0012) > threshold(0.0000)? ==> 0.0012
neuronOutput[9]: activation(0.0012) > threshold(0.0000)? ==> 0.0012
neuronOutput[10]: activation(0.0012) > threshold(0.0000)? ==> 0.0012
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0012)
networkOutput[2] := neuronOutput[10](0.0012)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.0010) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.0010) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.0010) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.0010) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=018 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0010) * weight[1,1](0.1000) => 0.0001
activation[2] = self(0.0010) * weight[2,2](0.1000) => 0.0001
activation[3] = self(-0.0051) * weight[3,3](0.1000) => -0.0005
activation[4] = self(-0.0051) * weight[4,4](0.1000) => -0.0005
activation[5] = self(-0.0051) * weight[5,5](0.1000) => -0.0005
activation[6] = self(0.0012) * weight[6,6](0.1000) => 0.0001
activation[7] = self(0.0012) * weight[7,7](0.1000) => 0.0001
activation[8] = self(0.0012) * weight[8,8](0.1000) => 0.0001
activation[9] = self(0.0012) * weight[9,9](0.1000) => 0.0001
activation[10] = self(0.0012) * weight[10,10](0.1000) => 0.0001
  b. Update inputs from other neurons
activation[1] = self(0.0001) + network_input[1]=0.0000 => 0.0001
activation[2] = self(0.0001) + network_input[2]=0.0000 => 0.0001
activation[3] = self(-0.0005) + output[1](0.0010) * 1.0000 => 0.0005
activation[3] = self(0.0005) + output[7](0.0012) * -0.5000 => -0.0001
activation[3] = self(-0.0001) + output[8](0.0012) * -0.5000 => -0.0007
activation[4] = self(-0.0005) + output[1](0.0010) * 0.5000 => -0.0000
activation[4] = self(-0.0000) + output[2](0.0010) * 0.5000 => 0.0005
activation[4] = self(0.0005) + output[6](0.0012) * -0.5000 => -0.0001
activation[4] = self(-0.0001) + output[8](0.0012) * -0.5000 => -0.0007
activation[5] = self(-0.0005) + output[2](0.0010) * 1.0000 => 0.0005
activation[5] = self(0.0005) + output[6](0.0012) * -0.5000 => -0.0001
activation[5] = self(-0.0001) + output[7](0.0012) * -0.5000 => -0.0007
activation[6] = self(0.0001) + output[3](0.0000) * 1.0000 => 0.0001
activation[7] = self(0.0001) + output[4](0.0000) * 1.0000 => 0.0001
activation[8] = self(0.0001) + output[5](0.0000) * 1.0000 => 0.0001
activation[9] = self(0.0001) + output[3](0.0000) * 0.7500 => 0.0001
activation[9] = self(0.0001) + output[4](0.0000) * 0.2500 => 0.0001
activation[10] = self(0.0001) + output[4](0.0000) * 0.2500 => 0.0001
activation[10] = self(0.0001) + output[5](0.0000) * 0.7500 => 0.0001
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0001) > threshold(0.0000)? ==> 0.0001
neuronOutput[2]: activation(0.0001) > threshold(0.0000)? ==> 0.0001
neuronOutput[3]: activation(-0.0007) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-0.0007) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-0.0007) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.0001) > threshold(0.0000)? ==> 0.0001
neuronOutput[7]: activation(0.0001) > threshold(0.0000)? ==> 0.0001
neuronOutput[8]: activation(0.0001) > threshold(0.0000)? ==> 0.0001
neuronOutput[9]: activation(0.0001) > threshold(0.0000)? ==> 0.0001
neuronOutput[10]: activation(0.0001) > threshold(0.0000)? ==> 0.0001
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0001)
networkOutput[2] := neuronOutput[10](0.0001)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.0001) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.0001) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.0001) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.0001) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=019 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0001) * weight[1,1](0.1000) => 0.0000
activation[2] = self(0.0001) * weight[2,2](0.1000) => 0.0000
activation[3] = self(-0.0007) * weight[3,3](0.1000) => -0.0001
activation[4] = self(-0.0007) * weight[4,4](0.1000) => -0.0001
activation[5] = self(-0.0007) * weight[5,5](0.1000) => -0.0001
activation[6] = self(0.0001) * weight[6,6](0.1000) => 0.0000
activation[7] = self(0.0001) * weight[7,7](0.1000) => 0.0000
activation[8] = self(0.0001) * weight[8,8](0.1000) => 0.0000
activation[9] = self(0.0001) * weight[9,9](0.1000) => 0.0000
activation[10] = self(0.0001) * weight[10,10](0.1000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(-0.0001) + output[1](0.0001) * 1.0000 => 0.0000
activation[3] = self(0.0000) + output[7](0.0001) * -0.5000 => -0.0000
activation[3] = self(-0.0000) + output[8](0.0001) * -0.5000 => -0.0001
activation[4] = self(-0.0001) + output[1](0.0001) * 0.5000 => -0.0000
activation[4] = self(-0.0000) + output[2](0.0001) * 0.5000 => 0.0000
activation[4] = self(0.0000) + output[6](0.0001) * -0.5000 => -0.0000
activation[4] = self(-0.0000) + output[8](0.0001) * -0.5000 => -0.0001
activation[5] = self(-0.0001) + output[2](0.0001) * 1.0000 => 0.0000
activation[5] = self(0.0000) + output[6](0.0001) * -0.5000 => -0.0000
activation[5] = self(-0.0000) + output[7](0.0001) * -0.5000 => -0.0001
activation[6] = self(0.0000) + output[3](0.0000) * 1.0000 => 0.0000
activation[7] = self(0.0000) + output[4](0.0000) * 1.0000 => 0.0000
activation[8] = self(0.0000) + output[5](0.0000) * 1.0000 => 0.0000
activation[9] = self(0.0000) + output[3](0.0000) * 0.7500 => 0.0000
activation[9] = self(0.0000) + output[4](0.0000) * 0.2500 => 0.0000
activation[10] = self(0.0000) + output[4](0.0000) * 0.2500 => 0.0000
activation[10] = self(0.0000) + output[5](0.0000) * 0.7500 => 0.0000
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(-0.0001) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-0.0001) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-0.0001) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0000)
networkOutput[2] := neuronOutput[10](0.0000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.0000) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.0000) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.0000) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.0000) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=020 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.1000) => 0.1000
activation[2] = self(1.0000) * weight[2,2](0.1000) => 0.1000
activation[3] = self(-0.0001) * weight[3,3](0.1000) => -0.0000
activation[4] = self(-0.0001) * weight[4,4](0.1000) => -0.0000
activation[5] = self(-0.0001) * weight[5,5](0.1000) => -0.0000
activation[6] = self(0.0000) * weight[6,6](0.1000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.1000) => 0.0000
activation[8] = self(0.0000) * weight[8,8](0.1000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.1000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.1000) + network_input[1]=0.0000 => 0.1000
activation[2] = self(0.1000) + network_input[2]=0.0000 => 0.1000
activation[3] = self(-0.0000) + output[1](1.0000) * 1.0000 => 1.0000
activation[3] = self(1.0000) + output[7](0.0000) * -0.5000 => 1.0000
activation[3] = self(1.0000) + output[8](0.0000) * -0.5000 => 1.0000
activation[4] = self(-0.0000) + output[1](1.0000) * 0.5000 => 0.5000
activation[4] = self(0.5000) + output[2](1.0000) * 0.5000 => 1.0000
activation[4] = self(1.0000) + output[6](0.0000) * -0.5000 => 1.0000
activation[4] = self(1.0000) + output[8](0.0000) * -0.5000 => 1.0000
activation[5] = self(-0.0000) + output[2](1.0000) * 1.0000 => 1.0000
activation[5] = self(1.0000) + output[6](0.0000) * -0.5000 => 1.0000
activation[5] = self(1.0000) + output[7](0.0000) * -0.5000 => 1.0000
activation[6] = self(0.0000) + output[3](0.0000) * 1.0000 => 0.0000
activation[7] = self(0.0000) + output[4](0.0000) * 1.0000 => 0.0000
activation[8] = self(0.0000) + output[5](0.0000) * 1.0000 => 0.0000
activation[9] = self(0.0000) + output[3](0.0000) * 0.7500 => 0.0000
activation[9] = self(0.0000) + output[4](0.0000) * 0.2500 => 0.0000
activation[10] = self(0.0000) + output[4](0.0000) * 0.2500 => 0.0000
activation[10] = self(0.0000) + output[5](0.0000) * 0.7500 => 0.0000
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.1000) > threshold(0.0000)? ==> 0.1000
neuronOutput[2]: activation(0.1000) > threshold(0.0000)? ==> 0.1000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[5]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0000)
networkOutput[2] := neuronOutput[10](0.0000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.1000) * output[3](1.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](1.0000) * output[3](1.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.1000) * output[4](1.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.1000) * output[4](1.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](1.0000) * output[4](1.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.1000) * output[5](1.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](1.0000) * output[5](1.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=021 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.1000) * weight[1,1](0.1000) => 0.0100
activation[2] = self(0.1000) * weight[2,2](0.1000) => 0.0100
activation[3] = self(1.0000) * weight[3,3](0.1000) => 0.1000
activation[4] = self(1.0000) * weight[4,4](0.1000) => 0.1000
activation[5] = self(1.0000) * weight[5,5](0.1000) => 0.1000
activation[6] = self(0.0000) * weight[6,6](0.1000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.1000) => 0.0000
activation[8] = self(0.0000) * weight[8,8](0.1000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.1000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0100) + network_input[1]=0.0000 => 0.0100
activation[2] = self(0.0100) + network_input[2]=0.0000 => 0.0100
activation[3] = self(0.1000) + output[1](0.1000) * 1.0000 => 0.2000
activation[3] = self(0.2000) + output[7](0.0000) * -0.5000 => 0.2000
activation[3] = self(0.2000) + output[8](0.0000) * -0.5000 => 0.2000
activation[4] = self(0.1000) + output[1](0.1000) * 0.5000 => 0.1500
activation[4] = self(0.1500) + output[2](0.1000) * 0.5000 => 0.2000
activation[4] = self(0.2000) + output[6](0.0000) * -0.5000 => 0.2000
activation[4] = self(0.2000) + output[8](0.0000) * -0.5000 => 0.2000
activation[5] = self(0.1000) + output[2](0.1000) * 1.0000 => 0.2000
activation[5] = self(0.2000) + output[6](0.0000) * -0.5000 => 0.2000
activation[5] = self(0.2000) + output[7](0.0000) * -0.5000 => 0.2000
activation[6] = self(0.0000) + output[3](1.0000) * 1.0000 => 1.0000
activation[7] = self(0.0000) + output[4](1.0000) * 1.0000 => 1.0000
activation[8] = self(0.0000) + output[5](1.0000) * 1.0000 => 1.0000
activation[9] = self(0.0000) + output[3](1.0000) * 0.7500 => 0.7500
activation[9] = self(0.7500) + output[4](1.0000) * 0.2500 => 1.0000
activation[10] = self(0.0000) + output[4](1.0000) * 0.2500 => 0.2500
activation[10] = self(0.2500) + output[5](1.0000) * 0.7500 => 1.0000
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0100) > threshold(0.0000)? ==> 0.0100
neuronOutput[2]: activation(0.0100) > threshold(0.0000)? ==> 0.0100
neuronOutput[3]: activation(0.2000) > threshold(0.0000)? ==> 0.2000
neuronOutput[4]: activation(0.2000) > threshold(0.0000)? ==> 0.2000
neuronOutput[5]: activation(0.2000) > threshold(0.0000)? ==> 0.2000
neuronOutput[6]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[7]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[8]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[9]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[10]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](1.0000)
networkOutput[2] := neuronOutput[10](1.0000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.0100) * output[3](0.2000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.2000) * output[3](0.2000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.0100) * output[4](0.2000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.0100) * output[4](0.2000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.2000) * output[4](0.2000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.0100) * output[5](0.2000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.2000) * output[5](0.2000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=022 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0100) * weight[1,1](0.1000) => 0.0010
activation[2] = self(0.0100) * weight[2,2](0.1000) => 0.0010
activation[3] = self(0.2000) * weight[3,3](0.1000) => 0.0200
activation[4] = self(0.2000) * weight[4,4](0.1000) => 0.0200
activation[5] = self(0.2000) * weight[5,5](0.1000) => 0.0200
activation[6] = self(1.0000) * weight[6,6](0.1000) => 0.1000
activation[7] = self(1.0000) * weight[7,7](0.1000) => 0.1000
activation[8] = self(1.0000) * weight[8,8](0.1000) => 0.1000
activation[9] = self(1.0000) * weight[9,9](0.1000) => 0.1000
activation[10] = self(1.0000) * weight[10,10](0.1000) => 0.1000
  b. Update inputs from other neurons
activation[1] = self(0.0010) + network_input[1]=0.0000 => 0.0010
activation[2] = self(0.0010) + network_input[2]=0.0000 => 0.0010
activation[3] = self(0.0200) + output[1](0.0100) * 1.0000 => 0.0300
activation[3] = self(0.0300) + output[7](1.0000) * -0.5000 => -0.4700
activation[3] = self(-0.4700) + output[8](1.0000) * -0.5000 => -0.9700
activation[4] = self(0.0200) + output[1](0.0100) * 0.5000 => 0.0250
activation[4] = self(0.0250) + output[2](0.0100) * 0.5000 => 0.0300
activation[4] = self(0.0300) + output[6](1.0000) * -0.5000 => -0.4700
activation[4] = self(-0.4700) + output[8](1.0000) * -0.5000 => -0.9700
activation[5] = self(0.0200) + output[2](0.0100) * 1.0000 => 0.0300
activation[5] = self(0.0300) + output[6](1.0000) * -0.5000 => -0.4700
activation[5] = self(-0.4700) + output[7](1.0000) * -0.5000 => -0.9700
activation[6] = self(0.1000) + output[3](0.2000) * 1.0000 => 0.3000
activation[7] = self(0.1000) + output[4](0.2000) * 1.0000 => 0.3000
activation[8] = self(0.1000) + output[5](0.2000) * 1.0000 => 0.3000
activation[9] = self(0.1000) + output[3](0.2000) * 0.7500 => 0.2500
activation[9] = self(0.2500) + output[4](0.2000) * 0.2500 => 0.3000
activation[10] = self(0.1000) + output[4](0.2000) * 0.2500 => 0.1500
activation[10] = self(0.1500) + output[5](0.2000) * 0.7500 => 0.3000
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0010) > threshold(0.0000)? ==> 0.0010
neuronOutput[2]: activation(0.0010) > threshold(0.0000)? ==> 0.0010
neuronOutput[3]: activation(-0.9700) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-0.9700) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-0.9700) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.3000) > threshold(0.0000)? ==> 0.3000
neuronOutput[7]: activation(0.3000) > threshold(0.0000)? ==> 0.3000
neuronOutput[8]: activation(0.3000) > threshold(0.0000)? ==> 0.3000
neuronOutput[9]: activation(0.3000) > threshold(0.0000)? ==> 0.3000
neuronOutput[10]: activation(0.3000) > threshold(0.0000)? ==> 0.3000
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.3000)
networkOutput[2] := neuronOutput[10](0.3000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.0010) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.0010) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.0010) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.0010) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=023 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0010) * weight[1,1](0.1000) => 0.0001
activation[2] = self(0.0010) * weight[2,2](0.1000) => 0.0001
activation[3] = self(-0.9700) * weight[3,3](0.1000) => -0.0970
activation[4] = self(-0.9700) * weight[4,4](0.1000) => -0.0970
activation[5] = self(-0.9700) * weight[5,5](0.1000) => -0.0970
activation[6] = self(0.3000) * weight[6,6](0.1000) => 0.0300
activation[7] = self(0.3000) * weight[7,7](0.1000) => 0.0300
activation[8] = self(0.3000) * weight[8,8](0.1000) => 0.0300
activation[9] = self(0.3000) * weight[9,9](0.1000) => 0.0300
activation[10] = self(0.3000) * weight[10,10](0.1000) => 0.0300
  b. Update inputs from other neurons
activation[1] = self(0.0001) + network_input[1]=0.0000 => 0.0001
activation[2] = self(0.0001) + network_input[2]=0.0000 => 0.0001
activation[3] = self(-0.0970) + output[1](0.0010) * 1.0000 => -0.0960
activation[3] = self(-0.0960) + output[7](0.3000) * -0.5000 => -0.2460
activation[3] = self(-0.2460) + output[8](0.3000) * -0.5000 => -0.3960
activation[4] = self(-0.0970) + output[1](0.0010) * 0.5000 => -0.0965
activation[4] = self(-0.0965) + output[2](0.0010) * 0.5000 => -0.0960
activation[4] = self(-0.0960) + output[6](0.3000) * -0.5000 => -0.2460
activation[4] = self(-0.2460) + output[8](0.3000) * -0.5000 => -0.3960
activation[5] = self(-0.0970) + output[2](0.0010) * 1.0000 => -0.0960
activation[5] = self(-0.0960) + output[6](0.3000) * -0.5000 => -0.2460
activation[5] = self(-0.2460) + output[7](0.3000) * -0.5000 => -0.3960
activation[6] = self(0.0300) + output[3](0.0000) * 1.0000 => 0.0300
activation[7] = self(0.0300) + output[4](0.0000) * 1.0000 => 0.0300
activation[8] = self(0.0300) + output[5](0.0000) * 1.0000 => 0.0300
activation[9] = self(0.0300) + output[3](0.0000) * 0.7500 => 0.0300
activation[9] = self(0.0300) + output[4](0.0000) * 0.2500 => 0.0300
activation[10] = self(0.0300) + output[4](0.0000) * 0.2500 => 0.0300
activation[10] = self(0.0300) + output[5](0.0000) * 0.7500 => 0.0300
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0001) > threshold(0.0000)? ==> 0.0001
neuronOutput[2]: activation(0.0001) > threshold(0.0000)? ==> 0.0001
neuronOutput[3]: activation(-0.3960) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-0.3960) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-0.3960) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.0300) > threshold(0.0000)? ==> 0.0300
neuronOutput[7]: activation(0.0300) > threshold(0.0000)? ==> 0.0300
neuronOutput[8]: activation(0.0300) > threshold(0.0000)? ==> 0.0300
neuronOutput[9]: activation(0.0300) > threshold(0.0000)? ==> 0.0300
neuronOutput[10]: activation(0.0300) > threshold(0.0000)? ==> 0.0300
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0300)
networkOutput[2] := neuronOutput[10](0.0300)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.0001) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.0001) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.0001) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.0001) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=024 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0001) * weight[1,1](0.1000) => 0.0000
activation[2] = self(0.0001) * weight[2,2](0.1000) => 0.0000
activation[3] = self(-0.3960) * weight[3,3](0.1000) => -0.0396
activation[4] = self(-0.3960) * weight[4,4](0.1000) => -0.0396
activation[5] = self(-0.3960) * weight[5,5](0.1000) => -0.0396
activation[6] = self(0.0300) * weight[6,6](0.1000) => 0.0030
activation[7] = self(0.0300) * weight[7,7](0.1000) => 0.0030
activation[8] = self(0.0300) * weight[8,8](0.1000) => 0.0030
activation[9] = self(0.0300) * weight[9,9](0.1000) => 0.0030
activation[10] = self(0.0300) * weight[10,10](0.1000) => 0.0030
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0396) + output[1](0.0001) * 1.0000 => -0.0395
activation[3] = self(-0.0395) + output[7](0.0300) * -0.5000 => -0.0545
activation[3] = self(-0.0545) + output[8](0.0300) * -0.5000 => -0.0695
activation[4] = self(-0.0396) + output[1](0.0001) * 0.5000 => -0.0395
activation[4] = self(-0.0395) + output[2](0.0001) * 0.5000 => -0.0395
activation[4] = self(-0.0395) + output[6](0.0300) * -0.5000 => -0.0545
activation[4] = self(-0.0545) + output[8](0.0300) * -0.5000 => -0.0695
activation[5] = self(-0.0396) + output[2](0.0001) * 1.0000 => -0.0395
activation[5] = self(-0.0395) + output[6](0.0300) * -0.5000 => -0.0545
activation[5] = self(-0.0545) + output[7](0.0300) * -0.5000 => -0.0695
activation[6] = self(0.0030) + output[3](0.0000) * 1.0000 => 0.0030
activation[7] = self(0.0030) + output[4](0.0000) * 1.0000 => 0.0030
activation[8] = self(0.0030) + output[5](0.0000) * 1.0000 => 0.0030
activation[9] = self(0.0030) + output[3](0.0000) * 0.7500 => 0.0030
activation[9] = self(0.0030) + output[4](0.0000) * 0.2500 => 0.0030
activation[10] = self(0.0030) + output[4](0.0000) * 0.2500 => 0.0030
activation[10] = self(0.0030) + output[5](0.0000) * 0.7500 => 0.0030
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(-0.0695) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-0.0695) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-0.0695) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.0030) > threshold(0.0000)? ==> 0.0030
neuronOutput[7]: activation(0.0030) > threshold(0.0000)? ==> 0.0030
neuronOutput[8]: activation(0.0030) > threshold(0.0000)? ==> 0.0030
neuronOutput[9]: activation(0.0030) > threshold(0.0000)? ==> 0.0030
neuronOutput[10]: activation(0.0030) > threshold(0.0000)? ==> 0.0030
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0030)
networkOutput[2] := neuronOutput[10](0.0030)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.0000) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.0000) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.0000) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.0000) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=025 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.1000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.1000) => 0.0000
activation[3] = self(-0.0695) * weight[3,3](0.1000) => -0.0069
activation[4] = self(-0.0695) * weight[4,4](0.1000) => -0.0069
activation[5] = self(-0.0695) * weight[5,5](0.1000) => -0.0069
activation[6] = self(0.0030) * weight[6,6](0.1000) => 0.0003
activation[7] = self(0.0030) * weight[7,7](0.1000) => 0.0003
activation[8] = self(0.0030) * weight[8,8](0.1000) => 0.0003
activation[9] = self(0.0030) * weight[9,9](0.1000) => 0.0003
activation[10] = self(0.0030) * weight[10,10](0.1000) => 0.0003
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(-0.0069) + output[1](0.0000) * 1.0000 => -0.0069
activation[3] = self(-0.0069) + output[7](0.0030) * -0.5000 => -0.0084
activation[3] = self(-0.0084) + output[8](0.0030) * -0.5000 => -0.0099
activation[4] = self(-0.0069) + output[1](0.0000) * 0.5000 => -0.0069
activation[4] = self(-0.0069) + output[2](0.0000) * 0.5000 => -0.0069
activation[4] = self(-0.0069) + output[6](0.0030) * -0.5000 => -0.0084
activation[4] = self(-0.0084) + output[8](0.0030) * -0.5000 => -0.0099
activation[5] = self(-0.0069) + output[2](0.0000) * 1.0000 => -0.0069
activation[5] = self(-0.0069) + output[6](0.0030) * -0.5000 => -0.0084
activation[5] = self(-0.0084) + output[7](0.0030) * -0.5000 => -0.0099
activation[6] = self(0.0003) + output[3](0.0000) * 1.0000 => 0.0003
activation[7] = self(0.0003) + output[4](0.0000) * 1.0000 => 0.0003
activation[8] = self(0.0003) + output[5](0.0000) * 1.0000 => 0.0003
activation[9] = self(0.0003) + output[3](0.0000) * 0.7500 => 0.0003
activation[9] = self(0.0003) + output[4](0.0000) * 0.2500 => 0.0003
activation[10] = self(0.0003) + output[4](0.0000) * 0.2500 => 0.0003
activation[10] = self(0.0003) + output[5](0.0000) * 0.7500 => 0.0003
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(-0.0099) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-0.0099) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-0.0099) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.0003) > threshold(0.0000)? ==> 0.0003
neuronOutput[7]: activation(0.0003) > threshold(0.0000)? ==> 0.0003
neuronOutput[8]: activation(0.0003) > threshold(0.0000)? ==> 0.0003
neuronOutput[9]: activation(0.0003) > threshold(0.0000)? ==> 0.0003
neuronOutput[10]: activation(0.0003) > threshold(0.0000)? ==> 0.0003
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0003)
networkOutput[2] := neuronOutput[10](0.0003)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.0000) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.0000) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.0000) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.0000) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=026 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.1000) => 0.1000
activation[2] = self(1.0000) * weight[2,2](0.1000) => 0.1000
activation[3] = self(-0.0099) * weight[3,3](0.1000) => -0.0010
activation[4] = self(-0.0099) * weight[4,4](0.1000) => -0.0010
activation[5] = self(-0.0099) * weight[5,5](0.1000) => -0.0010
activation[6] = self(0.0003) * weight[6,6](0.1000) => 0.0000
activation[7] = self(0.0003) * weight[7,7](0.1000) => 0.0000
activation[8] = self(0.0003) * weight[8,8](0.1000) => 0.0000
activation[9] = self(0.0003) * weight[9,9](0.1000) => 0.0000
activation[10] = self(0.0003) * weight[10,10](0.1000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.1000) + network_input[1]=0.0000 => 0.1000
activation[2] = self(0.1000) + network_input[2]=0.0000 => 0.1000
activation[3] = self(-0.0010) + output[1](1.0000) * 1.0000 => 0.9990
activation[3] = self(0.9990) + output[7](0.0003) * -0.5000 => 0.9989
activation[3] = self(0.9989) + output[8](0.0003) * -0.5000 => 0.9987
activation[4] = self(-0.0010) + output[1](1.0000) * 0.5000 => 0.4990
activation[4] = self(0.4990) + output[2](1.0000) * 0.5000 => 0.9990
activation[4] = self(0.9990) + output[6](0.0003) * -0.5000 => 0.9989
activation[4] = self(0.9989) + output[8](0.0003) * -0.5000 => 0.9987
activation[5] = self(-0.0010) + output[2](1.0000) * 1.0000 => 0.9990
activation[5] = self(0.9990) + output[6](0.0003) * -0.5000 => 0.9989
activation[5] = self(0.9989) + output[7](0.0003) * -0.5000 => 0.9987
activation[6] = self(0.0000) + output[3](0.0000) * 1.0000 => 0.0000
activation[7] = self(0.0000) + output[4](0.0000) * 1.0000 => 0.0000
activation[8] = self(0.0000) + output[5](0.0000) * 1.0000 => 0.0000
activation[9] = self(0.0000) + output[3](0.0000) * 0.7500 => 0.0000
activation[9] = self(0.0000) + output[4](0.0000) * 0.2500 => 0.0000
activation[10] = self(0.0000) + output[4](0.0000) * 0.2500 => 0.0000
activation[10] = self(0.0000) + output[5](0.0000) * 0.7500 => 0.0000
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.1000) > threshold(0.0000)? ==> 0.1000
neuronOutput[2]: activation(0.1000) > threshold(0.0000)? ==> 0.1000
neuronOutput[3]: activation(0.9987) > threshold(0.0000)? ==> 0.9987
neuronOutput[4]: activation(0.9987) > threshold(0.0000)? ==> 0.9987
neuronOutput[5]: activation(0.9987) > threshold(0.0000)? ==> 0.9987
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0000)
networkOutput[2] := neuronOutput[10](0.0000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.1000) * output[3](0.9987) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.9987) * output[3](0.9987) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.1000) * output[4](0.9987) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.1000) * output[4](0.9987) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.9987) * output[4](0.9987) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.1000) * output[5](0.9987) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.9987) * output[5](0.9987) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=027 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.1000) * weight[1,1](0.1000) => 0.0100
activation[2] = self(0.1000) * weight[2,2](0.1000) => 0.0100
activation[3] = self(0.9987) * weight[3,3](0.1000) => 0.0999
activation[4] = self(0.9987) * weight[4,4](0.1000) => 0.0999
activation[5] = self(0.9987) * weight[5,5](0.1000) => 0.0999
activation[6] = self(0.0000) * weight[6,6](0.1000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.1000) => 0.0000
activation[8] = self(0.0000) * weight[8,8](0.1000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.1000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0100) + network_input[1]=1.0000 => 1.0100
activation[2] = self(0.0100) + network_input[2]=1.0000 => 1.0100
activation[3] = self(0.0999) + output[1](0.1000) * 1.0000 => 0.1999
activation[3] = self(0.1999) + output[7](0.0000) * -0.5000 => 0.1999
activation[3] = self(0.1999) + output[8](0.0000) * -0.5000 => 0.1998
activation[4] = self(0.0999) + output[1](0.1000) * 0.5000 => 0.1499
activation[4] = self(0.1499) + output[2](0.1000) * 0.5000 => 0.1999
activation[4] = self(0.1999) + output[6](0.0000) * -0.5000 => 0.1999
activation[4] = self(0.1999) + output[8](0.0000) * -0.5000 => 0.1998
activation[5] = self(0.0999) + output[2](0.1000) * 1.0000 => 0.1999
activation[5] = self(0.1999) + output[6](0.0000) * -0.5000 => 0.1999
activation[5] = self(0.1999) + output[7](0.0000) * -0.5000 => 0.1998
activation[6] = self(0.0000) + output[3](0.9987) * 1.0000 => 0.9987
activation[7] = self(0.0000) + output[4](0.9987) * 1.0000 => 0.9987
activation[8] = self(0.0000) + output[5](0.9987) * 1.0000 => 0.9987
activation[9] = self(0.0000) + output[3](0.9987) * 0.7500 => 0.7490
activation[9] = self(0.7490) + output[4](0.9987) * 0.2500 => 0.9987
activation[10] = self(0.0000) + output[4](0.9987) * 0.2500 => 0.2497
activation[10] = self(0.2497) + output[5](0.9987) * 0.7500 => 0.9987
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0100) > threshold(0.0000)? ==> 1.0100
neuronOutput[2]: activation(1.0100) > threshold(0.0000)? ==> 1.0100
neuronOutput[3]: activation(0.1998) > threshold(0.0000)? ==> 0.1998
neuronOutput[4]: activation(0.1998) > threshold(0.0000)? ==> 0.1998
neuronOutput[5]: activation(0.1998) > threshold(0.0000)? ==> 0.1998
neuronOutput[6]: activation(0.9987) > threshold(0.0000)? ==> 0.9987
neuronOutput[7]: activation(0.9987) > threshold(0.0000)? ==> 0.9987
neuronOutput[8]: activation(0.9987) > threshold(0.0000)? ==> 0.9987
neuronOutput[9]: activation(0.9987) > threshold(0.0000)? ==> 0.9987
neuronOutput[10]: activation(0.9987) > threshold(0.0000)? ==> 0.9987
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.9987)
networkOutput[2] := neuronOutput[10](0.9987)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.0100) * output[3](0.1998) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.1998) * output[3](0.1998) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.0100) * output[4](0.1998) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.0100) * output[4](0.1998) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.1998) * output[4](0.1998) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.0100) * output[5](0.1998) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.1998) * output[5](0.1998) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=028 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0100) * weight[1,1](0.1000) => 0.1010
activation[2] = self(1.0100) * weight[2,2](0.1000) => 0.1010
activation[3] = self(0.1998) * weight[3,3](0.1000) => 0.0200
activation[4] = self(0.1998) * weight[4,4](0.1000) => 0.0200
activation[5] = self(0.1998) * weight[5,5](0.1000) => 0.0200
activation[6] = self(0.9987) * weight[6,6](0.1000) => 0.0999
activation[7] = self(0.9987) * weight[7,7](0.1000) => 0.0999
activation[8] = self(0.9987) * weight[8,8](0.1000) => 0.0999
activation[9] = self(0.9987) * weight[9,9](0.1000) => 0.0999
activation[10] = self(0.9987) * weight[10,10](0.1000) => 0.0999
  b. Update inputs from other neurons
activation[1] = self(0.1010) + network_input[1]=0.0000 => 0.1010
activation[2] = self(0.1010) + network_input[2]=0.0000 => 0.1010
activation[3] = self(0.0200) + output[1](1.0100) * 1.0000 => 1.0300
activation[3] = self(1.0300) + output[7](0.9987) * -0.5000 => 0.5306
activation[3] = self(0.5306) + output[8](0.9987) * -0.5000 => 0.0313
activation[4] = self(0.0200) + output[1](1.0100) * 0.5000 => 0.5250
activation[4] = self(0.5250) + output[2](1.0100) * 0.5000 => 1.0300
activation[4] = self(1.0300) + output[6](0.9987) * -0.5000 => 0.5306
activation[4] = self(0.5306) + output[8](0.9987) * -0.5000 => 0.0313
activation[5] = self(0.0200) + output[2](1.0100) * 1.0000 => 1.0300
activation[5] = self(1.0300) + output[6](0.9987) * -0.5000 => 0.5306
activation[5] = self(0.5306) + output[7](0.9987) * -0.5000 => 0.0313
activation[6] = self(0.0999) + output[3](0.1998) * 1.0000 => 0.2997
activation[7] = self(0.0999) + output[4](0.1998) * 1.0000 => 0.2997
activation[8] = self(0.0999) + output[5](0.1998) * 1.0000 => 0.2997
activation[9] = self(0.0999) + output[3](0.1998) * 0.7500 => 0.2498
activation[9] = self(0.2498) + output[4](0.1998) * 0.2500 => 0.2997
activation[10] = self(0.0999) + output[4](0.1998) * 0.2500 => 0.1498
activation[10] = self(0.1498) + output[5](0.1998) * 0.7500 => 0.2997
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.1010) > threshold(0.0000)? ==> 0.1010
neuronOutput[2]: activation(0.1010) > threshold(0.0000)? ==> 0.1010
neuronOutput[3]: activation(0.0313) > threshold(0.0000)? ==> 0.0313
neuronOutput[4]: activation(0.0313) > threshold(0.0000)? ==> 0.0313
neuronOutput[5]: activation(0.0313) > threshold(0.0000)? ==> 0.0313
neuronOutput[6]: activation(0.2997) > threshold(0.0000)? ==> 0.2997
neuronOutput[7]: activation(0.2997) > threshold(0.0000)? ==> 0.2997
neuronOutput[8]: activation(0.2997) > threshold(0.0000)? ==> 0.2997
neuronOutput[9]: activation(0.2997) > threshold(0.0000)? ==> 0.2997
neuronOutput[10]: activation(0.2997) > threshold(0.0000)? ==> 0.2997
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.2997)
networkOutput[2] := neuronOutput[10](0.2997)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.1010) * output[3](0.0313) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0313) * output[3](0.0313) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.1010) * output[4](0.0313) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.1010) * output[4](0.0313) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0313) * output[4](0.0313) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.1010) * output[5](0.0313) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0313) * output[5](0.0313) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=029 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.1010) * weight[1,1](0.1000) => 0.0101
activation[2] = self(0.1010) * weight[2,2](0.1000) => 0.0101
activation[3] = self(0.0313) * weight[3,3](0.1000) => 0.0031
activation[4] = self(0.0313) * weight[4,4](0.1000) => 0.0031
activation[5] = self(0.0313) * weight[5,5](0.1000) => 0.0031
activation[6] = self(0.2997) * weight[6,6](0.1000) => 0.0300
activation[7] = self(0.2997) * weight[7,7](0.1000) => 0.0300
activation[8] = self(0.2997) * weight[8,8](0.1000) => 0.0300
activation[9] = self(0.2997) * weight[9,9](0.1000) => 0.0300
activation[10] = self(0.2997) * weight[10,10](0.1000) => 0.0300
  b. Update inputs from other neurons
activation[1] = self(0.0101) + network_input[1]=0.0000 => 0.0101
activation[2] = self(0.0101) + network_input[2]=0.0000 => 0.0101
activation[3] = self(0.0031) + output[1](0.1010) * 1.0000 => 0.1041
activation[3] = self(0.1041) + output[7](0.2997) * -0.5000 => -0.0457
activation[3] = self(-0.0457) + output[8](0.2997) * -0.5000 => -0.1956
activation[4] = self(0.0031) + output[1](0.1010) * 0.5000 => 0.0536
activation[4] = self(0.0536) + output[2](0.1010) * 0.5000 => 0.1041
activation[4] = self(0.1041) + output[6](0.2997) * -0.5000 => -0.0457
activation[4] = self(-0.0457) + output[8](0.2997) * -0.5000 => -0.1956
activation[5] = self(0.0031) + output[2](0.1010) * 1.0000 => 0.1041
activation[5] = self(0.1041) + output[6](0.2997) * -0.5000 => -0.0457
activation[5] = self(-0.0457) + output[7](0.2997) * -0.5000 => -0.1956
activation[6] = self(0.0300) + output[3](0.0313) * 1.0000 => 0.0612
activation[7] = self(0.0300) + output[4](0.0313) * 1.0000 => 0.0612
activation[8] = self(0.0300) + output[5](0.0313) * 1.0000 => 0.0612
activation[9] = self(0.0300) + output[3](0.0313) * 0.7500 => 0.0534
activation[9] = self(0.0534) + output[4](0.0313) * 0.2500 => 0.0612
activation[10] = self(0.0300) + output[4](0.0313) * 0.2500 => 0.0378
activation[10] = self(0.0378) + output[5](0.0313) * 0.7500 => 0.0612
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0101) > threshold(0.0000)? ==> 0.0101
neuronOutput[2]: activation(0.0101) > threshold(0.0000)? ==> 0.0101
neuronOutput[3]: activation(-0.1956) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-0.1956) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-0.1956) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.0612) > threshold(0.0000)? ==> 0.0612
neuronOutput[7]: activation(0.0612) > threshold(0.0000)? ==> 0.0612
neuronOutput[8]: activation(0.0612) > threshold(0.0000)? ==> 0.0612
neuronOutput[9]: activation(0.0612) > threshold(0.0000)? ==> 0.0612
neuronOutput[10]: activation(0.0612) > threshold(0.0000)? ==> 0.0612
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0612)
networkOutput[2] := neuronOutput[10](0.0612)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.0101) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.0101) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.0101) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.0101) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=030 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0101) * weight[1,1](0.1000) => 0.0010
activation[2] = self(0.0101) * weight[2,2](0.1000) => 0.0010
activation[3] = self(-0.1956) * weight[3,3](0.1000) => -0.0196
activation[4] = self(-0.1956) * weight[4,4](0.1000) => -0.0196
activation[5] = self(-0.1956) * weight[5,5](0.1000) => -0.0196
activation[6] = self(0.0612) * weight[6,6](0.1000) => 0.0061
activation[7] = self(0.0612) * weight[7,7](0.1000) => 0.0061
activation[8] = self(0.0612) * weight[8,8](0.1000) => 0.0061
activation[9] = self(0.0612) * weight[9,9](0.1000) => 0.0061
activation[10] = self(0.0612) * weight[10,10](0.1000) => 0.0061
  b. Update inputs from other neurons
activation[1] = self(0.0010) + network_input[1]=0.0000 => 0.0010
activation[2] = self(0.0010) + network_input[2]=0.0000 => 0.0010
activation[3] = self(-0.0196) + output[1](0.0101) * 1.0000 => -0.0095
activation[3] = self(-0.0095) + output[7](0.0612) * -0.5000 => -0.0401
activation[3] = self(-0.0401) + output[8](0.0612) * -0.5000 => -0.0707
activation[4] = self(-0.0196) + output[1](0.0101) * 0.5000 => -0.0145
activation[4] = self(-0.0145) + output[2](0.0101) * 0.5000 => -0.0095
activation[4] = self(-0.0095) + output[6](0.0612) * -0.5000 => -0.0401
activation[4] = self(-0.0401) + output[8](0.0612) * -0.5000 => -0.0707
activation[5] = self(-0.0196) + output[2](0.0101) * 1.0000 => -0.0095
activation[5] = self(-0.0095) + output[6](0.0612) * -0.5000 => -0.0401
activation[5] = self(-0.0401) + output[7](0.0612) * -0.5000 => -0.0707
activation[6] = self(0.0061) + output[3](0.0000) * 1.0000 => 0.0061
activation[7] = self(0.0061) + output[4](0.0000) * 1.0000 => 0.0061
activation[8] = self(0.0061) + output[5](0.0000) * 1.0000 => 0.0061
activation[9] = self(0.0061) + output[3](0.0000) * 0.7500 => 0.0061
activation[9] = self(0.0061) + output[4](0.0000) * 0.2500 => 0.0061
activation[10] = self(0.0061) + output[4](0.0000) * 0.2500 => 0.0061
activation[10] = self(0.0061) + output[5](0.0000) * 0.7500 => 0.0061
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0010) > threshold(0.0000)? ==> 0.0010
neuronOutput[2]: activation(0.0010) > threshold(0.0000)? ==> 0.0010
neuronOutput[3]: activation(-0.0707) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-0.0707) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-0.0707) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.0061) > threshold(0.0000)? ==> 0.0061
neuronOutput[7]: activation(0.0061) > threshold(0.0000)? ==> 0.0061
neuronOutput[8]: activation(0.0061) > threshold(0.0000)? ==> 0.0061
neuronOutput[9]: activation(0.0061) > threshold(0.0000)? ==> 0.0061
neuronOutput[10]: activation(0.0061) > threshold(0.0000)? ==> 0.0061
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0061)
networkOutput[2] := neuronOutput[10](0.0061)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.0010) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.0010) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.0010) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.0010) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=031 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0010) * weight[1,1](0.1000) => 0.0001
activation[2] = self(0.0010) * weight[2,2](0.1000) => 0.0001
activation[3] = self(-0.0707) * weight[3,3](0.1000) => -0.0071
activation[4] = self(-0.0707) * weight[4,4](0.1000) => -0.0071
activation[5] = self(-0.0707) * weight[5,5](0.1000) => -0.0071
activation[6] = self(0.0061) * weight[6,6](0.1000) => 0.0006
activation[7] = self(0.0061) * weight[7,7](0.1000) => 0.0006
activation[8] = self(0.0061) * weight[8,8](0.1000) => 0.0006
activation[9] = self(0.0061) * weight[9,9](0.1000) => 0.0006
activation[10] = self(0.0061) * weight[10,10](0.1000) => 0.0006
  b. Update inputs from other neurons
activation[1] = self(0.0001) + network_input[1]=1.0000 => 1.0001
activation[2] = self(0.0001) + network_input[2]=1.0000 => 1.0001
activation[3] = self(-0.0071) + output[1](0.0010) * 1.0000 => -0.0061
activation[3] = self(-0.0061) + output[7](0.0061) * -0.5000 => -0.0091
activation[3] = self(-0.0091) + output[8](0.0061) * -0.5000 => -0.0122
activation[4] = self(-0.0071) + output[1](0.0010) * 0.5000 => -0.0066
activation[4] = self(-0.0066) + output[2](0.0010) * 0.5000 => -0.0061
activation[4] = self(-0.0061) + output[6](0.0061) * -0.5000 => -0.0091
activation[4] = self(-0.0091) + output[8](0.0061) * -0.5000 => -0.0122
activation[5] = self(-0.0071) + output[2](0.0010) * 1.0000 => -0.0061
activation[5] = self(-0.0061) + output[6](0.0061) * -0.5000 => -0.0091
activation[5] = self(-0.0091) + output[7](0.0061) * -0.5000 => -0.0122
activation[6] = self(0.0006) + output[3](0.0000) * 1.0000 => 0.0006
activation[7] = self(0.0006) + output[4](0.0000) * 1.0000 => 0.0006
activation[8] = self(0.0006) + output[5](0.0000) * 1.0000 => 0.0006
activation[9] = self(0.0006) + output[3](0.0000) * 0.7500 => 0.0006
activation[9] = self(0.0006) + output[4](0.0000) * 0.2500 => 0.0006
activation[10] = self(0.0006) + output[4](0.0000) * 0.2500 => 0.0006
activation[10] = self(0.0006) + output[5](0.0000) * 0.7500 => 0.0006
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0001) > threshold(0.0000)? ==> 1.0001
neuronOutput[2]: activation(1.0001) > threshold(0.0000)? ==> 1.0001
neuronOutput[3]: activation(-0.0122) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-0.0122) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-0.0122) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.0006) > threshold(0.0000)? ==> 0.0006
neuronOutput[7]: activation(0.0006) > threshold(0.0000)? ==> 0.0006
neuronOutput[8]: activation(0.0006) > threshold(0.0000)? ==> 0.0006
neuronOutput[9]: activation(0.0006) > threshold(0.0000)? ==> 0.0006
neuronOutput[10]: activation(0.0006) > threshold(0.0000)? ==> 0.0006
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0006)
networkOutput[2] := neuronOutput[10](0.0006)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.0001) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.0001) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.0001) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.0001) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=032 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0001) * weight[1,1](0.1000) => 0.1000
activation[2] = self(1.0001) * weight[2,2](0.1000) => 0.1000
activation[3] = self(-0.0122) * weight[3,3](0.1000) => -0.0012
activation[4] = self(-0.0122) * weight[4,4](0.1000) => -0.0012
activation[5] = self(-0.0122) * weight[5,5](0.1000) => -0.0012
activation[6] = self(0.0006) * weight[6,6](0.1000) => 0.0001
activation[7] = self(0.0006) * weight[7,7](0.1000) => 0.0001
activation[8] = self(0.0006) * weight[8,8](0.1000) => 0.0001
activation[9] = self(0.0006) * weight[9,9](0.1000) => 0.0001
activation[10] = self(0.0006) * weight[10,10](0.1000) => 0.0001
  b. Update inputs from other neurons
activation[1] = self(0.1000) + network_input[1]=1.0000 => 1.1000
activation[2] = self(0.1000) + network_input[2]=1.0000 => 1.1000
activation[3] = self(-0.0012) + output[1](1.0001) * 1.0000 => 0.9989
activation[3] = self(0.9989) + output[7](0.0006) * -0.5000 => 0.9986
activation[3] = self(0.9986) + output[8](0.0006) * -0.5000 => 0.9983
activation[4] = self(-0.0012) + output[1](1.0001) * 0.5000 => 0.4988
activation[4] = self(0.4988) + output[2](1.0001) * 0.5000 => 0.9989
activation[4] = self(0.9989) + output[6](0.0006) * -0.5000 => 0.9986
activation[4] = self(0.9986) + output[8](0.0006) * -0.5000 => 0.9983
activation[5] = self(-0.0012) + output[2](1.0001) * 1.0000 => 0.9989
activation[5] = self(0.9989) + output[6](0.0006) * -0.5000 => 0.9986
activation[5] = self(0.9986) + output[7](0.0006) * -0.5000 => 0.9983
activation[6] = self(0.0001) + output[3](0.0000) * 1.0000 => 0.0001
activation[7] = self(0.0001) + output[4](0.0000) * 1.0000 => 0.0001
activation[8] = self(0.0001) + output[5](0.0000) * 1.0000 => 0.0001
activation[9] = self(0.0001) + output[3](0.0000) * 0.7500 => 0.0001
activation[9] = self(0.0001) + output[4](0.0000) * 0.2500 => 0.0001
activation[10] = self(0.0001) + output[4](0.0000) * 0.2500 => 0.0001
activation[10] = self(0.0001) + output[5](0.0000) * 0.7500 => 0.0001
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.1000) > threshold(0.0000)? ==> 1.1000
neuronOutput[2]: activation(1.1000) > threshold(0.0000)? ==> 1.1000
neuronOutput[3]: activation(0.9983) > threshold(0.0000)? ==> 0.9983
neuronOutput[4]: activation(0.9983) > threshold(0.0000)? ==> 0.9983
neuronOutput[5]: activation(0.9983) > threshold(0.0000)? ==> 0.9983
neuronOutput[6]: activation(0.0001) > threshold(0.0000)? ==> 0.0001
neuronOutput[7]: activation(0.0001) > threshold(0.0000)? ==> 0.0001
neuronOutput[8]: activation(0.0001) > threshold(0.0000)? ==> 0.0001
neuronOutput[9]: activation(0.0001) > threshold(0.0000)? ==> 0.0001
neuronOutput[10]: activation(0.0001) > threshold(0.0000)? ==> 0.0001
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0001)
networkOutput[2] := neuronOutput[10](0.0001)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.1000) * output[3](0.9983) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.9983) * output[3](0.9983) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.1000) * output[4](0.9983) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.1000) * output[4](0.9983) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.9983) * output[4](0.9983) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.1000) * output[5](0.9983) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.9983) * output[5](0.9983) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=033 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.1000) * weight[1,1](0.1000) => 0.1100
activation[2] = self(1.1000) * weight[2,2](0.1000) => 0.1100
activation[3] = self(0.9983) * weight[3,3](0.1000) => 0.0998
activation[4] = self(0.9983) * weight[4,4](0.1000) => 0.0998
activation[5] = self(0.9983) * weight[5,5](0.1000) => 0.0998
activation[6] = self(0.0001) * weight[6,6](0.1000) => 0.0000
activation[7] = self(0.0001) * weight[7,7](0.1000) => 0.0000
activation[8] = self(0.0001) * weight[8,8](0.1000) => 0.0000
activation[9] = self(0.0001) * weight[9,9](0.1000) => 0.0000
activation[10] = self(0.0001) * weight[10,10](0.1000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.1100) + network_input[1]=0.0000 => 0.1100
activation[2] = self(0.1100) + network_input[2]=0.0000 => 0.1100
activation[3] = self(0.0998) + output[1](1.1000) * 1.0000 => 1.1998
activation[3] = self(1.1998) + output[7](0.0001) * -0.5000 => 1.1998
activation[3] = self(1.1998) + output[8](0.0001) * -0.5000 => 1.1998
activation[4] = self(0.0998) + output[1](1.1000) * 0.5000 => 0.6498
activation[4] = self(0.6498) + output[2](1.1000) * 0.5000 => 1.1998
activation[4] = self(1.1998) + output[6](0.0001) * -0.5000 => 1.1998
activation[4] = self(1.1998) + output[8](0.0001) * -0.5000 => 1.1998
activation[5] = self(0.0998) + output[2](1.1000) * 1.0000 => 1.1998
activation[5] = self(1.1998) + output[6](0.0001) * -0.5000 => 1.1998
activation[5] = self(1.1998) + output[7](0.0001) * -0.5000 => 1.1998
activation[6] = self(0.0000) + output[3](0.9983) * 1.0000 => 0.9983
activation[7] = self(0.0000) + output[4](0.9983) * 1.0000 => 0.9983
activation[8] = self(0.0000) + output[5](0.9983) * 1.0000 => 0.9983
activation[9] = self(0.0000) + output[3](0.9983) * 0.7500 => 0.7487
activation[9] = self(0.7487) + output[4](0.9983) * 0.2500 => 0.9983
activation[10] = self(0.0000) + output[4](0.9983) * 0.2500 => 0.2496
activation[10] = self(0.2496) + output[5](0.9983) * 0.7500 => 0.9983
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.1100) > threshold(0.0000)? ==> 0.1100
neuronOutput[2]: activation(0.1100) > threshold(0.0000)? ==> 0.1100
neuronOutput[3]: activation(1.1998) > threshold(0.0000)? ==> 1.1998
neuronOutput[4]: activation(1.1998) > threshold(0.0000)? ==> 1.1998
neuronOutput[5]: activation(1.1998) > threshold(0.0000)? ==> 1.1998
neuronOutput[6]: activation(0.9983) > threshold(0.0000)? ==> 0.9983
neuronOutput[7]: activation(0.9983) > threshold(0.0000)? ==> 0.9983
neuronOutput[8]: activation(0.9983) > threshold(0.0000)? ==> 0.9983
neuronOutput[9]: activation(0.9983) > threshold(0.0000)? ==> 0.9983
neuronOutput[10]: activation(0.9983) > threshold(0.0000)? ==> 0.9983
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.9983)
networkOutput[2] := neuronOutput[10](0.9983)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.1100) * output[3](1.1998) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](1.1998) * output[3](1.1998) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.1100) * output[4](1.1998) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.1100) * output[4](1.1998) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](1.1998) * output[4](1.1998) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.1100) * output[5](1.1998) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](1.1998) * output[5](1.1998) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=034 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.1100) * weight[1,1](0.1000) => 0.0110
activation[2] = self(0.1100) * weight[2,2](0.1000) => 0.0110
activation[3] = self(1.1998) * weight[3,3](0.1000) => 0.1200
activation[4] = self(1.1998) * weight[4,4](0.1000) => 0.1200
activation[5] = self(1.1998) * weight[5,5](0.1000) => 0.1200
activation[6] = self(0.9983) * weight[6,6](0.1000) => 0.0998
activation[7] = self(0.9983) * weight[7,7](0.1000) => 0.0998
activation[8] = self(0.9983) * weight[8,8](0.1000) => 0.0998
activation[9] = self(0.9983) * weight[9,9](0.1000) => 0.0998
activation[10] = self(0.9983) * weight[10,10](0.1000) => 0.0998
  b. Update inputs from other neurons
activation[1] = self(0.0110) + network_input[1]=1.0000 => 1.0110
activation[2] = self(0.0110) + network_input[2]=1.0000 => 1.0110
activation[3] = self(0.1200) + output[1](0.1100) * 1.0000 => 0.2300
activation[3] = self(0.2300) + output[7](0.9983) * -0.5000 => -0.2692
activation[3] = self(-0.2692) + output[8](0.9983) * -0.5000 => -0.7683
activation[4] = self(0.1200) + output[1](0.1100) * 0.5000 => 0.1750
activation[4] = self(0.1750) + output[2](0.1100) * 0.5000 => 0.2300
activation[4] = self(0.2300) + output[6](0.9983) * -0.5000 => -0.2692
activation[4] = self(-0.2692) + output[8](0.9983) * -0.5000 => -0.7683
activation[5] = self(0.1200) + output[2](0.1100) * 1.0000 => 0.2300
activation[5] = self(0.2300) + output[6](0.9983) * -0.5000 => -0.2692
activation[5] = self(-0.2692) + output[7](0.9983) * -0.5000 => -0.7683
activation[6] = self(0.0998) + output[3](1.1998) * 1.0000 => 1.2996
activation[7] = self(0.0998) + output[4](1.1998) * 1.0000 => 1.2996
activation[8] = self(0.0998) + output[5](1.1998) * 1.0000 => 1.2996
activation[9] = self(0.0998) + output[3](1.1998) * 0.7500 => 0.9997
activation[9] = self(0.9997) + output[4](1.1998) * 0.2500 => 1.2996
activation[10] = self(0.0998) + output[4](1.1998) * 0.2500 => 0.3998
activation[10] = self(0.3998) + output[5](1.1998) * 0.7500 => 1.2996
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0110) > threshold(0.0000)? ==> 1.0110
neuronOutput[2]: activation(1.0110) > threshold(0.0000)? ==> 1.0110
neuronOutput[3]: activation(-0.7683) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-0.7683) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-0.7683) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(1.2996) > threshold(0.0000)? ==> 1.2996
neuronOutput[7]: activation(1.2996) > threshold(0.0000)? ==> 1.2996
neuronOutput[8]: activation(1.2996) > threshold(0.0000)? ==> 1.2996
neuronOutput[9]: activation(1.2996) > threshold(0.0000)? ==> 1.2996
neuronOutput[10]: activation(1.2996) > threshold(0.0000)? ==> 1.2996
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](1.2996)
networkOutput[2] := neuronOutput[10](1.2996)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.0110) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.0110) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.0110) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.0110) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=035 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0110) * weight[1,1](0.1000) => 0.1011
activation[2] = self(1.0110) * weight[2,2](0.1000) => 0.1011
activation[3] = self(-0.7683) * weight[3,3](0.1000) => -0.0768
activation[4] = self(-0.7683) * weight[4,4](0.1000) => -0.0768
activation[5] = self(-0.7683) * weight[5,5](0.1000) => -0.0768
activation[6] = self(1.2996) * weight[6,6](0.1000) => 0.1300
activation[7] = self(1.2996) * weight[7,7](0.1000) => 0.1300
activation[8] = self(1.2996) * weight[8,8](0.1000) => 0.1300
activation[9] = self(1.2996) * weight[9,9](0.1000) => 0.1300
activation[10] = self(1.2996) * weight[10,10](0.1000) => 0.1300
  b. Update inputs from other neurons
activation[1] = self(0.1011) + network_input[1]=0.0000 => 0.1011
activation[2] = self(0.1011) + network_input[2]=0.0000 => 0.1011
activation[3] = self(-0.0768) + output[1](1.0110) * 1.0000 => 0.9342
activation[3] = self(0.9342) + output[7](1.2996) * -0.5000 => 0.2844
activation[3] = self(0.2844) + output[8](1.2996) * -0.5000 => -0.3654
activation[4] = self(-0.0768) + output[1](1.0110) * 0.5000 => 0.4287
activation[4] = self(0.4287) + output[2](1.0110) * 0.5000 => 0.9342
activation[4] = self(0.9342) + output[6](1.2996) * -0.5000 => 0.2844
activation[4] = self(0.2844) + output[8](1.2996) * -0.5000 => -0.3654
activation[5] = self(-0.0768) + output[2](1.0110) * 1.0000 => 0.9342
activation[5] = self(0.9342) + output[6](1.2996) * -0.5000 => 0.2844
activation[5] = self(0.2844) + output[7](1.2996) * -0.5000 => -0.3654
activation[6] = self(0.1300) + output[3](0.0000) * 1.0000 => 0.1300
activation[7] = self(0.1300) + output[4](0.0000) * 1.0000 => 0.1300
activation[8] = self(0.1300) + output[5](0.0000) * 1.0000 => 0.1300
activation[9] = self(0.1300) + output[3](0.0000) * 0.7500 => 0.1300
activation[9] = self(0.1300) + output[4](0.0000) * 0.2500 => 0.1300
activation[10] = self(0.1300) + output[4](0.0000) * 0.2500 => 0.1300
activation[10] = self(0.1300) + output[5](0.0000) * 0.7500 => 0.1300
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.1011) > threshold(0.0000)? ==> 0.1011
neuronOutput[2]: activation(0.1011) > threshold(0.0000)? ==> 0.1011
neuronOutput[3]: activation(-0.3654) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-0.3654) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-0.3654) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.1300) > threshold(0.0000)? ==> 0.1300
neuronOutput[7]: activation(0.1300) > threshold(0.0000)? ==> 0.1300
neuronOutput[8]: activation(0.1300) > threshold(0.0000)? ==> 0.1300
neuronOutput[9]: activation(0.1300) > threshold(0.0000)? ==> 0.1300
neuronOutput[10]: activation(0.1300) > threshold(0.0000)? ==> 0.1300
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.1300)
networkOutput[2] := neuronOutput[10](0.1300)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.1011) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.1011) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.1011) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.1011) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=036 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.1011) * weight[1,1](0.1000) => 0.0101
activation[2] = self(0.1011) * weight[2,2](0.1000) => 0.0101
activation[3] = self(-0.3654) * weight[3,3](0.1000) => -0.0365
activation[4] = self(-0.3654) * weight[4,4](0.1000) => -0.0365
activation[5] = self(-0.3654) * weight[5,5](0.1000) => -0.0365
activation[6] = self(0.1300) * weight[6,6](0.1000) => 0.0130
activation[7] = self(0.1300) * weight[7,7](0.1000) => 0.0130
activation[8] = self(0.1300) * weight[8,8](0.1000) => 0.0130
activation[9] = self(0.1300) * weight[9,9](0.1000) => 0.0130
activation[10] = self(0.1300) * weight[10,10](0.1000) => 0.0130
  b. Update inputs from other neurons
activation[1] = self(0.0101) + network_input[1]=0.0000 => 0.0101
activation[2] = self(0.0101) + network_input[2]=0.0000 => 0.0101
activation[3] = self(-0.0365) + output[1](0.1011) * 1.0000 => 0.0646
activation[3] = self(0.0646) + output[7](0.1300) * -0.5000 => -0.0004
activation[3] = self(-0.0004) + output[8](0.1300) * -0.5000 => -0.0654
activation[4] = self(-0.0365) + output[1](0.1011) * 0.5000 => 0.0140
activation[4] = self(0.0140) + output[2](0.1011) * 0.5000 => 0.0646
activation[4] = self(0.0646) + output[6](0.1300) * -0.5000 => -0.0004
activation[4] = self(-0.0004) + output[8](0.1300) * -0.5000 => -0.0654
activation[5] = self(-0.0365) + output[2](0.1011) * 1.0000 => 0.0646
activation[5] = self(0.0646) + output[6](0.1300) * -0.5000 => -0.0004
activation[5] = self(-0.0004) + output[7](0.1300) * -0.5000 => -0.0654
activation[6] = self(0.0130) + output[3](0.0000) * 1.0000 => 0.0130
activation[7] = self(0.0130) + output[4](0.0000) * 1.0000 => 0.0130
activation[8] = self(0.0130) + output[5](0.0000) * 1.0000 => 0.0130
activation[9] = self(0.0130) + output[3](0.0000) * 0.7500 => 0.0130
activation[9] = self(0.0130) + output[4](0.0000) * 0.2500 => 0.0130
activation[10] = self(0.0130) + output[4](0.0000) * 0.2500 => 0.0130
activation[10] = self(0.0130) + output[5](0.0000) * 0.7500 => 0.0130
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0101) > threshold(0.0000)? ==> 0.0101
neuronOutput[2]: activation(0.0101) > threshold(0.0000)? ==> 0.0101
neuronOutput[3]: activation(-0.0654) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-0.0654) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-0.0654) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.0130) > threshold(0.0000)? ==> 0.0130
neuronOutput[7]: activation(0.0130) > threshold(0.0000)? ==> 0.0130
neuronOutput[8]: activation(0.0130) > threshold(0.0000)? ==> 0.0130
neuronOutput[9]: activation(0.0130) > threshold(0.0000)? ==> 0.0130
neuronOutput[10]: activation(0.0130) > threshold(0.0000)? ==> 0.0130
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0130)
networkOutput[2] := neuronOutput[10](0.0130)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.0101) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.0101) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.0101) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.0101) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=037 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0101) * weight[1,1](0.1000) => 0.0010
activation[2] = self(0.0101) * weight[2,2](0.1000) => 0.0010
activation[3] = self(-0.0654) * weight[3,3](0.1000) => -0.0065
activation[4] = self(-0.0654) * weight[4,4](0.1000) => -0.0065
activation[5] = self(-0.0654) * weight[5,5](0.1000) => -0.0065
activation[6] = self(0.0130) * weight[6,6](0.1000) => 0.0013
activation[7] = self(0.0130) * weight[7,7](0.1000) => 0.0013
activation[8] = self(0.0130) * weight[8,8](0.1000) => 0.0013
activation[9] = self(0.0130) * weight[9,9](0.1000) => 0.0013
activation[10] = self(0.0130) * weight[10,10](0.1000) => 0.0013
  b. Update inputs from other neurons
activation[1] = self(0.0010) + network_input[1]=0.0000 => 0.0010
activation[2] = self(0.0010) + network_input[2]=0.0000 => 0.0010
activation[3] = self(-0.0065) + output[1](0.0101) * 1.0000 => 0.0036
activation[3] = self(0.0036) + output[7](0.0130) * -0.5000 => -0.0029
activation[3] = self(-0.0029) + output[8](0.0130) * -0.5000 => -0.0094
activation[4] = self(-0.0065) + output[1](0.0101) * 0.5000 => -0.0015
activation[4] = self(-0.0015) + output[2](0.0101) * 0.5000 => 0.0036
activation[4] = self(0.0036) + output[6](0.0130) * -0.5000 => -0.0029
activation[4] = self(-0.0029) + output[8](0.0130) * -0.5000 => -0.0094
activation[5] = self(-0.0065) + output[2](0.0101) * 1.0000 => 0.0036
activation[5] = self(0.0036) + output[6](0.0130) * -0.5000 => -0.0029
activation[5] = self(-0.0029) + output[7](0.0130) * -0.5000 => -0.0094
activation[6] = self(0.0013) + output[3](0.0000) * 1.0000 => 0.0013
activation[7] = self(0.0013) + output[4](0.0000) * 1.0000 => 0.0013
activation[8] = self(0.0013) + output[5](0.0000) * 1.0000 => 0.0013
activation[9] = self(0.0013) + output[3](0.0000) * 0.7500 => 0.0013
activation[9] = self(0.0013) + output[4](0.0000) * 0.2500 => 0.0013
activation[10] = self(0.0013) + output[4](0.0000) * 0.2500 => 0.0013
activation[10] = self(0.0013) + output[5](0.0000) * 0.7500 => 0.0013
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0010) > threshold(0.0000)? ==> 0.0010
neuronOutput[2]: activation(0.0010) > threshold(0.0000)? ==> 0.0010
neuronOutput[3]: activation(-0.0094) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-0.0094) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-0.0094) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.0013) > threshold(0.0000)? ==> 0.0013
neuronOutput[7]: activation(0.0013) > threshold(0.0000)? ==> 0.0013
neuronOutput[8]: activation(0.0013) > threshold(0.0000)? ==> 0.0013
neuronOutput[9]: activation(0.0013) > threshold(0.0000)? ==> 0.0013
neuronOutput[10]: activation(0.0013) > threshold(0.0000)? ==> 0.0013
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0013)
networkOutput[2] := neuronOutput[10](0.0013)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.0010) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.0010) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.0010) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.0010) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=038 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0010) * weight[1,1](0.1000) => 0.0001
activation[2] = self(0.0010) * weight[2,2](0.1000) => 0.0001
activation[3] = self(-0.0094) * weight[3,3](0.1000) => -0.0009
activation[4] = self(-0.0094) * weight[4,4](0.1000) => -0.0009
activation[5] = self(-0.0094) * weight[5,5](0.1000) => -0.0009
activation[6] = self(0.0013) * weight[6,6](0.1000) => 0.0001
activation[7] = self(0.0013) * weight[7,7](0.1000) => 0.0001
activation[8] = self(0.0013) * weight[8,8](0.1000) => 0.0001
activation[9] = self(0.0013) * weight[9,9](0.1000) => 0.0001
activation[10] = self(0.0013) * weight[10,10](0.1000) => 0.0001
  b. Update inputs from other neurons
activation[1] = self(0.0001) + network_input[1]=0.0000 => 0.0001
activation[2] = self(0.0001) + network_input[2]=0.0000 => 0.0001
activation[3] = self(-0.0009) + output[1](0.0010) * 1.0000 => 0.0001
activation[3] = self(0.0001) + output[7](0.0013) * -0.5000 => -0.0006
activation[3] = self(-0.0006) + output[8](0.0013) * -0.5000 => -0.0012
activation[4] = self(-0.0009) + output[1](0.0010) * 0.5000 => -0.0004
activation[4] = self(-0.0004) + output[2](0.0010) * 0.5000 => 0.0001
activation[4] = self(0.0001) + output[6](0.0013) * -0.5000 => -0.0006
activation[4] = self(-0.0006) + output[8](0.0013) * -0.5000 => -0.0012
activation[5] = self(-0.0009) + output[2](0.0010) * 1.0000 => 0.0001
activation[5] = self(0.0001) + output[6](0.0013) * -0.5000 => -0.0006
activation[5] = self(-0.0006) + output[7](0.0013) * -0.5000 => -0.0012
activation[6] = self(0.0001) + output[3](0.0000) * 1.0000 => 0.0001
activation[7] = self(0.0001) + output[4](0.0000) * 1.0000 => 0.0001
activation[8] = self(0.0001) + output[5](0.0000) * 1.0000 => 0.0001
activation[9] = self(0.0001) + output[3](0.0000) * 0.7500 => 0.0001
activation[9] = self(0.0001) + output[4](0.0000) * 0.2500 => 0.0001
activation[10] = self(0.0001) + output[4](0.0000) * 0.2500 => 0.0001
activation[10] = self(0.0001) + output[5](0.0000) * 0.7500 => 0.0001
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0001) > threshold(0.0000)? ==> 0.0001
neuronOutput[2]: activation(0.0001) > threshold(0.0000)? ==> 0.0001
neuronOutput[3]: activation(-0.0012) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-0.0012) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-0.0012) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.0001) > threshold(0.0000)? ==> 0.0001
neuronOutput[7]: activation(0.0001) > threshold(0.0000)? ==> 0.0001
neuronOutput[8]: activation(0.0001) > threshold(0.0000)? ==> 0.0001
neuronOutput[9]: activation(0.0001) > threshold(0.0000)? ==> 0.0001
neuronOutput[10]: activation(0.0001) > threshold(0.0000)? ==> 0.0001
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0001)
networkOutput[2] := neuronOutput[10](0.0001)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.0001) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.0001) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.0001) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.0001) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=039 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0001) * weight[1,1](0.1000) => 0.0000
activation[2] = self(0.0001) * weight[2,2](0.1000) => 0.0000
activation[3] = self(-0.0012) * weight[3,3](0.1000) => -0.0001
activation[4] = self(-0.0012) * weight[4,4](0.1000) => -0.0001
activation[5] = self(-0.0012) * weight[5,5](0.1000) => -0.0001
activation[6] = self(0.0001) * weight[6,6](0.1000) => 0.0000
activation[7] = self(0.0001) * weight[7,7](0.1000) => 0.0000
activation[8] = self(0.0001) * weight[8,8](0.1000) => 0.0000
activation[9] = self(0.0001) * weight[9,9](0.1000) => 0.0000
activation[10] = self(0.0001) * weight[10,10](0.1000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0001) + output[1](0.0001) * 1.0000 => -0.0000
activation[3] = self(-0.0000) + output[7](0.0001) * -0.5000 => -0.0001
activation[3] = self(-0.0001) + output[8](0.0001) * -0.5000 => -0.0002
activation[4] = self(-0.0001) + output[1](0.0001) * 0.5000 => -0.0001
activation[4] = self(-0.0001) + output[2](0.0001) * 0.5000 => -0.0000
activation[4] = self(-0.0000) + output[6](0.0001) * -0.5000 => -0.0001
activation[4] = self(-0.0001) + output[8](0.0001) * -0.5000 => -0.0002
activation[5] = self(-0.0001) + output[2](0.0001) * 1.0000 => -0.0000
activation[5] = self(-0.0000) + output[6](0.0001) * -0.5000 => -0.0001
activation[5] = self(-0.0001) + output[7](0.0001) * -0.5000 => -0.0002
activation[6] = self(0.0000) + output[3](0.0000) * 1.0000 => 0.0000
activation[7] = self(0.0000) + output[4](0.0000) * 1.0000 => 0.0000
activation[8] = self(0.0000) + output[5](0.0000) * 1.0000 => 0.0000
activation[9] = self(0.0000) + output[3](0.0000) * 0.7500 => 0.0000
activation[9] = self(0.0000) + output[4](0.0000) * 0.2500 => 0.0000
activation[10] = self(0.0000) + output[4](0.0000) * 0.2500 => 0.0000
activation[10] = self(0.0000) + output[5](0.0000) * 0.7500 => 0.0000
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(-0.0002) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-0.0002) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-0.0002) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0000)
networkOutput[2] := neuronOutput[10](0.0000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.0000) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.0000) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.0000) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.0000) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=040 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.1000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.1000) => 0.0000
activation[3] = self(-0.0002) * weight[3,3](0.1000) => -0.0000
activation[4] = self(-0.0002) * weight[4,4](0.1000) => -0.0000
activation[5] = self(-0.0002) * weight[5,5](0.1000) => -0.0000
activation[6] = self(0.0000) * weight[6,6](0.1000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.1000) => 0.0000
activation[8] = self(0.0000) * weight[8,8](0.1000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.1000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(-0.0000) + output[1](0.0000) * 1.0000 => -0.0000
activation[3] = self(-0.0000) + output[7](0.0000) * -0.5000 => -0.0000
activation[3] = self(-0.0000) + output[8](0.0000) * -0.5000 => -0.0000
activation[4] = self(-0.0000) + output[1](0.0000) * 0.5000 => -0.0000
activation[4] = self(-0.0000) + output[2](0.0000) * 0.5000 => -0.0000
activation[4] = self(-0.0000) + output[6](0.0000) * -0.5000 => -0.0000
activation[4] = self(-0.0000) + output[8](0.0000) * -0.5000 => -0.0000
activation[5] = self(-0.0000) + output[2](0.0000) * 1.0000 => -0.0000
activation[5] = self(-0.0000) + output[6](0.0000) * -0.5000 => -0.0000
activation[5] = self(-0.0000) + output[7](0.0000) * -0.5000 => -0.0000
activation[6] = self(0.0000) + output[3](0.0000) * 1.0000 => 0.0000
activation[7] = self(0.0000) + output[4](0.0000) * 1.0000 => 0.0000
activation[8] = self(0.0000) + output[5](0.0000) * 1.0000 => 0.0000
activation[9] = self(0.0000) + output[3](0.0000) * 0.7500 => 0.0000
activation[9] = self(0.0000) + output[4](0.0000) * 0.2500 => 0.0000
activation[10] = self(0.0000) + output[4](0.0000) * 0.2500 => 0.0000
activation[10] = self(0.0000) + output[5](0.0000) * 0.7500 => 0.0000
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(-0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0000)
networkOutput[2] := neuronOutput[10](0.0000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.0000) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.0000) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.0000) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.0000) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=041 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.1000) => 0.1000
activation[2] = self(1.0000) * weight[2,2](0.1000) => 0.1000
activation[3] = self(-0.0000) * weight[3,3](0.1000) => -0.0000
activation[4] = self(-0.0000) * weight[4,4](0.1000) => -0.0000
activation[5] = self(-0.0000) * weight[5,5](0.1000) => -0.0000
activation[6] = self(0.0000) * weight[6,6](0.1000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.1000) => 0.0000
activation[8] = self(0.0000) * weight[8,8](0.1000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.1000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.1000) + network_input[1]=1.0000 => 1.1000
activation[2] = self(0.1000) + network_input[2]=1.0000 => 1.1000
activation[3] = self(-0.0000) + output[1](1.0000) * 1.0000 => 1.0000
activation[3] = self(1.0000) + output[7](0.0000) * -0.5000 => 1.0000
activation[3] = self(1.0000) + output[8](0.0000) * -0.5000 => 1.0000
activation[4] = self(-0.0000) + output[1](1.0000) * 0.5000 => 0.5000
activation[4] = self(0.5000) + output[2](1.0000) * 0.5000 => 1.0000
activation[4] = self(1.0000) + output[6](0.0000) * -0.5000 => 1.0000
activation[4] = self(1.0000) + output[8](0.0000) * -0.5000 => 1.0000
activation[5] = self(-0.0000) + output[2](1.0000) * 1.0000 => 1.0000
activation[5] = self(1.0000) + output[6](0.0000) * -0.5000 => 1.0000
activation[5] = self(1.0000) + output[7](0.0000) * -0.5000 => 1.0000
activation[6] = self(0.0000) + output[3](0.0000) * 1.0000 => 0.0000
activation[7] = self(0.0000) + output[4](0.0000) * 1.0000 => 0.0000
activation[8] = self(0.0000) + output[5](0.0000) * 1.0000 => 0.0000
activation[9] = self(0.0000) + output[3](0.0000) * 0.7500 => 0.0000
activation[9] = self(0.0000) + output[4](0.0000) * 0.2500 => 0.0000
activation[10] = self(0.0000) + output[4](0.0000) * 0.2500 => 0.0000
activation[10] = self(0.0000) + output[5](0.0000) * 0.7500 => 0.0000
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.1000) > threshold(0.0000)? ==> 1.1000
neuronOutput[2]: activation(1.1000) > threshold(0.0000)? ==> 1.1000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[5]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0000)
networkOutput[2] := neuronOutput[10](0.0000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.1000) * output[3](1.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](1.0000) * output[3](1.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.1000) * output[4](1.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.1000) * output[4](1.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](1.0000) * output[4](1.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.1000) * output[5](1.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](1.0000) * output[5](1.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=042 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.1000) * weight[1,1](0.1000) => 0.1100
activation[2] = self(1.1000) * weight[2,2](0.1000) => 0.1100
activation[3] = self(1.0000) * weight[3,3](0.1000) => 0.1000
activation[4] = self(1.0000) * weight[4,4](0.1000) => 0.1000
activation[5] = self(1.0000) * weight[5,5](0.1000) => 0.1000
activation[6] = self(0.0000) * weight[6,6](0.1000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.1000) => 0.0000
activation[8] = self(0.0000) * weight[8,8](0.1000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.1000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.1100) + network_input[1]=0.0000 => 0.1100
activation[2] = self(0.1100) + network_input[2]=0.0000 => 0.1100
activation[3] = self(0.1000) + output[1](1.1000) * 1.0000 => 1.2000
activation[3] = self(1.2000) + output[7](0.0000) * -0.5000 => 1.2000
activation[3] = self(1.2000) + output[8](0.0000) * -0.5000 => 1.2000
activation[4] = self(0.1000) + output[1](1.1000) * 0.5000 => 0.6500
activation[4] = self(0.6500) + output[2](1.1000) * 0.5000 => 1.2000
activation[4] = self(1.2000) + output[6](0.0000) * -0.5000 => 1.2000
activation[4] = self(1.2000) + output[8](0.0000) * -0.5000 => 1.2000
activation[5] = self(0.1000) + output[2](1.1000) * 1.0000 => 1.2000
activation[5] = self(1.2000) + output[6](0.0000) * -0.5000 => 1.2000
activation[5] = self(1.2000) + output[7](0.0000) * -0.5000 => 1.2000
activation[6] = self(0.0000) + output[3](1.0000) * 1.0000 => 1.0000
activation[7] = self(0.0000) + output[4](1.0000) * 1.0000 => 1.0000
activation[8] = self(0.0000) + output[5](1.0000) * 1.0000 => 1.0000
activation[9] = self(0.0000) + output[3](1.0000) * 0.7500 => 0.7500
activation[9] = self(0.7500) + output[4](1.0000) * 0.2500 => 1.0000
activation[10] = self(0.0000) + output[4](1.0000) * 0.2500 => 0.2500
activation[10] = self(0.2500) + output[5](1.0000) * 0.7500 => 1.0000
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.1100) > threshold(0.0000)? ==> 0.1100
neuronOutput[2]: activation(0.1100) > threshold(0.0000)? ==> 0.1100
neuronOutput[3]: activation(1.2000) > threshold(0.0000)? ==> 1.2000
neuronOutput[4]: activation(1.2000) > threshold(0.0000)? ==> 1.2000
neuronOutput[5]: activation(1.2000) > threshold(0.0000)? ==> 1.2000
neuronOutput[6]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[7]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[8]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[9]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[10]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](1.0000)
networkOutput[2] := neuronOutput[10](1.0000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.1100) * output[3](1.2000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](1.2000) * output[3](1.2000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.1100) * output[4](1.2000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.1100) * output[4](1.2000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](1.2000) * output[4](1.2000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.1100) * output[5](1.2000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](1.2000) * output[5](1.2000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=043 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.1100) * weight[1,1](0.1000) => 0.0110
activation[2] = self(0.1100) * weight[2,2](0.1000) => 0.0110
activation[3] = self(1.2000) * weight[3,3](0.1000) => 0.1200
activation[4] = self(1.2000) * weight[4,4](0.1000) => 0.1200
activation[5] = self(1.2000) * weight[5,5](0.1000) => 0.1200
activation[6] = self(1.0000) * weight[6,6](0.1000) => 0.1000
activation[7] = self(1.0000) * weight[7,7](0.1000) => 0.1000
activation[8] = self(1.0000) * weight[8,8](0.1000) => 0.1000
activation[9] = self(1.0000) * weight[9,9](0.1000) => 0.1000
activation[10] = self(1.0000) * weight[10,10](0.1000) => 0.1000
  b. Update inputs from other neurons
activation[1] = self(0.0110) + network_input[1]=1.0000 => 1.0110
activation[2] = self(0.0110) + network_input[2]=1.0000 => 1.0110
activation[3] = self(0.1200) + output[1](0.1100) * 1.0000 => 0.2300
activation[3] = self(0.2300) + output[7](1.0000) * -0.5000 => -0.2700
activation[3] = self(-0.2700) + output[8](1.0000) * -0.5000 => -0.7700
activation[4] = self(0.1200) + output[1](0.1100) * 0.5000 => 0.1750
activation[4] = self(0.1750) + output[2](0.1100) * 0.5000 => 0.2300
activation[4] = self(0.2300) + output[6](1.0000) * -0.5000 => -0.2700
activation[4] = self(-0.2700) + output[8](1.0000) * -0.5000 => -0.7700
activation[5] = self(0.1200) + output[2](0.1100) * 1.0000 => 0.2300
activation[5] = self(0.2300) + output[6](1.0000) * -0.5000 => -0.2700
activation[5] = self(-0.2700) + output[7](1.0000) * -0.5000 => -0.7700
activation[6] = self(0.1000) + output[3](1.2000) * 1.0000 => 1.3000
activation[7] = self(0.1000) + output[4](1.2000) * 1.0000 => 1.3000
activation[8] = self(0.1000) + output[5](1.2000) * 1.0000 => 1.3000
activation[9] = self(0.1000) + output[3](1.2000) * 0.7500 => 1.0000
activation[9] = self(1.0000) + output[4](1.2000) * 0.2500 => 1.3000
activation[10] = self(0.1000) + output[4](1.2000) * 0.2500 => 0.4000
activation[10] = self(0.4000) + output[5](1.2000) * 0.7500 => 1.3000
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0110) > threshold(0.0000)? ==> 1.0110
neuronOutput[2]: activation(1.0110) > threshold(0.0000)? ==> 1.0110
neuronOutput[3]: activation(-0.7700) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-0.7700) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-0.7700) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(1.3000) > threshold(0.0000)? ==> 1.3000
neuronOutput[7]: activation(1.3000) > threshold(0.0000)? ==> 1.3000
neuronOutput[8]: activation(1.3000) > threshold(0.0000)? ==> 1.3000
neuronOutput[9]: activation(1.3000) > threshold(0.0000)? ==> 1.3000
neuronOutput[10]: activation(1.3000) > threshold(0.0000)? ==> 1.3000
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](1.3000)
networkOutput[2] := neuronOutput[10](1.3000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.0110) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.0110) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.0110) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.0110) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=044 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0110) * weight[1,1](0.1000) => 0.1011
activation[2] = self(1.0110) * weight[2,2](0.1000) => 0.1011
activation[3] = self(-0.7700) * weight[3,3](0.1000) => -0.0770
activation[4] = self(-0.7700) * weight[4,4](0.1000) => -0.0770
activation[5] = self(-0.7700) * weight[5,5](0.1000) => -0.0770
activation[6] = self(1.3000) * weight[6,6](0.1000) => 0.1300
activation[7] = self(1.3000) * weight[7,7](0.1000) => 0.1300
activation[8] = self(1.3000) * weight[8,8](0.1000) => 0.1300
activation[9] = self(1.3000) * weight[9,9](0.1000) => 0.1300
activation[10] = self(1.3000) * weight[10,10](0.1000) => 0.1300
  b. Update inputs from other neurons
activation[1] = self(0.1011) + network_input[1]=0.0000 => 0.1011
activation[2] = self(0.1011) + network_input[2]=0.0000 => 0.1011
activation[3] = self(-0.0770) + output[1](1.0110) * 1.0000 => 0.9340
activation[3] = self(0.9340) + output[7](1.3000) * -0.5000 => 0.2840
activation[3] = self(0.2840) + output[8](1.3000) * -0.5000 => -0.3660
activation[4] = self(-0.0770) + output[1](1.0110) * 0.5000 => 0.4285
activation[4] = self(0.4285) + output[2](1.0110) * 0.5000 => 0.9340
activation[4] = self(0.9340) + output[6](1.3000) * -0.5000 => 0.2840
activation[4] = self(0.2840) + output[8](1.3000) * -0.5000 => -0.3660
activation[5] = self(-0.0770) + output[2](1.0110) * 1.0000 => 0.9340
activation[5] = self(0.9340) + output[6](1.3000) * -0.5000 => 0.2840
activation[5] = self(0.2840) + output[7](1.3000) * -0.5000 => -0.3660
activation[6] = self(0.1300) + output[3](0.0000) * 1.0000 => 0.1300
activation[7] = self(0.1300) + output[4](0.0000) * 1.0000 => 0.1300
activation[8] = self(0.1300) + output[5](0.0000) * 1.0000 => 0.1300
activation[9] = self(0.1300) + output[3](0.0000) * 0.7500 => 0.1300
activation[9] = self(0.1300) + output[4](0.0000) * 0.2500 => 0.1300
activation[10] = self(0.1300) + output[4](0.0000) * 0.2500 => 0.1300
activation[10] = self(0.1300) + output[5](0.0000) * 0.7500 => 0.1300
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.1011) > threshold(0.0000)? ==> 0.1011
neuronOutput[2]: activation(0.1011) > threshold(0.0000)? ==> 0.1011
neuronOutput[3]: activation(-0.3660) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-0.3660) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-0.3660) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.1300) > threshold(0.0000)? ==> 0.1300
neuronOutput[7]: activation(0.1300) > threshold(0.0000)? ==> 0.1300
neuronOutput[8]: activation(0.1300) > threshold(0.0000)? ==> 0.1300
neuronOutput[9]: activation(0.1300) > threshold(0.0000)? ==> 0.1300
neuronOutput[10]: activation(0.1300) > threshold(0.0000)? ==> 0.1300
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.1300)
networkOutput[2] := neuronOutput[10](0.1300)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.1011) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.1011) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.1011) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.1011) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=045 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.1011) * weight[1,1](0.1000) => 0.0101
activation[2] = self(0.1011) * weight[2,2](0.1000) => 0.0101
activation[3] = self(-0.3660) * weight[3,3](0.1000) => -0.0366
activation[4] = self(-0.3660) * weight[4,4](0.1000) => -0.0366
activation[5] = self(-0.3660) * weight[5,5](0.1000) => -0.0366
activation[6] = self(0.1300) * weight[6,6](0.1000) => 0.0130
activation[7] = self(0.1300) * weight[7,7](0.1000) => 0.0130
activation[8] = self(0.1300) * weight[8,8](0.1000) => 0.0130
activation[9] = self(0.1300) * weight[9,9](0.1000) => 0.0130
activation[10] = self(0.1300) * weight[10,10](0.1000) => 0.0130
  b. Update inputs from other neurons
activation[1] = self(0.0101) + network_input[1]=0.0000 => 0.0101
activation[2] = self(0.0101) + network_input[2]=0.0000 => 0.0101
activation[3] = self(-0.0366) + output[1](0.1011) * 1.0000 => 0.0645
activation[3] = self(0.0645) + output[7](0.1300) * -0.5000 => -0.0005
activation[3] = self(-0.0005) + output[8](0.1300) * -0.5000 => -0.0655
activation[4] = self(-0.0366) + output[1](0.1011) * 0.5000 => 0.0140
activation[4] = self(0.0140) + output[2](0.1011) * 0.5000 => 0.0645
activation[4] = self(0.0645) + output[6](0.1300) * -0.5000 => -0.0005
activation[4] = self(-0.0005) + output[8](0.1300) * -0.5000 => -0.0655
activation[5] = self(-0.0366) + output[2](0.1011) * 1.0000 => 0.0645
activation[5] = self(0.0645) + output[6](0.1300) * -0.5000 => -0.0005
activation[5] = self(-0.0005) + output[7](0.1300) * -0.5000 => -0.0655
activation[6] = self(0.0130) + output[3](0.0000) * 1.0000 => 0.0130
activation[7] = self(0.0130) + output[4](0.0000) * 1.0000 => 0.0130
activation[8] = self(0.0130) + output[5](0.0000) * 1.0000 => 0.0130
activation[9] = self(0.0130) + output[3](0.0000) * 0.7500 => 0.0130
activation[9] = self(0.0130) + output[4](0.0000) * 0.2500 => 0.0130
activation[10] = self(0.0130) + output[4](0.0000) * 0.2500 => 0.0130
activation[10] = self(0.0130) + output[5](0.0000) * 0.7500 => 0.0130
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0101) > threshold(0.0000)? ==> 0.0101
neuronOutput[2]: activation(0.0101) > threshold(0.0000)? ==> 0.0101
neuronOutput[3]: activation(-0.0655) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-0.0655) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-0.0655) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.0130) > threshold(0.0000)? ==> 0.0130
neuronOutput[7]: activation(0.0130) > threshold(0.0000)? ==> 0.0130
neuronOutput[8]: activation(0.0130) > threshold(0.0000)? ==> 0.0130
neuronOutput[9]: activation(0.0130) > threshold(0.0000)? ==> 0.0130
neuronOutput[10]: activation(0.0130) > threshold(0.0000)? ==> 0.0130
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0130)
networkOutput[2] := neuronOutput[10](0.0130)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.0101) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.0101) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.0101) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.0101) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=046 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0101) * weight[1,1](0.1000) => 0.0010
activation[2] = self(0.0101) * weight[2,2](0.1000) => 0.0010
activation[3] = self(-0.0655) * weight[3,3](0.1000) => -0.0065
activation[4] = self(-0.0655) * weight[4,4](0.1000) => -0.0065
activation[5] = self(-0.0655) * weight[5,5](0.1000) => -0.0065
activation[6] = self(0.0130) * weight[6,6](0.1000) => 0.0013
activation[7] = self(0.0130) * weight[7,7](0.1000) => 0.0013
activation[8] = self(0.0130) * weight[8,8](0.1000) => 0.0013
activation[9] = self(0.0130) * weight[9,9](0.1000) => 0.0013
activation[10] = self(0.0130) * weight[10,10](0.1000) => 0.0013
  b. Update inputs from other neurons
activation[1] = self(0.0010) + network_input[1]=0.0000 => 0.0010
activation[2] = self(0.0010) + network_input[2]=0.0000 => 0.0010
activation[3] = self(-0.0065) + output[1](0.0101) * 1.0000 => 0.0036
activation[3] = self(0.0036) + output[7](0.0130) * -0.5000 => -0.0029
activation[3] = self(-0.0029) + output[8](0.0130) * -0.5000 => -0.0094
activation[4] = self(-0.0065) + output[1](0.0101) * 0.5000 => -0.0015
activation[4] = self(-0.0015) + output[2](0.0101) * 0.5000 => 0.0036
activation[4] = self(0.0036) + output[6](0.0130) * -0.5000 => -0.0029
activation[4] = self(-0.0029) + output[8](0.0130) * -0.5000 => -0.0094
activation[5] = self(-0.0065) + output[2](0.0101) * 1.0000 => 0.0036
activation[5] = self(0.0036) + output[6](0.0130) * -0.5000 => -0.0029
activation[5] = self(-0.0029) + output[7](0.0130) * -0.5000 => -0.0094
activation[6] = self(0.0013) + output[3](0.0000) * 1.0000 => 0.0013
activation[7] = self(0.0013) + output[4](0.0000) * 1.0000 => 0.0013
activation[8] = self(0.0013) + output[5](0.0000) * 1.0000 => 0.0013
activation[9] = self(0.0013) + output[3](0.0000) * 0.7500 => 0.0013
activation[9] = self(0.0013) + output[4](0.0000) * 0.2500 => 0.0013
activation[10] = self(0.0013) + output[4](0.0000) * 0.2500 => 0.0013
activation[10] = self(0.0013) + output[5](0.0000) * 0.7500 => 0.0013
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0010) > threshold(0.0000)? ==> 0.0010
neuronOutput[2]: activation(0.0010) > threshold(0.0000)? ==> 0.0010
neuronOutput[3]: activation(-0.0094) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-0.0094) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-0.0094) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.0013) > threshold(0.0000)? ==> 0.0013
neuronOutput[7]: activation(0.0013) > threshold(0.0000)? ==> 0.0013
neuronOutput[8]: activation(0.0013) > threshold(0.0000)? ==> 0.0013
neuronOutput[9]: activation(0.0013) > threshold(0.0000)? ==> 0.0013
neuronOutput[10]: activation(0.0013) > threshold(0.0000)? ==> 0.0013
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0013)
networkOutput[2] := neuronOutput[10](0.0013)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.0010) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.0010) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.0010) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.0010) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=047 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0010) * weight[1,1](0.1000) => 0.0001
activation[2] = self(0.0010) * weight[2,2](0.1000) => 0.0001
activation[3] = self(-0.0094) * weight[3,3](0.1000) => -0.0009
activation[4] = self(-0.0094) * weight[4,4](0.1000) => -0.0009
activation[5] = self(-0.0094) * weight[5,5](0.1000) => -0.0009
activation[6] = self(0.0013) * weight[6,6](0.1000) => 0.0001
activation[7] = self(0.0013) * weight[7,7](0.1000) => 0.0001
activation[8] = self(0.0013) * weight[8,8](0.1000) => 0.0001
activation[9] = self(0.0013) * weight[9,9](0.1000) => 0.0001
activation[10] = self(0.0013) * weight[10,10](0.1000) => 0.0001
  b. Update inputs from other neurons
activation[1] = self(0.0001) + network_input[1]=1.0000 => 1.0001
activation[2] = self(0.0001) + network_input[2]=1.0000 => 1.0001
activation[3] = self(-0.0009) + output[1](0.0010) * 1.0000 => 0.0001
activation[3] = self(0.0001) + output[7](0.0013) * -0.5000 => -0.0006
activation[3] = self(-0.0006) + output[8](0.0013) * -0.5000 => -0.0012
activation[4] = self(-0.0009) + output[1](0.0010) * 0.5000 => -0.0004
activation[4] = self(-0.0004) + output[2](0.0010) * 0.5000 => 0.0001
activation[4] = self(0.0001) + output[6](0.0013) * -0.5000 => -0.0006
activation[4] = self(-0.0006) + output[8](0.0013) * -0.5000 => -0.0012
activation[5] = self(-0.0009) + output[2](0.0010) * 1.0000 => 0.0001
activation[5] = self(0.0001) + output[6](0.0013) * -0.5000 => -0.0006
activation[5] = self(-0.0006) + output[7](0.0013) * -0.5000 => -0.0012
activation[6] = self(0.0001) + output[3](0.0000) * 1.0000 => 0.0001
activation[7] = self(0.0001) + output[4](0.0000) * 1.0000 => 0.0001
activation[8] = self(0.0001) + output[5](0.0000) * 1.0000 => 0.0001
activation[9] = self(0.0001) + output[3](0.0000) * 0.7500 => 0.0001
activation[9] = self(0.0001) + output[4](0.0000) * 0.2500 => 0.0001
activation[10] = self(0.0001) + output[4](0.0000) * 0.2500 => 0.0001
activation[10] = self(0.0001) + output[5](0.0000) * 0.7500 => 0.0001
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0001) > threshold(0.0000)? ==> 1.0001
neuronOutput[2]: activation(1.0001) > threshold(0.0000)? ==> 1.0001
neuronOutput[3]: activation(-0.0012) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-0.0012) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-0.0012) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.0001) > threshold(0.0000)? ==> 0.0001
neuronOutput[7]: activation(0.0001) > threshold(0.0000)? ==> 0.0001
neuronOutput[8]: activation(0.0001) > threshold(0.0000)? ==> 0.0001
neuronOutput[9]: activation(0.0001) > threshold(0.0000)? ==> 0.0001
neuronOutput[10]: activation(0.0001) > threshold(0.0000)? ==> 0.0001
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0001)
networkOutput[2] := neuronOutput[10](0.0001)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.0001) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.0001) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.0001) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.0001) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=048 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0001) * weight[1,1](0.1000) => 0.1000
activation[2] = self(1.0001) * weight[2,2](0.1000) => 0.1000
activation[3] = self(-0.0012) * weight[3,3](0.1000) => -0.0001
activation[4] = self(-0.0012) * weight[4,4](0.1000) => -0.0001
activation[5] = self(-0.0012) * weight[5,5](0.1000) => -0.0001
activation[6] = self(0.0001) * weight[6,6](0.1000) => 0.0000
activation[7] = self(0.0001) * weight[7,7](0.1000) => 0.0000
activation[8] = self(0.0001) * weight[8,8](0.1000) => 0.0000
activation[9] = self(0.0001) * weight[9,9](0.1000) => 0.0000
activation[10] = self(0.0001) * weight[10,10](0.1000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.1000) + network_input[1]=0.0000 => 0.1000
activation[2] = self(0.1000) + network_input[2]=0.0000 => 0.1000
activation[3] = self(-0.0001) + output[1](1.0001) * 1.0000 => 1.0000
activation[3] = self(1.0000) + output[7](0.0001) * -0.5000 => 0.9999
activation[3] = self(0.9999) + output[8](0.0001) * -0.5000 => 0.9998
activation[4] = self(-0.0001) + output[1](1.0001) * 0.5000 => 0.4999
activation[4] = self(0.4999) + output[2](1.0001) * 0.5000 => 1.0000
activation[4] = self(1.0000) + output[6](0.0001) * -0.5000 => 0.9999
activation[4] = self(0.9999) + output[8](0.0001) * -0.5000 => 0.9998
activation[5] = self(-0.0001) + output[2](1.0001) * 1.0000 => 1.0000
activation[5] = self(1.0000) + output[6](0.0001) * -0.5000 => 0.9999
activation[5] = self(0.9999) + output[7](0.0001) * -0.5000 => 0.9998
activation[6] = self(0.0000) + output[3](0.0000) * 1.0000 => 0.0000
activation[7] = self(0.0000) + output[4](0.0000) * 1.0000 => 0.0000
activation[8] = self(0.0000) + output[5](0.0000) * 1.0000 => 0.0000
activation[9] = self(0.0000) + output[3](0.0000) * 0.7500 => 0.0000
activation[9] = self(0.0000) + output[4](0.0000) * 0.2500 => 0.0000
activation[10] = self(0.0000) + output[4](0.0000) * 0.2500 => 0.0000
activation[10] = self(0.0000) + output[5](0.0000) * 0.7500 => 0.0000
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.1000) > threshold(0.0000)? ==> 0.1000
neuronOutput[2]: activation(0.1000) > threshold(0.0000)? ==> 0.1000
neuronOutput[3]: activation(0.9998) > threshold(0.0000)? ==> 0.9998
neuronOutput[4]: activation(0.9998) > threshold(0.0000)? ==> 0.9998
neuronOutput[5]: activation(0.9998) > threshold(0.0000)? ==> 0.9998
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0000)
networkOutput[2] := neuronOutput[10](0.0000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.1000) * output[3](0.9998) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.9998) * output[3](0.9998) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.1000) * output[4](0.9998) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.1000) * output[4](0.9998) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.9998) * output[4](0.9998) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.1000) * output[5](0.9998) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.9998) * output[5](0.9998) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=049 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.1000) * weight[1,1](0.1000) => 0.0100
activation[2] = self(0.1000) * weight[2,2](0.1000) => 0.0100
activation[3] = self(0.9998) * weight[3,3](0.1000) => 0.1000
activation[4] = self(0.9998) * weight[4,4](0.1000) => 0.1000
activation[5] = self(0.9998) * weight[5,5](0.1000) => 0.1000
activation[6] = self(0.0000) * weight[6,6](0.1000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.1000) => 0.0000
activation[8] = self(0.0000) * weight[8,8](0.1000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.1000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0100) + network_input[1]=1.0000 => 1.0100
activation[2] = self(0.0100) + network_input[2]=1.0000 => 1.0100
activation[3] = self(0.1000) + output[1](0.1000) * 1.0000 => 0.2000
activation[3] = self(0.2000) + output[7](0.0000) * -0.5000 => 0.2000
activation[3] = self(0.2000) + output[8](0.0000) * -0.5000 => 0.2000
activation[4] = self(0.1000) + output[1](0.1000) * 0.5000 => 0.1500
activation[4] = self(0.1500) + output[2](0.1000) * 0.5000 => 0.2000
activation[4] = self(0.2000) + output[6](0.0000) * -0.5000 => 0.2000
activation[4] = self(0.2000) + output[8](0.0000) * -0.5000 => 0.2000
activation[5] = self(0.1000) + output[2](0.1000) * 1.0000 => 0.2000
activation[5] = self(0.2000) + output[6](0.0000) * -0.5000 => 0.2000
activation[5] = self(0.2000) + output[7](0.0000) * -0.5000 => 0.2000
activation[6] = self(0.0000) + output[3](0.9998) * 1.0000 => 0.9998
activation[7] = self(0.0000) + output[4](0.9998) * 1.0000 => 0.9998
activation[8] = self(0.0000) + output[5](0.9998) * 1.0000 => 0.9998
activation[9] = self(0.0000) + output[3](0.9998) * 0.7500 => 0.7499
activation[9] = self(0.7499) + output[4](0.9998) * 0.2500 => 0.9998
activation[10] = self(0.0000) + output[4](0.9998) * 0.2500 => 0.2500
activation[10] = self(0.2500) + output[5](0.9998) * 0.7500 => 0.9998
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0100) > threshold(0.0000)? ==> 1.0100
neuronOutput[2]: activation(1.0100) > threshold(0.0000)? ==> 1.0100
neuronOutput[3]: activation(0.2000) > threshold(0.0000)? ==> 0.2000
neuronOutput[4]: activation(0.2000) > threshold(0.0000)? ==> 0.2000
neuronOutput[5]: activation(0.2000) > threshold(0.0000)? ==> 0.2000
neuronOutput[6]: activation(0.9998) > threshold(0.0000)? ==> 0.9998
neuronOutput[7]: activation(0.9998) > threshold(0.0000)? ==> 0.9998
neuronOutput[8]: activation(0.9998) > threshold(0.0000)? ==> 0.9998
neuronOutput[9]: activation(0.9998) > threshold(0.0000)? ==> 0.9998
neuronOutput[10]: activation(0.9998) > threshold(0.0000)? ==> 0.9998
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.9998)
networkOutput[2] := neuronOutput[10](0.9998)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.0100) * output[3](0.2000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.2000) * output[3](0.2000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.0100) * output[4](0.2000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.0100) * output[4](0.2000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.2000) * output[4](0.2000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.0100) * output[5](0.2000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.2000) * output[5](0.2000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=050 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0100) * weight[1,1](0.1000) => 0.1010
activation[2] = self(1.0100) * weight[2,2](0.1000) => 0.1010
activation[3] = self(0.2000) * weight[3,3](0.1000) => 0.0200
activation[4] = self(0.2000) * weight[4,4](0.1000) => 0.0200
activation[5] = self(0.2000) * weight[5,5](0.1000) => 0.0200
activation[6] = self(0.9998) * weight[6,6](0.1000) => 0.1000
activation[7] = self(0.9998) * weight[7,7](0.1000) => 0.1000
activation[8] = self(0.9998) * weight[8,8](0.1000) => 0.1000
activation[9] = self(0.9998) * weight[9,9](0.1000) => 0.1000
activation[10] = self(0.9998) * weight[10,10](0.1000) => 0.1000
  b. Update inputs from other neurons
activation[1] = self(0.1010) + network_input[1]=1.0000 => 1.1010
activation[2] = self(0.1010) + network_input[2]=1.0000 => 1.1010
activation[3] = self(0.0200) + output[1](1.0100) * 1.0000 => 1.0300
activation[3] = self(1.0300) + output[7](0.9998) * -0.5000 => 0.5301
activation[3] = self(0.5301) + output[8](0.9998) * -0.5000 => 0.0302
activation[4] = self(0.0200) + output[1](1.0100) * 0.5000 => 0.5250
activation[4] = self(0.5250) + output[2](1.0100) * 0.5000 => 1.0300
activation[4] = self(1.0300) + output[6](0.9998) * -0.5000 => 0.5301
activation[4] = self(0.5301) + output[8](0.9998) * -0.5000 => 0.0302
activation[5] = self(0.0200) + output[2](1.0100) * 1.0000 => 1.0300
activation[5] = self(1.0300) + output[6](0.9998) * -0.5000 => 0.5301
activation[5] = self(0.5301) + output[7](0.9998) * -0.5000 => 0.0302
activation[6] = self(0.1000) + output[3](0.2000) * 1.0000 => 0.3000
activation[7] = self(0.1000) + output[4](0.2000) * 1.0000 => 0.3000
activation[8] = self(0.1000) + output[5](0.2000) * 1.0000 => 0.3000
activation[9] = self(0.1000) + output[3](0.2000) * 0.7500 => 0.2500
activation[9] = self(0.2500) + output[4](0.2000) * 0.2500 => 0.3000
activation[10] = self(0.1000) + output[4](0.2000) * 0.2500 => 0.1500
activation[10] = self(0.1500) + output[5](0.2000) * 0.7500 => 0.3000
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.1010) > threshold(0.0000)? ==> 1.1010
neuronOutput[2]: activation(1.1010) > threshold(0.0000)? ==> 1.1010
neuronOutput[3]: activation(0.0302) > threshold(0.0000)? ==> 0.0302
neuronOutput[4]: activation(0.0302) > threshold(0.0000)? ==> 0.0302
neuronOutput[5]: activation(0.0302) > threshold(0.0000)? ==> 0.0302
neuronOutput[6]: activation(0.3000) > threshold(0.0000)? ==> 0.3000
neuronOutput[7]: activation(0.3000) > threshold(0.0000)? ==> 0.3000
neuronOutput[8]: activation(0.3000) > threshold(0.0000)? ==> 0.3000
neuronOutput[9]: activation(0.3000) > threshold(0.0000)? ==> 0.3000
neuronOutput[10]: activation(0.3000) > threshold(0.0000)? ==> 0.3000
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.3000)
networkOutput[2] := neuronOutput[10](0.3000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.1010) * output[3](0.0302) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0302) * output[3](0.0302) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.1010) * output[4](0.0302) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.1010) * output[4](0.0302) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0302) * output[4](0.0302) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.1010) * output[5](0.0302) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0302) * output[5](0.0302) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=051 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.1010) * weight[1,1](0.1000) => 0.1101
activation[2] = self(1.1010) * weight[2,2](0.1000) => 0.1101
activation[3] = self(0.0302) * weight[3,3](0.1000) => 0.0030
activation[4] = self(0.0302) * weight[4,4](0.1000) => 0.0030
activation[5] = self(0.0302) * weight[5,5](0.1000) => 0.0030
activation[6] = self(0.3000) * weight[6,6](0.1000) => 0.0300
activation[7] = self(0.3000) * weight[7,7](0.1000) => 0.0300
activation[8] = self(0.3000) * weight[8,8](0.1000) => 0.0300
activation[9] = self(0.3000) * weight[9,9](0.1000) => 0.0300
activation[10] = self(0.3000) * weight[10,10](0.1000) => 0.0300
  b. Update inputs from other neurons
activation[1] = self(0.1101) + network_input[1]=0.0000 => 0.1101
activation[2] = self(0.1101) + network_input[2]=0.0000 => 0.1101
activation[3] = self(0.0030) + output[1](1.1010) * 1.0000 => 1.1040
activation[3] = self(1.1040) + output[7](0.3000) * -0.5000 => 0.9540
activation[3] = self(0.9540) + output[8](0.3000) * -0.5000 => 0.8040
activation[4] = self(0.0030) + output[1](1.1010) * 0.5000 => 0.5535
activation[4] = self(0.5535) + output[2](1.1010) * 0.5000 => 1.1040
activation[4] = self(1.1040) + output[6](0.3000) * -0.5000 => 0.9540
activation[4] = self(0.9540) + output[8](0.3000) * -0.5000 => 0.8040
activation[5] = self(0.0030) + output[2](1.1010) * 1.0000 => 1.1040
activation[5] = self(1.1040) + output[6](0.3000) * -0.5000 => 0.9540
activation[5] = self(0.9540) + output[7](0.3000) * -0.5000 => 0.8040
activation[6] = self(0.0300) + output[3](0.0302) * 1.0000 => 0.0601
activation[7] = self(0.0300) + output[4](0.0302) * 1.0000 => 0.0601
activation[8] = self(0.0300) + output[5](0.0302) * 1.0000 => 0.0601
activation[9] = self(0.0300) + output[3](0.0302) * 0.7500 => 0.0526
activation[9] = self(0.0526) + output[4](0.0302) * 0.2500 => 0.0601
activation[10] = self(0.0300) + output[4](0.0302) * 0.2500 => 0.0375
activation[10] = self(0.0375) + output[5](0.0302) * 0.7500 => 0.0601
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.1101) > threshold(0.0000)? ==> 0.1101
neuronOutput[2]: activation(0.1101) > threshold(0.0000)? ==> 0.1101
neuronOutput[3]: activation(0.8040) > threshold(0.0000)? ==> 0.8040
neuronOutput[4]: activation(0.8040) > threshold(0.0000)? ==> 0.8040
neuronOutput[5]: activation(0.8040) > threshold(0.0000)? ==> 0.8040
neuronOutput[6]: activation(0.0601) > threshold(0.0000)? ==> 0.0601
neuronOutput[7]: activation(0.0601) > threshold(0.0000)? ==> 0.0601
neuronOutput[8]: activation(0.0601) > threshold(0.0000)? ==> 0.0601
neuronOutput[9]: activation(0.0601) > threshold(0.0000)? ==> 0.0601
neuronOutput[10]: activation(0.0601) > threshold(0.0000)? ==> 0.0601
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0601)
networkOutput[2] := neuronOutput[10](0.0601)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.1101) * output[3](0.8040) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.8040) * output[3](0.8040) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.1101) * output[4](0.8040) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.1101) * output[4](0.8040) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.8040) * output[4](0.8040) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.1101) * output[5](0.8040) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.8040) * output[5](0.8040) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=052 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.1101) * weight[1,1](0.1000) => 0.0110
activation[2] = self(0.1101) * weight[2,2](0.1000) => 0.0110
activation[3] = self(0.8040) * weight[3,3](0.1000) => 0.0804
activation[4] = self(0.8040) * weight[4,4](0.1000) => 0.0804
activation[5] = self(0.8040) * weight[5,5](0.1000) => 0.0804
activation[6] = self(0.0601) * weight[6,6](0.1000) => 0.0060
activation[7] = self(0.0601) * weight[7,7](0.1000) => 0.0060
activation[8] = self(0.0601) * weight[8,8](0.1000) => 0.0060
activation[9] = self(0.0601) * weight[9,9](0.1000) => 0.0060
activation[10] = self(0.0601) * weight[10,10](0.1000) => 0.0060
  b. Update inputs from other neurons
activation[1] = self(0.0110) + network_input[1]=1.0000 => 1.0110
activation[2] = self(0.0110) + network_input[2]=1.0000 => 1.0110
activation[3] = self(0.0804) + output[1](0.1101) * 1.0000 => 0.1905
activation[3] = self(0.1905) + output[7](0.0601) * -0.5000 => 0.1604
activation[3] = self(0.1604) + output[8](0.0601) * -0.5000 => 0.1304
activation[4] = self(0.0804) + output[1](0.1101) * 0.5000 => 0.1355
activation[4] = self(0.1355) + output[2](0.1101) * 0.5000 => 0.1905
activation[4] = self(0.1905) + output[6](0.0601) * -0.5000 => 0.1604
activation[4] = self(0.1604) + output[8](0.0601) * -0.5000 => 0.1304
activation[5] = self(0.0804) + output[2](0.1101) * 1.0000 => 0.1905
activation[5] = self(0.1905) + output[6](0.0601) * -0.5000 => 0.1604
activation[5] = self(0.1604) + output[7](0.0601) * -0.5000 => 0.1304
activation[6] = self(0.0060) + output[3](0.8040) * 1.0000 => 0.8101
activation[7] = self(0.0060) + output[4](0.8040) * 1.0000 => 0.8101
activation[8] = self(0.0060) + output[5](0.8040) * 1.0000 => 0.8101
activation[9] = self(0.0060) + output[3](0.8040) * 0.7500 => 0.6091
activation[9] = self(0.6091) + output[4](0.8040) * 0.2500 => 0.8101
activation[10] = self(0.0060) + output[4](0.8040) * 0.2500 => 0.2070
activation[10] = self(0.2070) + output[5](0.8040) * 0.7500 => 0.8101
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0110) > threshold(0.0000)? ==> 1.0110
neuronOutput[2]: activation(1.0110) > threshold(0.0000)? ==> 1.0110
neuronOutput[3]: activation(0.1304) > threshold(0.0000)? ==> 0.1304
neuronOutput[4]: activation(0.1304) > threshold(0.0000)? ==> 0.1304
neuronOutput[5]: activation(0.1304) > threshold(0.0000)? ==> 0.1304
neuronOutput[6]: activation(0.8101) > threshold(0.0000)? ==> 0.8101
neuronOutput[7]: activation(0.8101) > threshold(0.0000)? ==> 0.8101
neuronOutput[8]: activation(0.8101) > threshold(0.0000)? ==> 0.8101
neuronOutput[9]: activation(0.8101) > threshold(0.0000)? ==> 0.8101
neuronOutput[10]: activation(0.8101) > threshold(0.0000)? ==> 0.8101
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.8101)
networkOutput[2] := neuronOutput[10](0.8101)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.0110) * output[3](0.1304) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.1304) * output[3](0.1304) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.0110) * output[4](0.1304) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.0110) * output[4](0.1304) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.1304) * output[4](0.1304) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.0110) * output[5](0.1304) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.1304) * output[5](0.1304) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=053 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0110) * weight[1,1](0.1000) => 0.1011
activation[2] = self(1.0110) * weight[2,2](0.1000) => 0.1011
activation[3] = self(0.1304) * weight[3,3](0.1000) => 0.0130
activation[4] = self(0.1304) * weight[4,4](0.1000) => 0.0130
activation[5] = self(0.1304) * weight[5,5](0.1000) => 0.0130
activation[6] = self(0.8101) * weight[6,6](0.1000) => 0.0810
activation[7] = self(0.8101) * weight[7,7](0.1000) => 0.0810
activation[8] = self(0.8101) * weight[8,8](0.1000) => 0.0810
activation[9] = self(0.8101) * weight[9,9](0.1000) => 0.0810
activation[10] = self(0.8101) * weight[10,10](0.1000) => 0.0810
  b. Update inputs from other neurons
activation[1] = self(0.1011) + network_input[1]=0.0000 => 0.1011
activation[2] = self(0.1011) + network_input[2]=0.0000 => 0.1011
activation[3] = self(0.0130) + output[1](1.0110) * 1.0000 => 1.0240
activation[3] = self(1.0240) + output[7](0.8101) * -0.5000 => 0.6190
activation[3] = self(0.6190) + output[8](0.8101) * -0.5000 => 0.2140
activation[4] = self(0.0130) + output[1](1.0110) * 0.5000 => 0.5185
activation[4] = self(0.5185) + output[2](1.0110) * 0.5000 => 1.0240
activation[4] = self(1.0240) + output[6](0.8101) * -0.5000 => 0.6190
activation[4] = self(0.6190) + output[8](0.8101) * -0.5000 => 0.2140
activation[5] = self(0.0130) + output[2](1.0110) * 1.0000 => 1.0240
activation[5] = self(1.0240) + output[6](0.8101) * -0.5000 => 0.6190
activation[5] = self(0.6190) + output[7](0.8101) * -0.5000 => 0.2140
activation[6] = self(0.0810) + output[3](0.1304) * 1.0000 => 0.2114
activation[7] = self(0.0810) + output[4](0.1304) * 1.0000 => 0.2114
activation[8] = self(0.0810) + output[5](0.1304) * 1.0000 => 0.2114
activation[9] = self(0.0810) + output[3](0.1304) * 0.7500 => 0.1788
activation[9] = self(0.1788) + output[4](0.1304) * 0.2500 => 0.2114
activation[10] = self(0.0810) + output[4](0.1304) * 0.2500 => 0.1136
activation[10] = self(0.1136) + output[5](0.1304) * 0.7500 => 0.2114
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.1011) > threshold(0.0000)? ==> 0.1011
neuronOutput[2]: activation(0.1011) > threshold(0.0000)? ==> 0.1011
neuronOutput[3]: activation(0.2140) > threshold(0.0000)? ==> 0.2140
neuronOutput[4]: activation(0.2140) > threshold(0.0000)? ==> 0.2140
neuronOutput[5]: activation(0.2140) > threshold(0.0000)? ==> 0.2140
neuronOutput[6]: activation(0.2114) > threshold(0.0000)? ==> 0.2114
neuronOutput[7]: activation(0.2114) > threshold(0.0000)? ==> 0.2114
neuronOutput[8]: activation(0.2114) > threshold(0.0000)? ==> 0.2114
neuronOutput[9]: activation(0.2114) > threshold(0.0000)? ==> 0.2114
neuronOutput[10]: activation(0.2114) > threshold(0.0000)? ==> 0.2114
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.2114)
networkOutput[2] := neuronOutput[10](0.2114)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.1011) * output[3](0.2140) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.2140) * output[3](0.2140) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.1011) * output[4](0.2140) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.1011) * output[4](0.2140) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.2140) * output[4](0.2140) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.1011) * output[5](0.2140) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.2140) * output[5](0.2140) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=054 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.1011) * weight[1,1](0.1000) => 0.0101
activation[2] = self(0.1011) * weight[2,2](0.1000) => 0.0101
activation[3] = self(0.2140) * weight[3,3](0.1000) => 0.0214
activation[4] = self(0.2140) * weight[4,4](0.1000) => 0.0214
activation[5] = self(0.2140) * weight[5,5](0.1000) => 0.0214
activation[6] = self(0.2114) * weight[6,6](0.1000) => 0.0211
activation[7] = self(0.2114) * weight[7,7](0.1000) => 0.0211
activation[8] = self(0.2114) * weight[8,8](0.1000) => 0.0211
activation[9] = self(0.2114) * weight[9,9](0.1000) => 0.0211
activation[10] = self(0.2114) * weight[10,10](0.1000) => 0.0211
  b. Update inputs from other neurons
activation[1] = self(0.0101) + network_input[1]=1.0000 => 1.0101
activation[2] = self(0.0101) + network_input[2]=1.0000 => 1.0101
activation[3] = self(0.0214) + output[1](0.1011) * 1.0000 => 0.1225
activation[3] = self(0.1225) + output[7](0.2114) * -0.5000 => 0.0168
activation[3] = self(0.0168) + output[8](0.2114) * -0.5000 => -0.0889
activation[4] = self(0.0214) + output[1](0.1011) * 0.5000 => 0.0719
activation[4] = self(0.0719) + output[2](0.1011) * 0.5000 => 0.1225
activation[4] = self(0.1225) + output[6](0.2114) * -0.5000 => 0.0168
activation[4] = self(0.0168) + output[8](0.2114) * -0.5000 => -0.0889
activation[5] = self(0.0214) + output[2](0.1011) * 1.0000 => 0.1225
activation[5] = self(0.1225) + output[6](0.2114) * -0.5000 => 0.0168
activation[5] = self(0.0168) + output[7](0.2114) * -0.5000 => -0.0889
activation[6] = self(0.0211) + output[3](0.2140) * 1.0000 => 0.2351
activation[7] = self(0.0211) + output[4](0.2140) * 1.0000 => 0.2351
activation[8] = self(0.0211) + output[5](0.2140) * 1.0000 => 0.2351
activation[9] = self(0.0211) + output[3](0.2140) * 0.7500 => 0.1816
activation[9] = self(0.1816) + output[4](0.2140) * 0.2500 => 0.2351
activation[10] = self(0.0211) + output[4](0.2140) * 0.2500 => 0.0746
activation[10] = self(0.0746) + output[5](0.2140) * 0.7500 => 0.2351
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0101) > threshold(0.0000)? ==> 1.0101
neuronOutput[2]: activation(1.0101) > threshold(0.0000)? ==> 1.0101
neuronOutput[3]: activation(-0.0889) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-0.0889) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-0.0889) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.2351) > threshold(0.0000)? ==> 0.2351
neuronOutput[7]: activation(0.2351) > threshold(0.0000)? ==> 0.2351
neuronOutput[8]: activation(0.2351) > threshold(0.0000)? ==> 0.2351
neuronOutput[9]: activation(0.2351) > threshold(0.0000)? ==> 0.2351
neuronOutput[10]: activation(0.2351) > threshold(0.0000)? ==> 0.2351
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.2351)
networkOutput[2] := neuronOutput[10](0.2351)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.0101) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.0101) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.0101) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.0101) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=055 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0101) * weight[1,1](0.1000) => 0.1010
activation[2] = self(1.0101) * weight[2,2](0.1000) => 0.1010
activation[3] = self(-0.0889) * weight[3,3](0.1000) => -0.0089
activation[4] = self(-0.0889) * weight[4,4](0.1000) => -0.0089
activation[5] = self(-0.0889) * weight[5,5](0.1000) => -0.0089
activation[6] = self(0.2351) * weight[6,6](0.1000) => 0.0235
activation[7] = self(0.2351) * weight[7,7](0.1000) => 0.0235
activation[8] = self(0.2351) * weight[8,8](0.1000) => 0.0235
activation[9] = self(0.2351) * weight[9,9](0.1000) => 0.0235
activation[10] = self(0.2351) * weight[10,10](0.1000) => 0.0235
  b. Update inputs from other neurons
activation[1] = self(0.1010) + network_input[1]=0.0000 => 0.1010
activation[2] = self(0.1010) + network_input[2]=0.0000 => 0.1010
activation[3] = self(-0.0089) + output[1](1.0101) * 1.0000 => 1.0012
activation[3] = self(1.0012) + output[7](0.2351) * -0.5000 => 0.8837
activation[3] = self(0.8837) + output[8](0.2351) * -0.5000 => 0.7661
activation[4] = self(-0.0089) + output[1](1.0101) * 0.5000 => 0.4962
activation[4] = self(0.4962) + output[2](1.0101) * 0.5000 => 1.0012
activation[4] = self(1.0012) + output[6](0.2351) * -0.5000 => 0.8837
activation[4] = self(0.8837) + output[8](0.2351) * -0.5000 => 0.7661
activation[5] = self(-0.0089) + output[2](1.0101) * 1.0000 => 1.0012
activation[5] = self(1.0012) + output[6](0.2351) * -0.5000 => 0.8837
activation[5] = self(0.8837) + output[7](0.2351) * -0.5000 => 0.7661
activation[6] = self(0.0235) + output[3](0.0000) * 1.0000 => 0.0235
activation[7] = self(0.0235) + output[4](0.0000) * 1.0000 => 0.0235
activation[8] = self(0.0235) + output[5](0.0000) * 1.0000 => 0.0235
activation[9] = self(0.0235) + output[3](0.0000) * 0.7500 => 0.0235
activation[9] = self(0.0235) + output[4](0.0000) * 0.2500 => 0.0235
activation[10] = self(0.0235) + output[4](0.0000) * 0.2500 => 0.0235
activation[10] = self(0.0235) + output[5](0.0000) * 0.7500 => 0.0235
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.1010) > threshold(0.0000)? ==> 0.1010
neuronOutput[2]: activation(0.1010) > threshold(0.0000)? ==> 0.1010
neuronOutput[3]: activation(0.7661) > threshold(0.0000)? ==> 0.7661
neuronOutput[4]: activation(0.7661) > threshold(0.0000)? ==> 0.7661
neuronOutput[5]: activation(0.7661) > threshold(0.0000)? ==> 0.7661
neuronOutput[6]: activation(0.0235) > threshold(0.0000)? ==> 0.0235
neuronOutput[7]: activation(0.0235) > threshold(0.0000)? ==> 0.0235
neuronOutput[8]: activation(0.0235) > threshold(0.0000)? ==> 0.0235
neuronOutput[9]: activation(0.0235) > threshold(0.0000)? ==> 0.0235
neuronOutput[10]: activation(0.0235) > threshold(0.0000)? ==> 0.0235
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0235)
networkOutput[2] := neuronOutput[10](0.0235)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.1010) * output[3](0.7661) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.7661) * output[3](0.7661) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.1010) * output[4](0.7661) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.1010) * output[4](0.7661) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.7661) * output[4](0.7661) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.1010) * output[5](0.7661) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.7661) * output[5](0.7661) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=056 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.1010) * weight[1,1](0.1000) => 0.0101
activation[2] = self(0.1010) * weight[2,2](0.1000) => 0.0101
activation[3] = self(0.7661) * weight[3,3](0.1000) => 0.0766
activation[4] = self(0.7661) * weight[4,4](0.1000) => 0.0766
activation[5] = self(0.7661) * weight[5,5](0.1000) => 0.0766
activation[6] = self(0.0235) * weight[6,6](0.1000) => 0.0024
activation[7] = self(0.0235) * weight[7,7](0.1000) => 0.0024
activation[8] = self(0.0235) * weight[8,8](0.1000) => 0.0024
activation[9] = self(0.0235) * weight[9,9](0.1000) => 0.0024
activation[10] = self(0.0235) * weight[10,10](0.1000) => 0.0024
  b. Update inputs from other neurons
activation[1] = self(0.0101) + network_input[1]=1.0000 => 1.0101
activation[2] = self(0.0101) + network_input[2]=1.0000 => 1.0101
activation[3] = self(0.0766) + output[1](0.1010) * 1.0000 => 0.1776
activation[3] = self(0.1776) + output[7](0.0235) * -0.5000 => 0.1659
activation[3] = self(0.1659) + output[8](0.0235) * -0.5000 => 0.1541
activation[4] = self(0.0766) + output[1](0.1010) * 0.5000 => 0.1271
activation[4] = self(0.1271) + output[2](0.1010) * 0.5000 => 0.1776
activation[4] = self(0.1776) + output[6](0.0235) * -0.5000 => 0.1659
activation[4] = self(0.1659) + output[8](0.0235) * -0.5000 => 0.1541
activation[5] = self(0.0766) + output[2](0.1010) * 1.0000 => 0.1776
activation[5] = self(0.1776) + output[6](0.0235) * -0.5000 => 0.1659
activation[5] = self(0.1659) + output[7](0.0235) * -0.5000 => 0.1541
activation[6] = self(0.0024) + output[3](0.7661) * 1.0000 => 0.7685
activation[7] = self(0.0024) + output[4](0.7661) * 1.0000 => 0.7685
activation[8] = self(0.0024) + output[5](0.7661) * 1.0000 => 0.7685
activation[9] = self(0.0024) + output[3](0.7661) * 0.7500 => 0.5769
activation[9] = self(0.5769) + output[4](0.7661) * 0.2500 => 0.7685
activation[10] = self(0.0024) + output[4](0.7661) * 0.2500 => 0.1939
activation[10] = self(0.1939) + output[5](0.7661) * 0.7500 => 0.7685
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0101) > threshold(0.0000)? ==> 1.0101
neuronOutput[2]: activation(1.0101) > threshold(0.0000)? ==> 1.0101
neuronOutput[3]: activation(0.1541) > threshold(0.0000)? ==> 0.1541
neuronOutput[4]: activation(0.1541) > threshold(0.0000)? ==> 0.1541
neuronOutput[5]: activation(0.1541) > threshold(0.0000)? ==> 0.1541
neuronOutput[6]: activation(0.7685) > threshold(0.0000)? ==> 0.7685
neuronOutput[7]: activation(0.7685) > threshold(0.0000)? ==> 0.7685
neuronOutput[8]: activation(0.7685) > threshold(0.0000)? ==> 0.7685
neuronOutput[9]: activation(0.7685) > threshold(0.0000)? ==> 0.7685
neuronOutput[10]: activation(0.7685) > threshold(0.0000)? ==> 0.7685
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.7685)
networkOutput[2] := neuronOutput[10](0.7685)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.0101) * output[3](0.1541) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.1541) * output[3](0.1541) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.0101) * output[4](0.1541) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.0101) * output[4](0.1541) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.1541) * output[4](0.1541) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.0101) * output[5](0.1541) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.1541) * output[5](0.1541) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=057 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0101) * weight[1,1](0.1000) => 0.1010
activation[2] = self(1.0101) * weight[2,2](0.1000) => 0.1010
activation[3] = self(0.1541) * weight[3,3](0.1000) => 0.0154
activation[4] = self(0.1541) * weight[4,4](0.1000) => 0.0154
activation[5] = self(0.1541) * weight[5,5](0.1000) => 0.0154
activation[6] = self(0.7685) * weight[6,6](0.1000) => 0.0768
activation[7] = self(0.7685) * weight[7,7](0.1000) => 0.0768
activation[8] = self(0.7685) * weight[8,8](0.1000) => 0.0768
activation[9] = self(0.7685) * weight[9,9](0.1000) => 0.0768
activation[10] = self(0.7685) * weight[10,10](0.1000) => 0.0768
  b. Update inputs from other neurons
activation[1] = self(0.1010) + network_input[1]=1.0000 => 1.1010
activation[2] = self(0.1010) + network_input[2]=1.0000 => 1.1010
activation[3] = self(0.0154) + output[1](1.0101) * 1.0000 => 1.0255
activation[3] = self(1.0255) + output[7](0.7685) * -0.5000 => 0.6413
activation[3] = self(0.6413) + output[8](0.7685) * -0.5000 => 0.2571
activation[4] = self(0.0154) + output[1](1.0101) * 0.5000 => 0.5205
activation[4] = self(0.5205) + output[2](1.0101) * 0.5000 => 1.0255
activation[4] = self(1.0255) + output[6](0.7685) * -0.5000 => 0.6413
activation[4] = self(0.6413) + output[8](0.7685) * -0.5000 => 0.2571
activation[5] = self(0.0154) + output[2](1.0101) * 1.0000 => 1.0255
activation[5] = self(1.0255) + output[6](0.7685) * -0.5000 => 0.6413
activation[5] = self(0.6413) + output[7](0.7685) * -0.5000 => 0.2571
activation[6] = self(0.0768) + output[3](0.1541) * 1.0000 => 0.2310
activation[7] = self(0.0768) + output[4](0.1541) * 1.0000 => 0.2310
activation[8] = self(0.0768) + output[5](0.1541) * 1.0000 => 0.2310
activation[9] = self(0.0768) + output[3](0.1541) * 0.7500 => 0.1924
activation[9] = self(0.1924) + output[4](0.1541) * 0.2500 => 0.2310
activation[10] = self(0.0768) + output[4](0.1541) * 0.2500 => 0.1154
activation[10] = self(0.1154) + output[5](0.1541) * 0.7500 => 0.2310
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.1010) > threshold(0.0000)? ==> 1.1010
neuronOutput[2]: activation(1.1010) > threshold(0.0000)? ==> 1.1010
neuronOutput[3]: activation(0.2571) > threshold(0.0000)? ==> 0.2571
neuronOutput[4]: activation(0.2571) > threshold(0.0000)? ==> 0.2571
neuronOutput[5]: activation(0.2571) > threshold(0.0000)? ==> 0.2571
neuronOutput[6]: activation(0.2310) > threshold(0.0000)? ==> 0.2310
neuronOutput[7]: activation(0.2310) > threshold(0.0000)? ==> 0.2310
neuronOutput[8]: activation(0.2310) > threshold(0.0000)? ==> 0.2310
neuronOutput[9]: activation(0.2310) > threshold(0.0000)? ==> 0.2310
neuronOutput[10]: activation(0.2310) > threshold(0.0000)? ==> 0.2310
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.2310)
networkOutput[2] := neuronOutput[10](0.2310)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.1010) * output[3](0.2571) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.2571) * output[3](0.2571) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.1010) * output[4](0.2571) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.1010) * output[4](0.2571) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.2571) * output[4](0.2571) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.1010) * output[5](0.2571) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.2571) * output[5](0.2571) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=058 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.1010) * weight[1,1](0.1000) => 0.1101
activation[2] = self(1.1010) * weight[2,2](0.1000) => 0.1101
activation[3] = self(0.2571) * weight[3,3](0.1000) => 0.0257
activation[4] = self(0.2571) * weight[4,4](0.1000) => 0.0257
activation[5] = self(0.2571) * weight[5,5](0.1000) => 0.0257
activation[6] = self(0.2310) * weight[6,6](0.1000) => 0.0231
activation[7] = self(0.2310) * weight[7,7](0.1000) => 0.0231
activation[8] = self(0.2310) * weight[8,8](0.1000) => 0.0231
activation[9] = self(0.2310) * weight[9,9](0.1000) => 0.0231
activation[10] = self(0.2310) * weight[10,10](0.1000) => 0.0231
  b. Update inputs from other neurons
activation[1] = self(0.1101) + network_input[1]=0.0000 => 0.1101
activation[2] = self(0.1101) + network_input[2]=0.0000 => 0.1101
activation[3] = self(0.0257) + output[1](1.1010) * 1.0000 => 1.1267
activation[3] = self(1.1267) + output[7](0.2310) * -0.5000 => 1.0112
activation[3] = self(1.0112) + output[8](0.2310) * -0.5000 => 0.8958
activation[4] = self(0.0257) + output[1](1.1010) * 0.5000 => 0.5762
activation[4] = self(0.5762) + output[2](1.1010) * 0.5000 => 1.1267
activation[4] = self(1.1267) + output[6](0.2310) * -0.5000 => 1.0112
activation[4] = self(1.0112) + output[8](0.2310) * -0.5000 => 0.8958
activation[5] = self(0.0257) + output[2](1.1010) * 1.0000 => 1.1267
activation[5] = self(1.1267) + output[6](0.2310) * -0.5000 => 1.0112
activation[5] = self(1.0112) + output[7](0.2310) * -0.5000 => 0.8958
activation[6] = self(0.0231) + output[3](0.2571) * 1.0000 => 0.2802
activation[7] = self(0.0231) + output[4](0.2571) * 1.0000 => 0.2802
activation[8] = self(0.0231) + output[5](0.2571) * 1.0000 => 0.2802
activation[9] = self(0.0231) + output[3](0.2571) * 0.7500 => 0.2159
activation[9] = self(0.2159) + output[4](0.2571) * 0.2500 => 0.2802
activation[10] = self(0.0231) + output[4](0.2571) * 0.2500 => 0.0874
activation[10] = self(0.0874) + output[5](0.2571) * 0.7500 => 0.2802
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.1101) > threshold(0.0000)? ==> 0.1101
neuronOutput[2]: activation(0.1101) > threshold(0.0000)? ==> 0.1101
neuronOutput[3]: activation(0.8958) > threshold(0.0000)? ==> 0.8958
neuronOutput[4]: activation(0.8958) > threshold(0.0000)? ==> 0.8958
neuronOutput[5]: activation(0.8958) > threshold(0.0000)? ==> 0.8958
neuronOutput[6]: activation(0.2802) > threshold(0.0000)? ==> 0.2802
neuronOutput[7]: activation(0.2802) > threshold(0.0000)? ==> 0.2802
neuronOutput[8]: activation(0.2802) > threshold(0.0000)? ==> 0.2802
neuronOutput[9]: activation(0.2802) > threshold(0.0000)? ==> 0.2802
neuronOutput[10]: activation(0.2802) > threshold(0.0000)? ==> 0.2802
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.2802)
networkOutput[2] := neuronOutput[10](0.2802)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.1101) * output[3](0.8958) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.8958) * output[3](0.8958) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.1101) * output[4](0.8958) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.1101) * output[4](0.8958) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.8958) * output[4](0.8958) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.1101) * output[5](0.8958) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.8958) * output[5](0.8958) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=059 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.1101) * weight[1,1](0.1000) => 0.0110
activation[2] = self(0.1101) * weight[2,2](0.1000) => 0.0110
activation[3] = self(0.8958) * weight[3,3](0.1000) => 0.0896
activation[4] = self(0.8958) * weight[4,4](0.1000) => 0.0896
activation[5] = self(0.8958) * weight[5,5](0.1000) => 0.0896
activation[6] = self(0.2802) * weight[6,6](0.1000) => 0.0280
activation[7] = self(0.2802) * weight[7,7](0.1000) => 0.0280
activation[8] = self(0.2802) * weight[8,8](0.1000) => 0.0280
activation[9] = self(0.2802) * weight[9,9](0.1000) => 0.0280
activation[10] = self(0.2802) * weight[10,10](0.1000) => 0.0280
  b. Update inputs from other neurons
activation[1] = self(0.0110) + network_input[1]=1.0000 => 1.0110
activation[2] = self(0.0110) + network_input[2]=1.0000 => 1.0110
activation[3] = self(0.0896) + output[1](0.1101) * 1.0000 => 0.1997
activation[3] = self(0.1997) + output[7](0.2802) * -0.5000 => 0.0596
activation[3] = self(0.0596) + output[8](0.2802) * -0.5000 => -0.0805
activation[4] = self(0.0896) + output[1](0.1101) * 0.5000 => 0.1446
activation[4] = self(0.1446) + output[2](0.1101) * 0.5000 => 0.1997
activation[4] = self(0.1997) + output[6](0.2802) * -0.5000 => 0.0596
activation[4] = self(0.0596) + output[8](0.2802) * -0.5000 => -0.0805
activation[5] = self(0.0896) + output[2](0.1101) * 1.0000 => 0.1997
activation[5] = self(0.1997) + output[6](0.2802) * -0.5000 => 0.0596
activation[5] = self(0.0596) + output[7](0.2802) * -0.5000 => -0.0805
activation[6] = self(0.0280) + output[3](0.8958) * 1.0000 => 0.9238
activation[7] = self(0.0280) + output[4](0.8958) * 1.0000 => 0.9238
activation[8] = self(0.0280) + output[5](0.8958) * 1.0000 => 0.9238
activation[9] = self(0.0280) + output[3](0.8958) * 0.7500 => 0.6998
activation[9] = self(0.6998) + output[4](0.8958) * 0.2500 => 0.9238
activation[10] = self(0.0280) + output[4](0.8958) * 0.2500 => 0.2520
activation[10] = self(0.2520) + output[5](0.8958) * 0.7500 => 0.9238
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0110) > threshold(0.0000)? ==> 1.0110
neuronOutput[2]: activation(1.0110) > threshold(0.0000)? ==> 1.0110
neuronOutput[3]: activation(-0.0805) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-0.0805) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-0.0805) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.9238) > threshold(0.0000)? ==> 0.9238
neuronOutput[7]: activation(0.9238) > threshold(0.0000)? ==> 0.9238
neuronOutput[8]: activation(0.9238) > threshold(0.0000)? ==> 0.9238
neuronOutput[9]: activation(0.9238) > threshold(0.0000)? ==> 0.9238
neuronOutput[10]: activation(0.9238) > threshold(0.0000)? ==> 0.9238
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.9238)
networkOutput[2] := neuronOutput[10](0.9238)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.0110) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.0110) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.0110) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.0110) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=060 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0110) * weight[1,1](0.1000) => 0.1011
activation[2] = self(1.0110) * weight[2,2](0.1000) => 0.1011
activation[3] = self(-0.0805) * weight[3,3](0.1000) => -0.0080
activation[4] = self(-0.0805) * weight[4,4](0.1000) => -0.0080
activation[5] = self(-0.0805) * weight[5,5](0.1000) => -0.0080
activation[6] = self(0.9238) * weight[6,6](0.1000) => 0.0924
activation[7] = self(0.9238) * weight[7,7](0.1000) => 0.0924
activation[8] = self(0.9238) * weight[8,8](0.1000) => 0.0924
activation[9] = self(0.9238) * weight[9,9](0.1000) => 0.0924
activation[10] = self(0.9238) * weight[10,10](0.1000) => 0.0924
  b. Update inputs from other neurons
activation[1] = self(0.1011) + network_input[1]=1.0000 => 1.1011
activation[2] = self(0.1011) + network_input[2]=1.0000 => 1.1011
activation[3] = self(-0.0080) + output[1](1.0110) * 1.0000 => 1.0030
activation[3] = self(1.0030) + output[7](0.9238) * -0.5000 => 0.5411
activation[3] = self(0.5411) + output[8](0.9238) * -0.5000 => 0.0792
activation[4] = self(-0.0080) + output[1](1.0110) * 0.5000 => 0.4975
activation[4] = self(0.4975) + output[2](1.0110) * 0.5000 => 1.0030
activation[4] = self(1.0030) + output[6](0.9238) * -0.5000 => 0.5411
activation[4] = self(0.5411) + output[8](0.9238) * -0.5000 => 0.0792
activation[5] = self(-0.0080) + output[2](1.0110) * 1.0000 => 1.0030
activation[5] = self(1.0030) + output[6](0.9238) * -0.5000 => 0.5411
activation[5] = self(0.5411) + output[7](0.9238) * -0.5000 => 0.0792
activation[6] = self(0.0924) + output[3](0.0000) * 1.0000 => 0.0924
activation[7] = self(0.0924) + output[4](0.0000) * 1.0000 => 0.0924
activation[8] = self(0.0924) + output[5](0.0000) * 1.0000 => 0.0924
activation[9] = self(0.0924) + output[3](0.0000) * 0.7500 => 0.0924
activation[9] = self(0.0924) + output[4](0.0000) * 0.2500 => 0.0924
activation[10] = self(0.0924) + output[4](0.0000) * 0.2500 => 0.0924
activation[10] = self(0.0924) + output[5](0.0000) * 0.7500 => 0.0924
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.1011) > threshold(0.0000)? ==> 1.1011
neuronOutput[2]: activation(1.1011) > threshold(0.0000)? ==> 1.1011
neuronOutput[3]: activation(0.0792) > threshold(0.0000)? ==> 0.0792
neuronOutput[4]: activation(0.0792) > threshold(0.0000)? ==> 0.0792
neuronOutput[5]: activation(0.0792) > threshold(0.0000)? ==> 0.0792
neuronOutput[6]: activation(0.0924) > threshold(0.0000)? ==> 0.0924
neuronOutput[7]: activation(0.0924) > threshold(0.0000)? ==> 0.0924
neuronOutput[8]: activation(0.0924) > threshold(0.0000)? ==> 0.0924
neuronOutput[9]: activation(0.0924) > threshold(0.0000)? ==> 0.0924
neuronOutput[10]: activation(0.0924) > threshold(0.0000)? ==> 0.0924
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0924)
networkOutput[2] := neuronOutput[10](0.0924)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.1011) * output[3](0.0792) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0792) * output[3](0.0792) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.1011) * output[4](0.0792) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.1011) * output[4](0.0792) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0792) * output[4](0.0792) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.1011) * output[5](0.0792) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0792) * output[5](0.0792) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=061 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.1011) * weight[1,1](0.1000) => 0.1101
activation[2] = self(1.1011) * weight[2,2](0.1000) => 0.1101
activation[3] = self(0.0792) * weight[3,3](0.1000) => 0.0079
activation[4] = self(0.0792) * weight[4,4](0.1000) => 0.0079
activation[5] = self(0.0792) * weight[5,5](0.1000) => 0.0079
activation[6] = self(0.0924) * weight[6,6](0.1000) => 0.0092
activation[7] = self(0.0924) * weight[7,7](0.1000) => 0.0092
activation[8] = self(0.0924) * weight[8,8](0.1000) => 0.0092
activation[9] = self(0.0924) * weight[9,9](0.1000) => 0.0092
activation[10] = self(0.0924) * weight[10,10](0.1000) => 0.0092
  b. Update inputs from other neurons
activation[1] = self(0.1101) + network_input[1]=0.0000 => 0.1101
activation[2] = self(0.1101) + network_input[2]=0.0000 => 0.1101
activation[3] = self(0.0079) + output[1](1.1011) * 1.0000 => 1.1090
activation[3] = self(1.1090) + output[7](0.0924) * -0.5000 => 1.0628
activation[3] = self(1.0628) + output[8](0.0924) * -0.5000 => 1.0166
activation[4] = self(0.0079) + output[1](1.1011) * 0.5000 => 0.5585
activation[4] = self(0.5585) + output[2](1.1011) * 0.5000 => 1.1090
activation[4] = self(1.1090) + output[6](0.0924) * -0.5000 => 1.0628
activation[4] = self(1.0628) + output[8](0.0924) * -0.5000 => 1.0166
activation[5] = self(0.0079) + output[2](1.1011) * 1.0000 => 1.1090
activation[5] = self(1.1090) + output[6](0.0924) * -0.5000 => 1.0628
activation[5] = self(1.0628) + output[7](0.0924) * -0.5000 => 1.0166
activation[6] = self(0.0092) + output[3](0.0792) * 1.0000 => 0.0884
activation[7] = self(0.0092) + output[4](0.0792) * 1.0000 => 0.0884
activation[8] = self(0.0092) + output[5](0.0792) * 1.0000 => 0.0884
activation[9] = self(0.0092) + output[3](0.0792) * 0.7500 => 0.0686
activation[9] = self(0.0686) + output[4](0.0792) * 0.2500 => 0.0884
activation[10] = self(0.0092) + output[4](0.0792) * 0.2500 => 0.0290
activation[10] = self(0.0290) + output[5](0.0792) * 0.7500 => 0.0884
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.1101) > threshold(0.0000)? ==> 0.1101
neuronOutput[2]: activation(0.1101) > threshold(0.0000)? ==> 0.1101
neuronOutput[3]: activation(1.0166) > threshold(0.0000)? ==> 1.0166
neuronOutput[4]: activation(1.0166) > threshold(0.0000)? ==> 1.0166
neuronOutput[5]: activation(1.0166) > threshold(0.0000)? ==> 1.0166
neuronOutput[6]: activation(0.0884) > threshold(0.0000)? ==> 0.0884
neuronOutput[7]: activation(0.0884) > threshold(0.0000)? ==> 0.0884
neuronOutput[8]: activation(0.0884) > threshold(0.0000)? ==> 0.0884
neuronOutput[9]: activation(0.0884) > threshold(0.0000)? ==> 0.0884
neuronOutput[10]: activation(0.0884) > threshold(0.0000)? ==> 0.0884
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0884)
networkOutput[2] := neuronOutput[10](0.0884)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.1101) * output[3](1.0166) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](1.0166) * output[3](1.0166) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.1101) * output[4](1.0166) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.1101) * output[4](1.0166) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](1.0166) * output[4](1.0166) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.1101) * output[5](1.0166) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](1.0166) * output[5](1.0166) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=062 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.1101) * weight[1,1](0.1000) => 0.0110
activation[2] = self(0.1101) * weight[2,2](0.1000) => 0.0110
activation[3] = self(1.0166) * weight[3,3](0.1000) => 0.1017
activation[4] = self(1.0166) * weight[4,4](0.1000) => 0.1017
activation[5] = self(1.0166) * weight[5,5](0.1000) => 0.1017
activation[6] = self(0.0884) * weight[6,6](0.1000) => 0.0088
activation[7] = self(0.0884) * weight[7,7](0.1000) => 0.0088
activation[8] = self(0.0884) * weight[8,8](0.1000) => 0.0088
activation[9] = self(0.0884) * weight[9,9](0.1000) => 0.0088
activation[10] = self(0.0884) * weight[10,10](0.1000) => 0.0088
  b. Update inputs from other neurons
activation[1] = self(0.0110) + network_input[1]=0.0000 => 0.0110
activation[2] = self(0.0110) + network_input[2]=0.0000 => 0.0110
activation[3] = self(0.1017) + output[1](0.1101) * 1.0000 => 0.2118
activation[3] = self(0.2118) + output[7](0.0884) * -0.5000 => 0.1676
activation[3] = self(0.1676) + output[8](0.0884) * -0.5000 => 0.1233
activation[4] = self(0.1017) + output[1](0.1101) * 0.5000 => 0.1567
activation[4] = self(0.1567) + output[2](0.1101) * 0.5000 => 0.2118
activation[4] = self(0.2118) + output[6](0.0884) * -0.5000 => 0.1676
activation[4] = self(0.1676) + output[8](0.0884) * -0.5000 => 0.1233
activation[5] = self(0.1017) + output[2](0.1101) * 1.0000 => 0.2118
activation[5] = self(0.2118) + output[6](0.0884) * -0.5000 => 0.1676
activation[5] = self(0.1676) + output[7](0.0884) * -0.5000 => 0.1233
activation[6] = self(0.0088) + output[3](1.0166) * 1.0000 => 1.0255
activation[7] = self(0.0088) + output[4](1.0166) * 1.0000 => 1.0255
activation[8] = self(0.0088) + output[5](1.0166) * 1.0000 => 1.0255
activation[9] = self(0.0088) + output[3](1.0166) * 0.7500 => 0.7713
activation[9] = self(0.7713) + output[4](1.0166) * 0.2500 => 1.0255
activation[10] = self(0.0088) + output[4](1.0166) * 0.2500 => 0.2630
activation[10] = self(0.2630) + output[5](1.0166) * 0.7500 => 1.0255
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0110) > threshold(0.0000)? ==> 0.0110
neuronOutput[2]: activation(0.0110) > threshold(0.0000)? ==> 0.0110
neuronOutput[3]: activation(0.1233) > threshold(0.0000)? ==> 0.1233
neuronOutput[4]: activation(0.1233) > threshold(0.0000)? ==> 0.1233
neuronOutput[5]: activation(0.1233) > threshold(0.0000)? ==> 0.1233
neuronOutput[6]: activation(1.0255) > threshold(0.0000)? ==> 1.0255
neuronOutput[7]: activation(1.0255) > threshold(0.0000)? ==> 1.0255
neuronOutput[8]: activation(1.0255) > threshold(0.0000)? ==> 1.0255
neuronOutput[9]: activation(1.0255) > threshold(0.0000)? ==> 1.0255
neuronOutput[10]: activation(1.0255) > threshold(0.0000)? ==> 1.0255
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](1.0255)
networkOutput[2] := neuronOutput[10](1.0255)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.0110) * output[3](0.1233) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.1233) * output[3](0.1233) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.0110) * output[4](0.1233) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.0110) * output[4](0.1233) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.1233) * output[4](0.1233) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.0110) * output[5](0.1233) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.1233) * output[5](0.1233) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=063 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0110) * weight[1,1](0.1000) => 0.0011
activation[2] = self(0.0110) * weight[2,2](0.1000) => 0.0011
activation[3] = self(0.1233) * weight[3,3](0.1000) => 0.0123
activation[4] = self(0.1233) * weight[4,4](0.1000) => 0.0123
activation[5] = self(0.1233) * weight[5,5](0.1000) => 0.0123
activation[6] = self(1.0255) * weight[6,6](0.1000) => 0.1025
activation[7] = self(1.0255) * weight[7,7](0.1000) => 0.1025
activation[8] = self(1.0255) * weight[8,8](0.1000) => 0.1025
activation[9] = self(1.0255) * weight[9,9](0.1000) => 0.1025
activation[10] = self(1.0255) * weight[10,10](0.1000) => 0.1025
  b. Update inputs from other neurons
activation[1] = self(0.0011) + network_input[1]=1.0000 => 1.0011
activation[2] = self(0.0011) + network_input[2]=1.0000 => 1.0011
activation[3] = self(0.0123) + output[1](0.0110) * 1.0000 => 0.0233
activation[3] = self(0.0233) + output[7](1.0255) * -0.5000 => -0.4894
activation[3] = self(-0.4894) + output[8](1.0255) * -0.5000 => -1.0021
activation[4] = self(0.0123) + output[1](0.0110) * 0.5000 => 0.0178
activation[4] = self(0.0178) + output[2](0.0110) * 0.5000 => 0.0233
activation[4] = self(0.0233) + output[6](1.0255) * -0.5000 => -0.4894
activation[4] = self(-0.4894) + output[8](1.0255) * -0.5000 => -1.0021
activation[5] = self(0.0123) + output[2](0.0110) * 1.0000 => 0.0233
activation[5] = self(0.0233) + output[6](1.0255) * -0.5000 => -0.4894
activation[5] = self(-0.4894) + output[7](1.0255) * -0.5000 => -1.0021
activation[6] = self(0.1025) + output[3](0.1233) * 1.0000 => 0.2259
activation[7] = self(0.1025) + output[4](0.1233) * 1.0000 => 0.2259
activation[8] = self(0.1025) + output[5](0.1233) * 1.0000 => 0.2259
activation[9] = self(0.1025) + output[3](0.1233) * 0.7500 => 0.1951
activation[9] = self(0.1951) + output[4](0.1233) * 0.2500 => 0.2259
activation[10] = self(0.1025) + output[4](0.1233) * 0.2500 => 0.1334
activation[10] = self(0.1334) + output[5](0.1233) * 0.7500 => 0.2259
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0011) > threshold(0.0000)? ==> 1.0011
neuronOutput[2]: activation(1.0011) > threshold(0.0000)? ==> 1.0011
neuronOutput[3]: activation(-1.0021) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-1.0021) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-1.0021) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.2259) > threshold(0.0000)? ==> 0.2259
neuronOutput[7]: activation(0.2259) > threshold(0.0000)? ==> 0.2259
neuronOutput[8]: activation(0.2259) > threshold(0.0000)? ==> 0.2259
neuronOutput[9]: activation(0.2259) > threshold(0.0000)? ==> 0.2259
neuronOutput[10]: activation(0.2259) > threshold(0.0000)? ==> 0.2259
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.2259)
networkOutput[2] := neuronOutput[10](0.2259)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.0011) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.0011) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.0011) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.0011) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=064 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0011) * weight[1,1](0.1000) => 0.1001
activation[2] = self(1.0011) * weight[2,2](0.1000) => 0.1001
activation[3] = self(-1.0021) * weight[3,3](0.1000) => -0.1002
activation[4] = self(-1.0021) * weight[4,4](0.1000) => -0.1002
activation[5] = self(-1.0021) * weight[5,5](0.1000) => -0.1002
activation[6] = self(0.2259) * weight[6,6](0.1000) => 0.0226
activation[7] = self(0.2259) * weight[7,7](0.1000) => 0.0226
activation[8] = self(0.2259) * weight[8,8](0.1000) => 0.0226
activation[9] = self(0.2259) * weight[9,9](0.1000) => 0.0226
activation[10] = self(0.2259) * weight[10,10](0.1000) => 0.0226
  b. Update inputs from other neurons
activation[1] = self(0.1001) + network_input[1]=1.0000 => 1.1001
activation[2] = self(0.1001) + network_input[2]=1.0000 => 1.1001
activation[3] = self(-0.1002) + output[1](1.0011) * 1.0000 => 0.9009
activation[3] = self(0.9009) + output[7](0.2259) * -0.5000 => 0.7879
activation[3] = self(0.7879) + output[8](0.2259) * -0.5000 => 0.6750
activation[4] = self(-0.1002) + output[1](1.0011) * 0.5000 => 0.4003
activation[4] = self(0.4003) + output[2](1.0011) * 0.5000 => 0.9009
activation[4] = self(0.9009) + output[6](0.2259) * -0.5000 => 0.7879
activation[4] = self(0.7879) + output[8](0.2259) * -0.5000 => 0.6750
activation[5] = self(-0.1002) + output[2](1.0011) * 1.0000 => 0.9009
activation[5] = self(0.9009) + output[6](0.2259) * -0.5000 => 0.7879
activation[5] = self(0.7879) + output[7](0.2259) * -0.5000 => 0.6750
activation[6] = self(0.0226) + output[3](0.0000) * 1.0000 => 0.0226
activation[7] = self(0.0226) + output[4](0.0000) * 1.0000 => 0.0226
activation[8] = self(0.0226) + output[5](0.0000) * 1.0000 => 0.0226
activation[9] = self(0.0226) + output[3](0.0000) * 0.7500 => 0.0226
activation[9] = self(0.0226) + output[4](0.0000) * 0.2500 => 0.0226
activation[10] = self(0.0226) + output[4](0.0000) * 0.2500 => 0.0226
activation[10] = self(0.0226) + output[5](0.0000) * 0.7500 => 0.0226
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.1001) > threshold(0.0000)? ==> 1.1001
neuronOutput[2]: activation(1.1001) > threshold(0.0000)? ==> 1.1001
neuronOutput[3]: activation(0.6750) > threshold(0.0000)? ==> 0.6750
neuronOutput[4]: activation(0.6750) > threshold(0.0000)? ==> 0.6750
neuronOutput[5]: activation(0.6750) > threshold(0.0000)? ==> 0.6750
neuronOutput[6]: activation(0.0226) > threshold(0.0000)? ==> 0.0226
neuronOutput[7]: activation(0.0226) > threshold(0.0000)? ==> 0.0226
neuronOutput[8]: activation(0.0226) > threshold(0.0000)? ==> 0.0226
neuronOutput[9]: activation(0.0226) > threshold(0.0000)? ==> 0.0226
neuronOutput[10]: activation(0.0226) > threshold(0.0000)? ==> 0.0226
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0226)
networkOutput[2] := neuronOutput[10](0.0226)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.1001) * output[3](0.6750) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.6750) * output[3](0.6750) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.1001) * output[4](0.6750) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.1001) * output[4](0.6750) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.6750) * output[4](0.6750) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.1001) * output[5](0.6750) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.6750) * output[5](0.6750) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=065 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.1001) * weight[1,1](0.1000) => 0.1100
activation[2] = self(1.1001) * weight[2,2](0.1000) => 0.1100
activation[3] = self(0.6750) * weight[3,3](0.1000) => 0.0675
activation[4] = self(0.6750) * weight[4,4](0.1000) => 0.0675
activation[5] = self(0.6750) * weight[5,5](0.1000) => 0.0675
activation[6] = self(0.0226) * weight[6,6](0.1000) => 0.0023
activation[7] = self(0.0226) * weight[7,7](0.1000) => 0.0023
activation[8] = self(0.0226) * weight[8,8](0.1000) => 0.0023
activation[9] = self(0.0226) * weight[9,9](0.1000) => 0.0023
activation[10] = self(0.0226) * weight[10,10](0.1000) => 0.0023
  b. Update inputs from other neurons
activation[1] = self(0.1100) + network_input[1]=0.0000 => 0.1100
activation[2] = self(0.1100) + network_input[2]=0.0000 => 0.1100
activation[3] = self(0.0675) + output[1](1.1001) * 1.0000 => 1.1676
activation[3] = self(1.1676) + output[7](0.0226) * -0.5000 => 1.1563
activation[3] = self(1.1563) + output[8](0.0226) * -0.5000 => 1.1450
activation[4] = self(0.0675) + output[1](1.1001) * 0.5000 => 0.6176
activation[4] = self(0.6176) + output[2](1.1001) * 0.5000 => 1.1676
activation[4] = self(1.1676) + output[6](0.0226) * -0.5000 => 1.1563
activation[4] = self(1.1563) + output[8](0.0226) * -0.5000 => 1.1450
activation[5] = self(0.0675) + output[2](1.1001) * 1.0000 => 1.1676
activation[5] = self(1.1676) + output[6](0.0226) * -0.5000 => 1.1563
activation[5] = self(1.1563) + output[7](0.0226) * -0.5000 => 1.1450
activation[6] = self(0.0023) + output[3](0.6750) * 1.0000 => 0.6772
activation[7] = self(0.0023) + output[4](0.6750) * 1.0000 => 0.6772
activation[8] = self(0.0023) + output[5](0.6750) * 1.0000 => 0.6772
activation[9] = self(0.0023) + output[3](0.6750) * 0.7500 => 0.5085
activation[9] = self(0.5085) + output[4](0.6750) * 0.2500 => 0.6772
activation[10] = self(0.0023) + output[4](0.6750) * 0.2500 => 0.1710
activation[10] = self(0.1710) + output[5](0.6750) * 0.7500 => 0.6772
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.1100) > threshold(0.0000)? ==> 0.1100
neuronOutput[2]: activation(0.1100) > threshold(0.0000)? ==> 0.1100
neuronOutput[3]: activation(1.1450) > threshold(0.0000)? ==> 1.1450
neuronOutput[4]: activation(1.1450) > threshold(0.0000)? ==> 1.1450
neuronOutput[5]: activation(1.1450) > threshold(0.0000)? ==> 1.1450
neuronOutput[6]: activation(0.6772) > threshold(0.0000)? ==> 0.6772
neuronOutput[7]: activation(0.6772) > threshold(0.0000)? ==> 0.6772
neuronOutput[8]: activation(0.6772) > threshold(0.0000)? ==> 0.6772
neuronOutput[9]: activation(0.6772) > threshold(0.0000)? ==> 0.6772
neuronOutput[10]: activation(0.6772) > threshold(0.0000)? ==> 0.6772
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.6772)
networkOutput[2] := neuronOutput[10](0.6772)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.1100) * output[3](1.1450) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](1.1450) * output[3](1.1450) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.1100) * output[4](1.1450) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.1100) * output[4](1.1450) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](1.1450) * output[4](1.1450) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.1100) * output[5](1.1450) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](1.1450) * output[5](1.1450) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=066 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.1100) * weight[1,1](0.1000) => 0.0110
activation[2] = self(0.1100) * weight[2,2](0.1000) => 0.0110
activation[3] = self(1.1450) * weight[3,3](0.1000) => 0.1145
activation[4] = self(1.1450) * weight[4,4](0.1000) => 0.1145
activation[5] = self(1.1450) * weight[5,5](0.1000) => 0.1145
activation[6] = self(0.6772) * weight[6,6](0.1000) => 0.0677
activation[7] = self(0.6772) * weight[7,7](0.1000) => 0.0677
activation[8] = self(0.6772) * weight[8,8](0.1000) => 0.0677
activation[9] = self(0.6772) * weight[9,9](0.1000) => 0.0677
activation[10] = self(0.6772) * weight[10,10](0.1000) => 0.0677
  b. Update inputs from other neurons
activation[1] = self(0.0110) + network_input[1]=0.0000 => 0.0110
activation[2] = self(0.0110) + network_input[2]=0.0000 => 0.0110
activation[3] = self(0.1145) + output[1](0.1100) * 1.0000 => 0.2245
activation[3] = self(0.2245) + output[7](0.6772) * -0.5000 => -0.1141
activation[3] = self(-0.1141) + output[8](0.6772) * -0.5000 => -0.4527
activation[4] = self(0.1145) + output[1](0.1100) * 0.5000 => 0.1695
activation[4] = self(0.1695) + output[2](0.1100) * 0.5000 => 0.2245
activation[4] = self(0.2245) + output[6](0.6772) * -0.5000 => -0.1141
activation[4] = self(-0.1141) + output[8](0.6772) * -0.5000 => -0.4527
activation[5] = self(0.1145) + output[2](0.1100) * 1.0000 => 0.2245
activation[5] = self(0.2245) + output[6](0.6772) * -0.5000 => -0.1141
activation[5] = self(-0.1141) + output[7](0.6772) * -0.5000 => -0.4527
activation[6] = self(0.0677) + output[3](1.1450) * 1.0000 => 1.2127
activation[7] = self(0.0677) + output[4](1.1450) * 1.0000 => 1.2127
activation[8] = self(0.0677) + output[5](1.1450) * 1.0000 => 1.2127
activation[9] = self(0.0677) + output[3](1.1450) * 0.7500 => 0.9265
activation[9] = self(0.9265) + output[4](1.1450) * 0.2500 => 1.2127
activation[10] = self(0.0677) + output[4](1.1450) * 0.2500 => 0.3540
activation[10] = self(0.3540) + output[5](1.1450) * 0.7500 => 1.2127
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0110) > threshold(0.0000)? ==> 0.0110
neuronOutput[2]: activation(0.0110) > threshold(0.0000)? ==> 0.0110
neuronOutput[3]: activation(-0.4527) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-0.4527) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-0.4527) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(1.2127) > threshold(0.0000)? ==> 1.2127
neuronOutput[7]: activation(1.2127) > threshold(0.0000)? ==> 1.2127
neuronOutput[8]: activation(1.2127) > threshold(0.0000)? ==> 1.2127
neuronOutput[9]: activation(1.2127) > threshold(0.0000)? ==> 1.2127
neuronOutput[10]: activation(1.2127) > threshold(0.0000)? ==> 1.2127
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](1.2127)
networkOutput[2] := neuronOutput[10](1.2127)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.0110) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.0110) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.0110) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.0110) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=067 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0110) * weight[1,1](0.1000) => 0.0011
activation[2] = self(0.0110) * weight[2,2](0.1000) => 0.0011
activation[3] = self(-0.4527) * weight[3,3](0.1000) => -0.0453
activation[4] = self(-0.4527) * weight[4,4](0.1000) => -0.0453
activation[5] = self(-0.4527) * weight[5,5](0.1000) => -0.0453
activation[6] = self(1.2127) * weight[6,6](0.1000) => 0.1213
activation[7] = self(1.2127) * weight[7,7](0.1000) => 0.1213
activation[8] = self(1.2127) * weight[8,8](0.1000) => 0.1213
activation[9] = self(1.2127) * weight[9,9](0.1000) => 0.1213
activation[10] = self(1.2127) * weight[10,10](0.1000) => 0.1213
  b. Update inputs from other neurons
activation[1] = self(0.0011) + network_input[1]=0.0000 => 0.0011
activation[2] = self(0.0011) + network_input[2]=0.0000 => 0.0011
activation[3] = self(-0.0453) + output[1](0.0110) * 1.0000 => -0.0343
activation[3] = self(-0.0343) + output[7](1.2127) * -0.5000 => -0.6406
activation[3] = self(-0.6406) + output[8](1.2127) * -0.5000 => -1.2470
activation[4] = self(-0.0453) + output[1](0.0110) * 0.5000 => -0.0398
activation[4] = self(-0.0398) + output[2](0.0110) * 0.5000 => -0.0343
activation[4] = self(-0.0343) + output[6](1.2127) * -0.5000 => -0.6406
activation[4] = self(-0.6406) + output[8](1.2127) * -0.5000 => -1.2470
activation[5] = self(-0.0453) + output[2](0.0110) * 1.0000 => -0.0343
activation[5] = self(-0.0343) + output[6](1.2127) * -0.5000 => -0.6406
activation[5] = self(-0.6406) + output[7](1.2127) * -0.5000 => -1.2470
activation[6] = self(0.1213) + output[3](0.0000) * 1.0000 => 0.1213
activation[7] = self(0.1213) + output[4](0.0000) * 1.0000 => 0.1213
activation[8] = self(0.1213) + output[5](0.0000) * 1.0000 => 0.1213
activation[9] = self(0.1213) + output[3](0.0000) * 0.7500 => 0.1213
activation[9] = self(0.1213) + output[4](0.0000) * 0.2500 => 0.1213
activation[10] = self(0.1213) + output[4](0.0000) * 0.2500 => 0.1213
activation[10] = self(0.1213) + output[5](0.0000) * 0.7500 => 0.1213
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0011) > threshold(0.0000)? ==> 0.0011
neuronOutput[2]: activation(0.0011) > threshold(0.0000)? ==> 0.0011
neuronOutput[3]: activation(-1.2470) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-1.2470) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-1.2470) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.1213) > threshold(0.0000)? ==> 0.1213
neuronOutput[7]: activation(0.1213) > threshold(0.0000)? ==> 0.1213
neuronOutput[8]: activation(0.1213) > threshold(0.0000)? ==> 0.1213
neuronOutput[9]: activation(0.1213) > threshold(0.0000)? ==> 0.1213
neuronOutput[10]: activation(0.1213) > threshold(0.0000)? ==> 0.1213
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.1213)
networkOutput[2] := neuronOutput[10](0.1213)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.0011) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.0011) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.0011) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.0011) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=068 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0011) * weight[1,1](0.1000) => 0.0001
activation[2] = self(0.0011) * weight[2,2](0.1000) => 0.0001
activation[3] = self(-1.2470) * weight[3,3](0.1000) => -0.1247
activation[4] = self(-1.2470) * weight[4,4](0.1000) => -0.1247
activation[5] = self(-1.2470) * weight[5,5](0.1000) => -0.1247
activation[6] = self(0.1213) * weight[6,6](0.1000) => 0.0121
activation[7] = self(0.1213) * weight[7,7](0.1000) => 0.0121
activation[8] = self(0.1213) * weight[8,8](0.1000) => 0.0121
activation[9] = self(0.1213) * weight[9,9](0.1000) => 0.0121
activation[10] = self(0.1213) * weight[10,10](0.1000) => 0.0121
  b. Update inputs from other neurons
activation[1] = self(0.0001) + network_input[1]=0.0000 => 0.0001
activation[2] = self(0.0001) + network_input[2]=0.0000 => 0.0001
activation[3] = self(-0.1247) + output[1](0.0011) * 1.0000 => -0.1236
activation[3] = self(-0.1236) + output[7](0.1213) * -0.5000 => -0.1842
activation[3] = self(-0.1842) + output[8](0.1213) * -0.5000 => -0.2449
activation[4] = self(-0.1247) + output[1](0.0011) * 0.5000 => -0.1242
activation[4] = self(-0.1242) + output[2](0.0011) * 0.5000 => -0.1236
activation[4] = self(-0.1236) + output[6](0.1213) * -0.5000 => -0.1842
activation[4] = self(-0.1842) + output[8](0.1213) * -0.5000 => -0.2449
activation[5] = self(-0.1247) + output[2](0.0011) * 1.0000 => -0.1236
activation[5] = self(-0.1236) + output[6](0.1213) * -0.5000 => -0.1842
activation[5] = self(-0.1842) + output[7](0.1213) * -0.5000 => -0.2449
activation[6] = self(0.0121) + output[3](0.0000) * 1.0000 => 0.0121
activation[7] = self(0.0121) + output[4](0.0000) * 1.0000 => 0.0121
activation[8] = self(0.0121) + output[5](0.0000) * 1.0000 => 0.0121
activation[9] = self(0.0121) + output[3](0.0000) * 0.7500 => 0.0121
activation[9] = self(0.0121) + output[4](0.0000) * 0.2500 => 0.0121
activation[10] = self(0.0121) + output[4](0.0000) * 0.2500 => 0.0121
activation[10] = self(0.0121) + output[5](0.0000) * 0.7500 => 0.0121
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0001) > threshold(0.0000)? ==> 0.0001
neuronOutput[2]: activation(0.0001) > threshold(0.0000)? ==> 0.0001
neuronOutput[3]: activation(-0.2449) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-0.2449) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-0.2449) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.0121) > threshold(0.0000)? ==> 0.0121
neuronOutput[7]: activation(0.0121) > threshold(0.0000)? ==> 0.0121
neuronOutput[8]: activation(0.0121) > threshold(0.0000)? ==> 0.0121
neuronOutput[9]: activation(0.0121) > threshold(0.0000)? ==> 0.0121
neuronOutput[10]: activation(0.0121) > threshold(0.0000)? ==> 0.0121
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0121)
networkOutput[2] := neuronOutput[10](0.0121)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.0001) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.0001) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.0001) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.0001) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=069 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0001) * weight[1,1](0.1000) => 0.0000
activation[2] = self(0.0001) * weight[2,2](0.1000) => 0.0000
activation[3] = self(-0.2449) * weight[3,3](0.1000) => -0.0245
activation[4] = self(-0.2449) * weight[4,4](0.1000) => -0.0245
activation[5] = self(-0.2449) * weight[5,5](0.1000) => -0.0245
activation[6] = self(0.0121) * weight[6,6](0.1000) => 0.0012
activation[7] = self(0.0121) * weight[7,7](0.1000) => 0.0012
activation[8] = self(0.0121) * weight[8,8](0.1000) => 0.0012
activation[9] = self(0.0121) * weight[9,9](0.1000) => 0.0012
activation[10] = self(0.0121) * weight[10,10](0.1000) => 0.0012
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0245) + output[1](0.0001) * 1.0000 => -0.0244
activation[3] = self(-0.0244) + output[7](0.0121) * -0.5000 => -0.0304
activation[3] = self(-0.0304) + output[8](0.0121) * -0.5000 => -0.0365
activation[4] = self(-0.0245) + output[1](0.0001) * 0.5000 => -0.0244
activation[4] = self(-0.0244) + output[2](0.0001) * 0.5000 => -0.0244
activation[4] = self(-0.0244) + output[6](0.0121) * -0.5000 => -0.0304
activation[4] = self(-0.0304) + output[8](0.0121) * -0.5000 => -0.0365
activation[5] = self(-0.0245) + output[2](0.0001) * 1.0000 => -0.0244
activation[5] = self(-0.0244) + output[6](0.0121) * -0.5000 => -0.0304
activation[5] = self(-0.0304) + output[7](0.0121) * -0.5000 => -0.0365
activation[6] = self(0.0012) + output[3](0.0000) * 1.0000 => 0.0012
activation[7] = self(0.0012) + output[4](0.0000) * 1.0000 => 0.0012
activation[8] = self(0.0012) + output[5](0.0000) * 1.0000 => 0.0012
activation[9] = self(0.0012) + output[3](0.0000) * 0.7500 => 0.0012
activation[9] = self(0.0012) + output[4](0.0000) * 0.2500 => 0.0012
activation[10] = self(0.0012) + output[4](0.0000) * 0.2500 => 0.0012
activation[10] = self(0.0012) + output[5](0.0000) * 0.7500 => 0.0012
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(-0.0365) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-0.0365) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-0.0365) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.0012) > threshold(0.0000)? ==> 0.0012
neuronOutput[7]: activation(0.0012) > threshold(0.0000)? ==> 0.0012
neuronOutput[8]: activation(0.0012) > threshold(0.0000)? ==> 0.0012
neuronOutput[9]: activation(0.0012) > threshold(0.0000)? ==> 0.0012
neuronOutput[10]: activation(0.0012) > threshold(0.0000)? ==> 0.0012
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0012)
networkOutput[2] := neuronOutput[10](0.0012)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.0000) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.0000) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.0000) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.0000) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=070 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.1000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.1000) => 0.0000
activation[3] = self(-0.0365) * weight[3,3](0.1000) => -0.0037
activation[4] = self(-0.0365) * weight[4,4](0.1000) => -0.0037
activation[5] = self(-0.0365) * weight[5,5](0.1000) => -0.0037
activation[6] = self(0.0012) * weight[6,6](0.1000) => 0.0001
activation[7] = self(0.0012) * weight[7,7](0.1000) => 0.0001
activation[8] = self(0.0012) * weight[8,8](0.1000) => 0.0001
activation[9] = self(0.0012) * weight[9,9](0.1000) => 0.0001
activation[10] = self(0.0012) * weight[10,10](0.1000) => 0.0001
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0037) + output[1](0.0000) * 1.0000 => -0.0036
activation[3] = self(-0.0036) + output[7](0.0012) * -0.5000 => -0.0042
activation[3] = self(-0.0042) + output[8](0.0012) * -0.5000 => -0.0049
activation[4] = self(-0.0037) + output[1](0.0000) * 0.5000 => -0.0036
activation[4] = self(-0.0036) + output[2](0.0000) * 0.5000 => -0.0036
activation[4] = self(-0.0036) + output[6](0.0012) * -0.5000 => -0.0042
activation[4] = self(-0.0042) + output[8](0.0012) * -0.5000 => -0.0049
activation[5] = self(-0.0037) + output[2](0.0000) * 1.0000 => -0.0036
activation[5] = self(-0.0036) + output[6](0.0012) * -0.5000 => -0.0042
activation[5] = self(-0.0042) + output[7](0.0012) * -0.5000 => -0.0049
activation[6] = self(0.0001) + output[3](0.0000) * 1.0000 => 0.0001
activation[7] = self(0.0001) + output[4](0.0000) * 1.0000 => 0.0001
activation[8] = self(0.0001) + output[5](0.0000) * 1.0000 => 0.0001
activation[9] = self(0.0001) + output[3](0.0000) * 0.7500 => 0.0001
activation[9] = self(0.0001) + output[4](0.0000) * 0.2500 => 0.0001
activation[10] = self(0.0001) + output[4](0.0000) * 0.2500 => 0.0001
activation[10] = self(0.0001) + output[5](0.0000) * 0.7500 => 0.0001
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(-0.0049) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-0.0049) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-0.0049) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.0001) > threshold(0.0000)? ==> 0.0001
neuronOutput[7]: activation(0.0001) > threshold(0.0000)? ==> 0.0001
neuronOutput[8]: activation(0.0001) > threshold(0.0000)? ==> 0.0001
neuronOutput[9]: activation(0.0001) > threshold(0.0000)? ==> 0.0001
neuronOutput[10]: activation(0.0001) > threshold(0.0000)? ==> 0.0001
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0001)
networkOutput[2] := neuronOutput[10](0.0001)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.0000) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.0000) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.0000) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.0000) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=071 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.1000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.1000) => 0.0000
activation[3] = self(-0.0049) * weight[3,3](0.1000) => -0.0005
activation[4] = self(-0.0049) * weight[4,4](0.1000) => -0.0005
activation[5] = self(-0.0049) * weight[5,5](0.1000) => -0.0005
activation[6] = self(0.0001) * weight[6,6](0.1000) => 0.0000
activation[7] = self(0.0001) * weight[7,7](0.1000) => 0.0000
activation[8] = self(0.0001) * weight[8,8](0.1000) => 0.0000
activation[9] = self(0.0001) * weight[9,9](0.1000) => 0.0000
activation[10] = self(0.0001) * weight[10,10](0.1000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=0.0000 => 0.0000
activation[2] = self(0.0000) + network_input[2]=0.0000 => 0.0000
activation[3] = self(-0.0005) + output[1](0.0000) * 1.0000 => -0.0005
activation[3] = self(-0.0005) + output[7](0.0001) * -0.5000 => -0.0005
activation[3] = self(-0.0005) + output[8](0.0001) * -0.5000 => -0.0006
activation[4] = self(-0.0005) + output[1](0.0000) * 0.5000 => -0.0005
activation[4] = self(-0.0005) + output[2](0.0000) * 0.5000 => -0.0005
activation[4] = self(-0.0005) + output[6](0.0001) * -0.5000 => -0.0005
activation[4] = self(-0.0005) + output[8](0.0001) * -0.5000 => -0.0006
activation[5] = self(-0.0005) + output[2](0.0000) * 1.0000 => -0.0005
activation[5] = self(-0.0005) + output[6](0.0001) * -0.5000 => -0.0005
activation[5] = self(-0.0005) + output[7](0.0001) * -0.5000 => -0.0006
activation[6] = self(0.0000) + output[3](0.0000) * 1.0000 => 0.0000
activation[7] = self(0.0000) + output[4](0.0000) * 1.0000 => 0.0000
activation[8] = self(0.0000) + output[5](0.0000) * 1.0000 => 0.0000
activation[9] = self(0.0000) + output[3](0.0000) * 0.7500 => 0.0000
activation[9] = self(0.0000) + output[4](0.0000) * 0.2500 => 0.0000
activation[10] = self(0.0000) + output[4](0.0000) * 0.2500 => 0.0000
activation[10] = self(0.0000) + output[5](0.0000) * 0.7500 => 0.0000
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[2]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[3]: activation(-0.0006) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-0.0006) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-0.0006) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0000)
networkOutput[2] := neuronOutput[10](0.0000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.0000) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.0000) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.0000) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.0000) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=072 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0000) * weight[1,1](0.1000) => 0.0000
activation[2] = self(0.0000) * weight[2,2](0.1000) => 0.0000
activation[3] = self(-0.0006) * weight[3,3](0.1000) => -0.0001
activation[4] = self(-0.0006) * weight[4,4](0.1000) => -0.0001
activation[5] = self(-0.0006) * weight[5,5](0.1000) => -0.0001
activation[6] = self(0.0000) * weight[6,6](0.1000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.1000) => 0.0000
activation[8] = self(0.0000) * weight[8,8](0.1000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.1000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.0000) + network_input[1]=1.0000 => 1.0000
activation[2] = self(0.0000) + network_input[2]=1.0000 => 1.0000
activation[3] = self(-0.0001) + output[1](0.0000) * 1.0000 => -0.0001
activation[3] = self(-0.0001) + output[7](0.0000) * -0.5000 => -0.0001
activation[3] = self(-0.0001) + output[8](0.0000) * -0.5000 => -0.0001
activation[4] = self(-0.0001) + output[1](0.0000) * 0.5000 => -0.0001
activation[4] = self(-0.0001) + output[2](0.0000) * 0.5000 => -0.0001
activation[4] = self(-0.0001) + output[6](0.0000) * -0.5000 => -0.0001
activation[4] = self(-0.0001) + output[8](0.0000) * -0.5000 => -0.0001
activation[5] = self(-0.0001) + output[2](0.0000) * 1.0000 => -0.0001
activation[5] = self(-0.0001) + output[6](0.0000) * -0.5000 => -0.0001
activation[5] = self(-0.0001) + output[7](0.0000) * -0.5000 => -0.0001
activation[6] = self(0.0000) + output[3](0.0000) * 1.0000 => 0.0000
activation[7] = self(0.0000) + output[4](0.0000) * 1.0000 => 0.0000
activation[8] = self(0.0000) + output[5](0.0000) * 1.0000 => 0.0000
activation[9] = self(0.0000) + output[3](0.0000) * 0.7500 => 0.0000
activation[9] = self(0.0000) + output[4](0.0000) * 0.2500 => 0.0000
activation[10] = self(0.0000) + output[4](0.0000) * 0.2500 => 0.0000
activation[10] = self(0.0000) + output[5](0.0000) * 0.7500 => 0.0000
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[2]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[3]: activation(-0.0001) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-0.0001) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-0.0001) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0000)
networkOutput[2] := neuronOutput[10](0.0000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.0000) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.0000) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.0000) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.0000) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=073 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0000) * weight[1,1](0.1000) => 0.1000
activation[2] = self(1.0000) * weight[2,2](0.1000) => 0.1000
activation[3] = self(-0.0001) * weight[3,3](0.1000) => -0.0000
activation[4] = self(-0.0001) * weight[4,4](0.1000) => -0.0000
activation[5] = self(-0.0001) * weight[5,5](0.1000) => -0.0000
activation[6] = self(0.0000) * weight[6,6](0.1000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.1000) => 0.0000
activation[8] = self(0.0000) * weight[8,8](0.1000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.1000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.1000) + network_input[1]=1.0000 => 1.1000
activation[2] = self(0.1000) + network_input[2]=1.0000 => 1.1000
activation[3] = self(-0.0000) + output[1](1.0000) * 1.0000 => 1.0000
activation[3] = self(1.0000) + output[7](0.0000) * -0.5000 => 1.0000
activation[3] = self(1.0000) + output[8](0.0000) * -0.5000 => 1.0000
activation[4] = self(-0.0000) + output[1](1.0000) * 0.5000 => 0.5000
activation[4] = self(0.5000) + output[2](1.0000) * 0.5000 => 1.0000
activation[4] = self(1.0000) + output[6](0.0000) * -0.5000 => 1.0000
activation[4] = self(1.0000) + output[8](0.0000) * -0.5000 => 1.0000
activation[5] = self(-0.0000) + output[2](1.0000) * 1.0000 => 1.0000
activation[5] = self(1.0000) + output[6](0.0000) * -0.5000 => 1.0000
activation[5] = self(1.0000) + output[7](0.0000) * -0.5000 => 1.0000
activation[6] = self(0.0000) + output[3](0.0000) * 1.0000 => 0.0000
activation[7] = self(0.0000) + output[4](0.0000) * 1.0000 => 0.0000
activation[8] = self(0.0000) + output[5](0.0000) * 1.0000 => 0.0000
activation[9] = self(0.0000) + output[3](0.0000) * 0.7500 => 0.0000
activation[9] = self(0.0000) + output[4](0.0000) * 0.2500 => 0.0000
activation[10] = self(0.0000) + output[4](0.0000) * 0.2500 => 0.0000
activation[10] = self(0.0000) + output[5](0.0000) * 0.7500 => 0.0000
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.1000) > threshold(0.0000)? ==> 1.1000
neuronOutput[2]: activation(1.1000) > threshold(0.0000)? ==> 1.1000
neuronOutput[3]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[4]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[5]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[6]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[7]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[8]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[9]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
neuronOutput[10]: activation(0.0000) > threshold(0.0000)? ==> 0.0000
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0000)
networkOutput[2] := neuronOutput[10](0.0000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.1000) * output[3](1.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](1.0000) * output[3](1.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.1000) * output[4](1.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.1000) * output[4](1.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](1.0000) * output[4](1.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.1000) * output[5](1.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](1.0000) * output[5](1.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=074 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.1000) * weight[1,1](0.1000) => 0.1100
activation[2] = self(1.1000) * weight[2,2](0.1000) => 0.1100
activation[3] = self(1.0000) * weight[3,3](0.1000) => 0.1000
activation[4] = self(1.0000) * weight[4,4](0.1000) => 0.1000
activation[5] = self(1.0000) * weight[5,5](0.1000) => 0.1000
activation[6] = self(0.0000) * weight[6,6](0.1000) => 0.0000
activation[7] = self(0.0000) * weight[7,7](0.1000) => 0.0000
activation[8] = self(0.0000) * weight[8,8](0.1000) => 0.0000
activation[9] = self(0.0000) * weight[9,9](0.1000) => 0.0000
activation[10] = self(0.0000) * weight[10,10](0.1000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.1100) + network_input[1]=1.0000 => 1.1100
activation[2] = self(0.1100) + network_input[2]=1.0000 => 1.1100
activation[3] = self(0.1000) + output[1](1.1000) * 1.0000 => 1.2000
activation[3] = self(1.2000) + output[7](0.0000) * -0.5000 => 1.2000
activation[3] = self(1.2000) + output[8](0.0000) * -0.5000 => 1.2000
activation[4] = self(0.1000) + output[1](1.1000) * 0.5000 => 0.6500
activation[4] = self(0.6500) + output[2](1.1000) * 0.5000 => 1.2000
activation[4] = self(1.2000) + output[6](0.0000) * -0.5000 => 1.2000
activation[4] = self(1.2000) + output[8](0.0000) * -0.5000 => 1.2000
activation[5] = self(0.1000) + output[2](1.1000) * 1.0000 => 1.2000
activation[5] = self(1.2000) + output[6](0.0000) * -0.5000 => 1.2000
activation[5] = self(1.2000) + output[7](0.0000) * -0.5000 => 1.2000
activation[6] = self(0.0000) + output[3](1.0000) * 1.0000 => 1.0000
activation[7] = self(0.0000) + output[4](1.0000) * 1.0000 => 1.0000
activation[8] = self(0.0000) + output[5](1.0000) * 1.0000 => 1.0000
activation[9] = self(0.0000) + output[3](1.0000) * 0.7500 => 0.7500
activation[9] = self(0.7500) + output[4](1.0000) * 0.2500 => 1.0000
activation[10] = self(0.0000) + output[4](1.0000) * 0.2500 => 0.2500
activation[10] = self(0.2500) + output[5](1.0000) * 0.7500 => 1.0000
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.1100) > threshold(0.0000)? ==> 1.1100
neuronOutput[2]: activation(1.1100) > threshold(0.0000)? ==> 1.1100
neuronOutput[3]: activation(1.2000) > threshold(0.0000)? ==> 1.2000
neuronOutput[4]: activation(1.2000) > threshold(0.0000)? ==> 1.2000
neuronOutput[5]: activation(1.2000) > threshold(0.0000)? ==> 1.2000
neuronOutput[6]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[7]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[8]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[9]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
neuronOutput[10]: activation(1.0000) > threshold(0.0000)? ==> 1.0000
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](1.0000)
networkOutput[2] := neuronOutput[10](1.0000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.1100) * output[3](1.2000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](1.2000) * output[3](1.2000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.1100) * output[4](1.2000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.1100) * output[4](1.2000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](1.2000) * output[4](1.2000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.1100) * output[5](1.2000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](1.2000) * output[5](1.2000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=075 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.1100) * weight[1,1](0.1000) => 0.1110
activation[2] = self(1.1100) * weight[2,2](0.1000) => 0.1110
activation[3] = self(1.2000) * weight[3,3](0.1000) => 0.1200
activation[4] = self(1.2000) * weight[4,4](0.1000) => 0.1200
activation[5] = self(1.2000) * weight[5,5](0.1000) => 0.1200
activation[6] = self(1.0000) * weight[6,6](0.1000) => 0.1000
activation[7] = self(1.0000) * weight[7,7](0.1000) => 0.1000
activation[8] = self(1.0000) * weight[8,8](0.1000) => 0.1000
activation[9] = self(1.0000) * weight[9,9](0.1000) => 0.1000
activation[10] = self(1.0000) * weight[10,10](0.1000) => 0.1000
  b. Update inputs from other neurons
activation[1] = self(0.1110) + network_input[1]=1.0000 => 1.1110
activation[2] = self(0.1110) + network_input[2]=1.0000 => 1.1110
activation[3] = self(0.1200) + output[1](1.1100) * 1.0000 => 1.2300
activation[3] = self(1.2300) + output[7](1.0000) * -0.5000 => 0.7300
activation[3] = self(0.7300) + output[8](1.0000) * -0.5000 => 0.2300
activation[4] = self(0.1200) + output[1](1.1100) * 0.5000 => 0.6750
activation[4] = self(0.6750) + output[2](1.1100) * 0.5000 => 1.2300
activation[4] = self(1.2300) + output[6](1.0000) * -0.5000 => 0.7300
activation[4] = self(0.7300) + output[8](1.0000) * -0.5000 => 0.2300
activation[5] = self(0.1200) + output[2](1.1100) * 1.0000 => 1.2300
activation[5] = self(1.2300) + output[6](1.0000) * -0.5000 => 0.7300
activation[5] = self(0.7300) + output[7](1.0000) * -0.5000 => 0.2300
activation[6] = self(0.1000) + output[3](1.2000) * 1.0000 => 1.3000
activation[7] = self(0.1000) + output[4](1.2000) * 1.0000 => 1.3000
activation[8] = self(0.1000) + output[5](1.2000) * 1.0000 => 1.3000
activation[9] = self(0.1000) + output[3](1.2000) * 0.7500 => 1.0000
activation[9] = self(1.0000) + output[4](1.2000) * 0.2500 => 1.3000
activation[10] = self(0.1000) + output[4](1.2000) * 0.2500 => 0.4000
activation[10] = self(0.4000) + output[5](1.2000) * 0.7500 => 1.3000
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.1110) > threshold(0.0000)? ==> 1.1110
neuronOutput[2]: activation(1.1110) > threshold(0.0000)? ==> 1.1110
neuronOutput[3]: activation(0.2300) > threshold(0.0000)? ==> 0.2300
neuronOutput[4]: activation(0.2300) > threshold(0.0000)? ==> 0.2300
neuronOutput[5]: activation(0.2300) > threshold(0.0000)? ==> 0.2300
neuronOutput[6]: activation(1.3000) > threshold(0.0000)? ==> 1.3000
neuronOutput[7]: activation(1.3000) > threshold(0.0000)? ==> 1.3000
neuronOutput[8]: activation(1.3000) > threshold(0.0000)? ==> 1.3000
neuronOutput[9]: activation(1.3000) > threshold(0.0000)? ==> 1.3000
neuronOutput[10]: activation(1.3000) > threshold(0.0000)? ==> 1.3000
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](1.3000)
networkOutput[2] := neuronOutput[10](1.3000)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.1110) * output[3](0.2300) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.2300) * output[3](0.2300) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.1110) * output[4](0.2300) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.1110) * output[4](0.2300) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.2300) * output[4](0.2300) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.1110) * output[5](0.2300) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.2300) * output[5](0.2300) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=076 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.1110) * weight[1,1](0.1000) => 0.1111
activation[2] = self(1.1110) * weight[2,2](0.1000) => 0.1111
activation[3] = self(0.2300) * weight[3,3](0.1000) => 0.0230
activation[4] = self(0.2300) * weight[4,4](0.1000) => 0.0230
activation[5] = self(0.2300) * weight[5,5](0.1000) => 0.0230
activation[6] = self(1.3000) * weight[6,6](0.1000) => 0.1300
activation[7] = self(1.3000) * weight[7,7](0.1000) => 0.1300
activation[8] = self(1.3000) * weight[8,8](0.1000) => 0.1300
activation[9] = self(1.3000) * weight[9,9](0.1000) => 0.1300
activation[10] = self(1.3000) * weight[10,10](0.1000) => 0.1300
  b. Update inputs from other neurons
activation[1] = self(0.1111) + network_input[1]=1.0000 => 1.1111
activation[2] = self(0.1111) + network_input[2]=1.0000 => 1.1111
activation[3] = self(0.0230) + output[1](1.1110) * 1.0000 => 1.1340
activation[3] = self(1.1340) + output[7](1.3000) * -0.5000 => 0.4840
activation[3] = self(0.4840) + output[8](1.3000) * -0.5000 => -0.1660
activation[4] = self(0.0230) + output[1](1.1110) * 0.5000 => 0.5785
activation[4] = self(0.5785) + output[2](1.1110) * 0.5000 => 1.1340
activation[4] = self(1.1340) + output[6](1.3000) * -0.5000 => 0.4840
activation[4] = self(0.4840) + output[8](1.3000) * -0.5000 => -0.1660
activation[5] = self(0.0230) + output[2](1.1110) * 1.0000 => 1.1340
activation[5] = self(1.1340) + output[6](1.3000) * -0.5000 => 0.4840
activation[5] = self(0.4840) + output[7](1.3000) * -0.5000 => -0.1660
activation[6] = self(0.1300) + output[3](0.2300) * 1.0000 => 0.3600
activation[7] = self(0.1300) + output[4](0.2300) * 1.0000 => 0.3600
activation[8] = self(0.1300) + output[5](0.2300) * 1.0000 => 0.3600
activation[9] = self(0.1300) + output[3](0.2300) * 0.7500 => 0.3025
activation[9] = self(0.3025) + output[4](0.2300) * 0.2500 => 0.3600
activation[10] = self(0.1300) + output[4](0.2300) * 0.2500 => 0.1875
activation[10] = self(0.1875) + output[5](0.2300) * 0.7500 => 0.3600
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.1111) > threshold(0.0000)? ==> 1.1111
neuronOutput[2]: activation(1.1111) > threshold(0.0000)? ==> 1.1111
neuronOutput[3]: activation(-0.1660) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-0.1660) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-0.1660) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.3600) > threshold(0.0000)? ==> 0.3600
neuronOutput[7]: activation(0.3600) > threshold(0.0000)? ==> 0.3600
neuronOutput[8]: activation(0.3600) > threshold(0.0000)? ==> 0.3600
neuronOutput[9]: activation(0.3600) > threshold(0.0000)? ==> 0.3600
neuronOutput[10]: activation(0.3600) > threshold(0.0000)? ==> 0.3600
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.3600)
networkOutput[2] := neuronOutput[10](0.3600)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.1111) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.1111) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.1111) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.1111) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=077 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.1111) * weight[1,1](0.1000) => 0.1111
activation[2] = self(1.1111) * weight[2,2](0.1000) => 0.1111
activation[3] = self(-0.1660) * weight[3,3](0.1000) => -0.0166
activation[4] = self(-0.1660) * weight[4,4](0.1000) => -0.0166
activation[5] = self(-0.1660) * weight[5,5](0.1000) => -0.0166
activation[6] = self(0.3600) * weight[6,6](0.1000) => 0.0360
activation[7] = self(0.3600) * weight[7,7](0.1000) => 0.0360
activation[8] = self(0.3600) * weight[8,8](0.1000) => 0.0360
activation[9] = self(0.3600) * weight[9,9](0.1000) => 0.0360
activation[10] = self(0.3600) * weight[10,10](0.1000) => 0.0360
  b. Update inputs from other neurons
activation[1] = self(0.1111) + network_input[1]=1.0000 => 1.1111
activation[2] = self(0.1111) + network_input[2]=1.0000 => 1.1111
activation[3] = self(-0.0166) + output[1](1.1111) * 1.0000 => 1.0945
activation[3] = self(1.0945) + output[7](0.3600) * -0.5000 => 0.9145
activation[3] = self(0.9145) + output[8](0.3600) * -0.5000 => 0.7345
activation[4] = self(-0.0166) + output[1](1.1111) * 0.5000 => 0.5390
activation[4] = self(0.5390) + output[2](1.1111) * 0.5000 => 1.0945
activation[4] = self(1.0945) + output[6](0.3600) * -0.5000 => 0.9145
activation[4] = self(0.9145) + output[8](0.3600) * -0.5000 => 0.7345
activation[5] = self(-0.0166) + output[2](1.1111) * 1.0000 => 1.0945
activation[5] = self(1.0945) + output[6](0.3600) * -0.5000 => 0.9145
activation[5] = self(0.9145) + output[7](0.3600) * -0.5000 => 0.7345
activation[6] = self(0.0360) + output[3](0.0000) * 1.0000 => 0.0360
activation[7] = self(0.0360) + output[4](0.0000) * 1.0000 => 0.0360
activation[8] = self(0.0360) + output[5](0.0000) * 1.0000 => 0.0360
activation[9] = self(0.0360) + output[3](0.0000) * 0.7500 => 0.0360
activation[9] = self(0.0360) + output[4](0.0000) * 0.2500 => 0.0360
activation[10] = self(0.0360) + output[4](0.0000) * 0.2500 => 0.0360
activation[10] = self(0.0360) + output[5](0.0000) * 0.7500 => 0.0360
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.1111) > threshold(0.0000)? ==> 1.1111
neuronOutput[2]: activation(1.1111) > threshold(0.0000)? ==> 1.1111
neuronOutput[3]: activation(0.7345) > threshold(0.0000)? ==> 0.7345
neuronOutput[4]: activation(0.7345) > threshold(0.0000)? ==> 0.7345
neuronOutput[5]: activation(0.7345) > threshold(0.0000)? ==> 0.7345
neuronOutput[6]: activation(0.0360) > threshold(0.0000)? ==> 0.0360
neuronOutput[7]: activation(0.0360) > threshold(0.0000)? ==> 0.0360
neuronOutput[8]: activation(0.0360) > threshold(0.0000)? ==> 0.0360
neuronOutput[9]: activation(0.0360) > threshold(0.0000)? ==> 0.0360
neuronOutput[10]: activation(0.0360) > threshold(0.0000)? ==> 0.0360
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0360)
networkOutput[2] := neuronOutput[10](0.0360)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.1111) * output[3](0.7345) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.7345) * output[3](0.7345) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.1111) * output[4](0.7345) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.1111) * output[4](0.7345) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.7345) * output[4](0.7345) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.1111) * output[5](0.7345) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.7345) * output[5](0.7345) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=078 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.1111) * weight[1,1](0.1000) => 0.1111
activation[2] = self(1.1111) * weight[2,2](0.1000) => 0.1111
activation[3] = self(0.7345) * weight[3,3](0.1000) => 0.0734
activation[4] = self(0.7345) * weight[4,4](0.1000) => 0.0734
activation[5] = self(0.7345) * weight[5,5](0.1000) => 0.0734
activation[6] = self(0.0360) * weight[6,6](0.1000) => 0.0036
activation[7] = self(0.0360) * weight[7,7](0.1000) => 0.0036
activation[8] = self(0.0360) * weight[8,8](0.1000) => 0.0036
activation[9] = self(0.0360) * weight[9,9](0.1000) => 0.0036
activation[10] = self(0.0360) * weight[10,10](0.1000) => 0.0036
  b. Update inputs from other neurons
activation[1] = self(0.1111) + network_input[1]=1.0000 => 1.1111
activation[2] = self(0.1111) + network_input[2]=1.0000 => 1.1111
activation[3] = self(0.0734) + output[1](1.1111) * 1.0000 => 1.1846
activation[3] = self(1.1846) + output[7](0.0360) * -0.5000 => 1.1666
activation[3] = self(1.1666) + output[8](0.0360) * -0.5000 => 1.1486
activation[4] = self(0.0734) + output[1](1.1111) * 0.5000 => 0.6290
activation[4] = self(0.6290) + output[2](1.1111) * 0.5000 => 1.1846
activation[4] = self(1.1846) + output[6](0.0360) * -0.5000 => 1.1666
activation[4] = self(1.1666) + output[8](0.0360) * -0.5000 => 1.1486
activation[5] = self(0.0734) + output[2](1.1111) * 1.0000 => 1.1846
activation[5] = self(1.1846) + output[6](0.0360) * -0.5000 => 1.1666
activation[5] = self(1.1666) + output[7](0.0360) * -0.5000 => 1.1486
activation[6] = self(0.0036) + output[3](0.7345) * 1.0000 => 0.7381
activation[7] = self(0.0036) + output[4](0.7345) * 1.0000 => 0.7381
activation[8] = self(0.0036) + output[5](0.7345) * 1.0000 => 0.7381
activation[9] = self(0.0036) + output[3](0.7345) * 0.7500 => 0.5545
activation[9] = self(0.5545) + output[4](0.7345) * 0.2500 => 0.7381
activation[10] = self(0.0036) + output[4](0.7345) * 0.2500 => 0.1872
activation[10] = self(0.1872) + output[5](0.7345) * 0.7500 => 0.7381
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.1111) > threshold(0.0000)? ==> 1.1111
neuronOutput[2]: activation(1.1111) > threshold(0.0000)? ==> 1.1111
neuronOutput[3]: activation(1.1486) > threshold(0.0000)? ==> 1.1486
neuronOutput[4]: activation(1.1486) > threshold(0.0000)? ==> 1.1486
neuronOutput[5]: activation(1.1486) > threshold(0.0000)? ==> 1.1486
neuronOutput[6]: activation(0.7381) > threshold(0.0000)? ==> 0.7381
neuronOutput[7]: activation(0.7381) > threshold(0.0000)? ==> 0.7381
neuronOutput[8]: activation(0.7381) > threshold(0.0000)? ==> 0.7381
neuronOutput[9]: activation(0.7381) > threshold(0.0000)? ==> 0.7381
neuronOutput[10]: activation(0.7381) > threshold(0.0000)? ==> 0.7381
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.7381)
networkOutput[2] := neuronOutput[10](0.7381)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.1111) * output[3](1.1486) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](1.1486) * output[3](1.1486) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.1111) * output[4](1.1486) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.1111) * output[4](1.1486) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](1.1486) * output[4](1.1486) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.1111) * output[5](1.1486) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](1.1486) * output[5](1.1486) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=079 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.1111) * weight[1,1](0.1000) => 0.1111
activation[2] = self(1.1111) * weight[2,2](0.1000) => 0.1111
activation[3] = self(1.1486) * weight[3,3](0.1000) => 0.1149
activation[4] = self(1.1486) * weight[4,4](0.1000) => 0.1149
activation[5] = self(1.1486) * weight[5,5](0.1000) => 0.1149
activation[6] = self(0.7381) * weight[6,6](0.1000) => 0.0738
activation[7] = self(0.7381) * weight[7,7](0.1000) => 0.0738
activation[8] = self(0.7381) * weight[8,8](0.1000) => 0.0738
activation[9] = self(0.7381) * weight[9,9](0.1000) => 0.0738
activation[10] = self(0.7381) * weight[10,10](0.1000) => 0.0738
  b. Update inputs from other neurons
activation[1] = self(0.1111) + network_input[1]=0.0000 => 0.1111
activation[2] = self(0.1111) + network_input[2]=0.0000 => 0.1111
activation[3] = self(0.1149) + output[1](1.1111) * 1.0000 => 1.2260
activation[3] = self(1.2260) + output[7](0.7381) * -0.5000 => 0.8569
activation[3] = self(0.8569) + output[8](0.7381) * -0.5000 => 0.4879
activation[4] = self(0.1149) + output[1](1.1111) * 0.5000 => 0.6704
activation[4] = self(0.6704) + output[2](1.1111) * 0.5000 => 1.2260
activation[4] = self(1.2260) + output[6](0.7381) * -0.5000 => 0.8569
activation[4] = self(0.8569) + output[8](0.7381) * -0.5000 => 0.4879
activation[5] = self(0.1149) + output[2](1.1111) * 1.0000 => 1.2260
activation[5] = self(1.2260) + output[6](0.7381) * -0.5000 => 0.8569
activation[5] = self(0.8569) + output[7](0.7381) * -0.5000 => 0.4879
activation[6] = self(0.0738) + output[3](1.1486) * 1.0000 => 1.2224
activation[7] = self(0.0738) + output[4](1.1486) * 1.0000 => 1.2224
activation[8] = self(0.0738) + output[5](1.1486) * 1.0000 => 1.2224
activation[9] = self(0.0738) + output[3](1.1486) * 0.7500 => 0.9352
activation[9] = self(0.9352) + output[4](1.1486) * 0.2500 => 1.2224
activation[10] = self(0.0738) + output[4](1.1486) * 0.2500 => 0.3609
activation[10] = self(0.3609) + output[5](1.1486) * 0.7500 => 1.2224
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.1111) > threshold(0.0000)? ==> 0.1111
neuronOutput[2]: activation(0.1111) > threshold(0.0000)? ==> 0.1111
neuronOutput[3]: activation(0.4879) > threshold(0.0000)? ==> 0.4879
neuronOutput[4]: activation(0.4879) > threshold(0.0000)? ==> 0.4879
neuronOutput[5]: activation(0.4879) > threshold(0.0000)? ==> 0.4879
neuronOutput[6]: activation(1.2224) > threshold(0.0000)? ==> 1.2224
neuronOutput[7]: activation(1.2224) > threshold(0.0000)? ==> 1.2224
neuronOutput[8]: activation(1.2224) > threshold(0.0000)? ==> 1.2224
neuronOutput[9]: activation(1.2224) > threshold(0.0000)? ==> 1.2224
neuronOutput[10]: activation(1.2224) > threshold(0.0000)? ==> 1.2224
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](1.2224)
networkOutput[2] := neuronOutput[10](1.2224)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.1111) * output[3](0.4879) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.4879) * output[3](0.4879) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.1111) * output[4](0.4879) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.1111) * output[4](0.4879) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.4879) * output[4](0.4879) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.1111) * output[5](0.4879) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.4879) * output[5](0.4879) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=080 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.1111) * weight[1,1](0.1000) => 0.0111
activation[2] = self(0.1111) * weight[2,2](0.1000) => 0.0111
activation[3] = self(0.4879) * weight[3,3](0.1000) => 0.0488
activation[4] = self(0.4879) * weight[4,4](0.1000) => 0.0488
activation[5] = self(0.4879) * weight[5,5](0.1000) => 0.0488
activation[6] = self(1.2224) * weight[6,6](0.1000) => 0.1222
activation[7] = self(1.2224) * weight[7,7](0.1000) => 0.1222
activation[8] = self(1.2224) * weight[8,8](0.1000) => 0.1222
activation[9] = self(1.2224) * weight[9,9](0.1000) => 0.1222
activation[10] = self(1.2224) * weight[10,10](0.1000) => 0.1222
  b. Update inputs from other neurons
activation[1] = self(0.0111) + network_input[1]=0.0000 => 0.0111
activation[2] = self(0.0111) + network_input[2]=0.0000 => 0.0111
activation[3] = self(0.0488) + output[1](0.1111) * 1.0000 => 0.1599
activation[3] = self(0.1599) + output[7](1.2224) * -0.5000 => -0.4513
activation[3] = self(-0.4513) + output[8](1.2224) * -0.5000 => -1.0625
activation[4] = self(0.0488) + output[1](0.1111) * 0.5000 => 0.1043
activation[4] = self(0.1043) + output[2](0.1111) * 0.5000 => 0.1599
activation[4] = self(0.1599) + output[6](1.2224) * -0.5000 => -0.4513
activation[4] = self(-0.4513) + output[8](1.2224) * -0.5000 => -1.0625
activation[5] = self(0.0488) + output[2](0.1111) * 1.0000 => 0.1599
activation[5] = self(0.1599) + output[6](1.2224) * -0.5000 => -0.4513
activation[5] = self(-0.4513) + output[7](1.2224) * -0.5000 => -1.0625
activation[6] = self(0.1222) + output[3](0.4879) * 1.0000 => 0.6101
activation[7] = self(0.1222) + output[4](0.4879) * 1.0000 => 0.6101
activation[8] = self(0.1222) + output[5](0.4879) * 1.0000 => 0.6101
activation[9] = self(0.1222) + output[3](0.4879) * 0.7500 => 0.4881
activation[9] = self(0.4881) + output[4](0.4879) * 0.2500 => 0.6101
activation[10] = self(0.1222) + output[4](0.4879) * 0.2500 => 0.2442
activation[10] = self(0.2442) + output[5](0.4879) * 0.7500 => 0.6101
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0111) > threshold(0.0000)? ==> 0.0111
neuronOutput[2]: activation(0.0111) > threshold(0.0000)? ==> 0.0111
neuronOutput[3]: activation(-1.0625) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-1.0625) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-1.0625) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.6101) > threshold(0.0000)? ==> 0.6101
neuronOutput[7]: activation(0.6101) > threshold(0.0000)? ==> 0.6101
neuronOutput[8]: activation(0.6101) > threshold(0.0000)? ==> 0.6101
neuronOutput[9]: activation(0.6101) > threshold(0.0000)? ==> 0.6101
neuronOutput[10]: activation(0.6101) > threshold(0.0000)? ==> 0.6101
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.6101)
networkOutput[2] := neuronOutput[10](0.6101)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.0111) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.0111) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.0111) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.0111) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=081 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0111) * weight[1,1](0.1000) => 0.0011
activation[2] = self(0.0111) * weight[2,2](0.1000) => 0.0011
activation[3] = self(-1.0625) * weight[3,3](0.1000) => -0.1062
activation[4] = self(-1.0625) * weight[4,4](0.1000) => -0.1062
activation[5] = self(-1.0625) * weight[5,5](0.1000) => -0.1062
activation[6] = self(0.6101) * weight[6,6](0.1000) => 0.0610
activation[7] = self(0.6101) * weight[7,7](0.1000) => 0.0610
activation[8] = self(0.6101) * weight[8,8](0.1000) => 0.0610
activation[9] = self(0.6101) * weight[9,9](0.1000) => 0.0610
activation[10] = self(0.6101) * weight[10,10](0.1000) => 0.0610
  b. Update inputs from other neurons
activation[1] = self(0.0011) + network_input[1]=0.0000 => 0.0011
activation[2] = self(0.0011) + network_input[2]=0.0000 => 0.0011
activation[3] = self(-0.1062) + output[1](0.0111) * 1.0000 => -0.0951
activation[3] = self(-0.0951) + output[7](0.6101) * -0.5000 => -0.4002
activation[3] = self(-0.4002) + output[8](0.6101) * -0.5000 => -0.7052
activation[4] = self(-0.1062) + output[1](0.0111) * 0.5000 => -0.1007
activation[4] = self(-0.1007) + output[2](0.0111) * 0.5000 => -0.0951
activation[4] = self(-0.0951) + output[6](0.6101) * -0.5000 => -0.4002
activation[4] = self(-0.4002) + output[8](0.6101) * -0.5000 => -0.7052
activation[5] = self(-0.1062) + output[2](0.0111) * 1.0000 => -0.0951
activation[5] = self(-0.0951) + output[6](0.6101) * -0.5000 => -0.4002
activation[5] = self(-0.4002) + output[7](0.6101) * -0.5000 => -0.7052
activation[6] = self(0.0610) + output[3](0.0000) * 1.0000 => 0.0610
activation[7] = self(0.0610) + output[4](0.0000) * 1.0000 => 0.0610
activation[8] = self(0.0610) + output[5](0.0000) * 1.0000 => 0.0610
activation[9] = self(0.0610) + output[3](0.0000) * 0.7500 => 0.0610
activation[9] = self(0.0610) + output[4](0.0000) * 0.2500 => 0.0610
activation[10] = self(0.0610) + output[4](0.0000) * 0.2500 => 0.0610
activation[10] = self(0.0610) + output[5](0.0000) * 0.7500 => 0.0610
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0011) > threshold(0.0000)? ==> 0.0011
neuronOutput[2]: activation(0.0011) > threshold(0.0000)? ==> 0.0011
neuronOutput[3]: activation(-0.7052) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-0.7052) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-0.7052) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.0610) > threshold(0.0000)? ==> 0.0610
neuronOutput[7]: activation(0.0610) > threshold(0.0000)? ==> 0.0610
neuronOutput[8]: activation(0.0610) > threshold(0.0000)? ==> 0.0610
neuronOutput[9]: activation(0.0610) > threshold(0.0000)? ==> 0.0610
neuronOutput[10]: activation(0.0610) > threshold(0.0000)? ==> 0.0610
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0610)
networkOutput[2] := neuronOutput[10](0.0610)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.0011) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.0011) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.0011) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.0011) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=082 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0011) * weight[1,1](0.1000) => 0.0001
activation[2] = self(0.0011) * weight[2,2](0.1000) => 0.0001
activation[3] = self(-0.7052) * weight[3,3](0.1000) => -0.0705
activation[4] = self(-0.7052) * weight[4,4](0.1000) => -0.0705
activation[5] = self(-0.7052) * weight[5,5](0.1000) => -0.0705
activation[6] = self(0.0610) * weight[6,6](0.1000) => 0.0061
activation[7] = self(0.0610) * weight[7,7](0.1000) => 0.0061
activation[8] = self(0.0610) * weight[8,8](0.1000) => 0.0061
activation[9] = self(0.0610) * weight[9,9](0.1000) => 0.0061
activation[10] = self(0.0610) * weight[10,10](0.1000) => 0.0061
  b. Update inputs from other neurons
activation[1] = self(0.0001) + network_input[1]=1.0000 => 1.0001
activation[2] = self(0.0001) + network_input[2]=1.0000 => 1.0001
activation[3] = self(-0.0705) + output[1](0.0011) * 1.0000 => -0.0694
activation[3] = self(-0.0694) + output[7](0.0610) * -0.5000 => -0.0999
activation[3] = self(-0.0999) + output[8](0.0610) * -0.5000 => -0.1304
activation[4] = self(-0.0705) + output[1](0.0011) * 0.5000 => -0.0700
activation[4] = self(-0.0700) + output[2](0.0011) * 0.5000 => -0.0694
activation[4] = self(-0.0694) + output[6](0.0610) * -0.5000 => -0.0999
activation[4] = self(-0.0999) + output[8](0.0610) * -0.5000 => -0.1304
activation[5] = self(-0.0705) + output[2](0.0011) * 1.0000 => -0.0694
activation[5] = self(-0.0694) + output[6](0.0610) * -0.5000 => -0.0999
activation[5] = self(-0.0999) + output[7](0.0610) * -0.5000 => -0.1304
activation[6] = self(0.0061) + output[3](0.0000) * 1.0000 => 0.0061
activation[7] = self(0.0061) + output[4](0.0000) * 1.0000 => 0.0061
activation[8] = self(0.0061) + output[5](0.0000) * 1.0000 => 0.0061
activation[9] = self(0.0061) + output[3](0.0000) * 0.7500 => 0.0061
activation[9] = self(0.0061) + output[4](0.0000) * 0.2500 => 0.0061
activation[10] = self(0.0061) + output[4](0.0000) * 0.2500 => 0.0061
activation[10] = self(0.0061) + output[5](0.0000) * 0.7500 => 0.0061
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0001) > threshold(0.0000)? ==> 1.0001
neuronOutput[2]: activation(1.0001) > threshold(0.0000)? ==> 1.0001
neuronOutput[3]: activation(-0.1304) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-0.1304) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-0.1304) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.0061) > threshold(0.0000)? ==> 0.0061
neuronOutput[7]: activation(0.0061) > threshold(0.0000)? ==> 0.0061
neuronOutput[8]: activation(0.0061) > threshold(0.0000)? ==> 0.0061
neuronOutput[9]: activation(0.0061) > threshold(0.0000)? ==> 0.0061
neuronOutput[10]: activation(0.0061) > threshold(0.0000)? ==> 0.0061
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0061)
networkOutput[2] := neuronOutput[10](0.0061)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.0001) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.0001) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.0001) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.0001) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=083 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0001) * weight[1,1](0.1000) => 0.1000
activation[2] = self(1.0001) * weight[2,2](0.1000) => 0.1000
activation[3] = self(-0.1304) * weight[3,3](0.1000) => -0.0130
activation[4] = self(-0.1304) * weight[4,4](0.1000) => -0.0130
activation[5] = self(-0.1304) * weight[5,5](0.1000) => -0.0130
activation[6] = self(0.0061) * weight[6,6](0.1000) => 0.0006
activation[7] = self(0.0061) * weight[7,7](0.1000) => 0.0006
activation[8] = self(0.0061) * weight[8,8](0.1000) => 0.0006
activation[9] = self(0.0061) * weight[9,9](0.1000) => 0.0006
activation[10] = self(0.0061) * weight[10,10](0.1000) => 0.0006
  b. Update inputs from other neurons
activation[1] = self(0.1000) + network_input[1]=1.0000 => 1.1000
activation[2] = self(0.1000) + network_input[2]=1.0000 => 1.1000
activation[3] = self(-0.0130) + output[1](1.0001) * 1.0000 => 0.9871
activation[3] = self(0.9871) + output[7](0.0061) * -0.5000 => 0.9840
activation[3] = self(0.9840) + output[8](0.0061) * -0.5000 => 0.9810
activation[4] = self(-0.0130) + output[1](1.0001) * 0.5000 => 0.4870
activation[4] = self(0.4870) + output[2](1.0001) * 0.5000 => 0.9871
activation[4] = self(0.9871) + output[6](0.0061) * -0.5000 => 0.9840
activation[4] = self(0.9840) + output[8](0.0061) * -0.5000 => 0.9810
activation[5] = self(-0.0130) + output[2](1.0001) * 1.0000 => 0.9871
activation[5] = self(0.9871) + output[6](0.0061) * -0.5000 => 0.9840
activation[5] = self(0.9840) + output[7](0.0061) * -0.5000 => 0.9810
activation[6] = self(0.0006) + output[3](0.0000) * 1.0000 => 0.0006
activation[7] = self(0.0006) + output[4](0.0000) * 1.0000 => 0.0006
activation[8] = self(0.0006) + output[5](0.0000) * 1.0000 => 0.0006
activation[9] = self(0.0006) + output[3](0.0000) * 0.7500 => 0.0006
activation[9] = self(0.0006) + output[4](0.0000) * 0.2500 => 0.0006
activation[10] = self(0.0006) + output[4](0.0000) * 0.2500 => 0.0006
activation[10] = self(0.0006) + output[5](0.0000) * 0.7500 => 0.0006
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.1000) > threshold(0.0000)? ==> 1.1000
neuronOutput[2]: activation(1.1000) > threshold(0.0000)? ==> 1.1000
neuronOutput[3]: activation(0.9810) > threshold(0.0000)? ==> 0.9810
neuronOutput[4]: activation(0.9810) > threshold(0.0000)? ==> 0.9810
neuronOutput[5]: activation(0.9810) > threshold(0.0000)? ==> 0.9810
neuronOutput[6]: activation(0.0006) > threshold(0.0000)? ==> 0.0006
neuronOutput[7]: activation(0.0006) > threshold(0.0000)? ==> 0.0006
neuronOutput[8]: activation(0.0006) > threshold(0.0000)? ==> 0.0006
neuronOutput[9]: activation(0.0006) > threshold(0.0000)? ==> 0.0006
neuronOutput[10]: activation(0.0006) > threshold(0.0000)? ==> 0.0006
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0006)
networkOutput[2] := neuronOutput[10](0.0006)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.1000) * output[3](0.9810) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.9810) * output[3](0.9810) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.1000) * output[4](0.9810) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.1000) * output[4](0.9810) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.9810) * output[4](0.9810) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.1000) * output[5](0.9810) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.9810) * output[5](0.9810) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=084 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.1000) * weight[1,1](0.1000) => 0.1100
activation[2] = self(1.1000) * weight[2,2](0.1000) => 0.1100
activation[3] = self(0.9810) * weight[3,3](0.1000) => 0.0981
activation[4] = self(0.9810) * weight[4,4](0.1000) => 0.0981
activation[5] = self(0.9810) * weight[5,5](0.1000) => 0.0981
activation[6] = self(0.0006) * weight[6,6](0.1000) => 0.0001
activation[7] = self(0.0006) * weight[7,7](0.1000) => 0.0001
activation[8] = self(0.0006) * weight[8,8](0.1000) => 0.0001
activation[9] = self(0.0006) * weight[9,9](0.1000) => 0.0001
activation[10] = self(0.0006) * weight[10,10](0.1000) => 0.0001
  b. Update inputs from other neurons
activation[1] = self(0.1100) + network_input[1]=1.0000 => 1.1100
activation[2] = self(0.1100) + network_input[2]=1.0000 => 1.1100
activation[3] = self(0.0981) + output[1](1.1000) * 1.0000 => 1.1981
activation[3] = self(1.1981) + output[7](0.0006) * -0.5000 => 1.1978
activation[3] = self(1.1978) + output[8](0.0006) * -0.5000 => 1.1975
activation[4] = self(0.0981) + output[1](1.1000) * 0.5000 => 0.6481
activation[4] = self(0.6481) + output[2](1.1000) * 0.5000 => 1.1981
activation[4] = self(1.1981) + output[6](0.0006) * -0.5000 => 1.1978
activation[4] = self(1.1978) + output[8](0.0006) * -0.5000 => 1.1975
activation[5] = self(0.0981) + output[2](1.1000) * 1.0000 => 1.1981
activation[5] = self(1.1981) + output[6](0.0006) * -0.5000 => 1.1978
activation[5] = self(1.1978) + output[7](0.0006) * -0.5000 => 1.1975
activation[6] = self(0.0001) + output[3](0.9810) * 1.0000 => 0.9810
activation[7] = self(0.0001) + output[4](0.9810) * 1.0000 => 0.9810
activation[8] = self(0.0001) + output[5](0.9810) * 1.0000 => 0.9810
activation[9] = self(0.0001) + output[3](0.9810) * 0.7500 => 0.7358
activation[9] = self(0.7358) + output[4](0.9810) * 0.2500 => 0.9810
activation[10] = self(0.0001) + output[4](0.9810) * 0.2500 => 0.2453
activation[10] = self(0.2453) + output[5](0.9810) * 0.7500 => 0.9810
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.1100) > threshold(0.0000)? ==> 1.1100
neuronOutput[2]: activation(1.1100) > threshold(0.0000)? ==> 1.1100
neuronOutput[3]: activation(1.1975) > threshold(0.0000)? ==> 1.1975
neuronOutput[4]: activation(1.1975) > threshold(0.0000)? ==> 1.1975
neuronOutput[5]: activation(1.1975) > threshold(0.0000)? ==> 1.1975
neuronOutput[6]: activation(0.9810) > threshold(0.0000)? ==> 0.9810
neuronOutput[7]: activation(0.9810) > threshold(0.0000)? ==> 0.9810
neuronOutput[8]: activation(0.9810) > threshold(0.0000)? ==> 0.9810
neuronOutput[9]: activation(0.9810) > threshold(0.0000)? ==> 0.9810
neuronOutput[10]: activation(0.9810) > threshold(0.0000)? ==> 0.9810
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.9810)
networkOutput[2] := neuronOutput[10](0.9810)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.1100) * output[3](1.1975) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](1.1975) * output[3](1.1975) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.1100) * output[4](1.1975) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.1100) * output[4](1.1975) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](1.1975) * output[4](1.1975) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.1100) * output[5](1.1975) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](1.1975) * output[5](1.1975) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=085 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.1100) * weight[1,1](0.1000) => 0.1110
activation[2] = self(1.1100) * weight[2,2](0.1000) => 0.1110
activation[3] = self(1.1975) * weight[3,3](0.1000) => 0.1197
activation[4] = self(1.1975) * weight[4,4](0.1000) => 0.1197
activation[5] = self(1.1975) * weight[5,5](0.1000) => 0.1197
activation[6] = self(0.9810) * weight[6,6](0.1000) => 0.0981
activation[7] = self(0.9810) * weight[7,7](0.1000) => 0.0981
activation[8] = self(0.9810) * weight[8,8](0.1000) => 0.0981
activation[9] = self(0.9810) * weight[9,9](0.1000) => 0.0981
activation[10] = self(0.9810) * weight[10,10](0.1000) => 0.0981
  b. Update inputs from other neurons
activation[1] = self(0.1110) + network_input[1]=1.0000 => 1.1110
activation[2] = self(0.1110) + network_input[2]=1.0000 => 1.1110
activation[3] = self(0.1197) + output[1](1.1100) * 1.0000 => 1.2298
activation[3] = self(1.2298) + output[7](0.9810) * -0.5000 => 0.7392
activation[3] = self(0.7392) + output[8](0.9810) * -0.5000 => 0.2487
activation[4] = self(0.1197) + output[1](1.1100) * 0.5000 => 0.6748
activation[4] = self(0.6748) + output[2](1.1100) * 0.5000 => 1.2298
activation[4] = self(1.2298) + output[6](0.9810) * -0.5000 => 0.7392
activation[4] = self(0.7392) + output[8](0.9810) * -0.5000 => 0.2487
activation[5] = self(0.1197) + output[2](1.1100) * 1.0000 => 1.2298
activation[5] = self(1.2298) + output[6](0.9810) * -0.5000 => 0.7392
activation[5] = self(0.7392) + output[7](0.9810) * -0.5000 => 0.2487
activation[6] = self(0.0981) + output[3](1.1975) * 1.0000 => 1.2956
activation[7] = self(0.0981) + output[4](1.1975) * 1.0000 => 1.2956
activation[8] = self(0.0981) + output[5](1.1975) * 1.0000 => 1.2956
activation[9] = self(0.0981) + output[3](1.1975) * 0.7500 => 0.9962
activation[9] = self(0.9962) + output[4](1.1975) * 0.2500 => 1.2956
activation[10] = self(0.0981) + output[4](1.1975) * 0.2500 => 0.3975
activation[10] = self(0.3975) + output[5](1.1975) * 0.7500 => 1.2956
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.1110) > threshold(0.0000)? ==> 1.1110
neuronOutput[2]: activation(1.1110) > threshold(0.0000)? ==> 1.1110
neuronOutput[3]: activation(0.2487) > threshold(0.0000)? ==> 0.2487
neuronOutput[4]: activation(0.2487) > threshold(0.0000)? ==> 0.2487
neuronOutput[5]: activation(0.2487) > threshold(0.0000)? ==> 0.2487
neuronOutput[6]: activation(1.2956) > threshold(0.0000)? ==> 1.2956
neuronOutput[7]: activation(1.2956) > threshold(0.0000)? ==> 1.2956
neuronOutput[8]: activation(1.2956) > threshold(0.0000)? ==> 1.2956
neuronOutput[9]: activation(1.2956) > threshold(0.0000)? ==> 1.2956
neuronOutput[10]: activation(1.2956) > threshold(0.0000)? ==> 1.2956
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](1.2956)
networkOutput[2] := neuronOutput[10](1.2956)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.1110) * output[3](0.2487) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.2487) * output[3](0.2487) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.1110) * output[4](0.2487) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.1110) * output[4](0.2487) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.2487) * output[4](0.2487) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.1110) * output[5](0.2487) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.2487) * output[5](0.2487) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=086 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.1110) * weight[1,1](0.1000) => 0.1111
activation[2] = self(1.1110) * weight[2,2](0.1000) => 0.1111
activation[3] = self(0.2487) * weight[3,3](0.1000) => 0.0249
activation[4] = self(0.2487) * weight[4,4](0.1000) => 0.0249
activation[5] = self(0.2487) * weight[5,5](0.1000) => 0.0249
activation[6] = self(1.2956) * weight[6,6](0.1000) => 0.1296
activation[7] = self(1.2956) * weight[7,7](0.1000) => 0.1296
activation[8] = self(1.2956) * weight[8,8](0.1000) => 0.1296
activation[9] = self(1.2956) * weight[9,9](0.1000) => 0.1296
activation[10] = self(1.2956) * weight[10,10](0.1000) => 0.1296
  b. Update inputs from other neurons
activation[1] = self(0.1111) + network_input[1]=1.0000 => 1.1111
activation[2] = self(0.1111) + network_input[2]=1.0000 => 1.1111
activation[3] = self(0.0249) + output[1](1.1110) * 1.0000 => 1.1359
activation[3] = self(1.1359) + output[7](1.2956) * -0.5000 => 0.4881
activation[3] = self(0.4881) + output[8](1.2956) * -0.5000 => -0.1597
activation[4] = self(0.0249) + output[1](1.1110) * 0.5000 => 0.5804
activation[4] = self(0.5804) + output[2](1.1110) * 0.5000 => 1.1359
activation[4] = self(1.1359) + output[6](1.2956) * -0.5000 => 0.4881
activation[4] = self(0.4881) + output[8](1.2956) * -0.5000 => -0.1597
activation[5] = self(0.0249) + output[2](1.1110) * 1.0000 => 1.1359
activation[5] = self(1.1359) + output[6](1.2956) * -0.5000 => 0.4881
activation[5] = self(0.4881) + output[7](1.2956) * -0.5000 => -0.1597
activation[6] = self(0.1296) + output[3](0.2487) * 1.0000 => 0.3783
activation[7] = self(0.1296) + output[4](0.2487) * 1.0000 => 0.3783
activation[8] = self(0.1296) + output[5](0.2487) * 1.0000 => 0.3783
activation[9] = self(0.1296) + output[3](0.2487) * 0.7500 => 0.3161
activation[9] = self(0.3161) + output[4](0.2487) * 0.2500 => 0.3783
activation[10] = self(0.1296) + output[4](0.2487) * 0.2500 => 0.1917
activation[10] = self(0.1917) + output[5](0.2487) * 0.7500 => 0.3783
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.1111) > threshold(0.0000)? ==> 1.1111
neuronOutput[2]: activation(1.1111) > threshold(0.0000)? ==> 1.1111
neuronOutput[3]: activation(-0.1597) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-0.1597) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-0.1597) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.3783) > threshold(0.0000)? ==> 0.3783
neuronOutput[7]: activation(0.3783) > threshold(0.0000)? ==> 0.3783
neuronOutput[8]: activation(0.3783) > threshold(0.0000)? ==> 0.3783
neuronOutput[9]: activation(0.3783) > threshold(0.0000)? ==> 0.3783
neuronOutput[10]: activation(0.3783) > threshold(0.0000)? ==> 0.3783
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.3783)
networkOutput[2] := neuronOutput[10](0.3783)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.1111) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.1111) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.1111) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.1111) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=087 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.1111) * weight[1,1](0.1000) => 0.1111
activation[2] = self(1.1111) * weight[2,2](0.1000) => 0.1111
activation[3] = self(-0.1597) * weight[3,3](0.1000) => -0.0160
activation[4] = self(-0.1597) * weight[4,4](0.1000) => -0.0160
activation[5] = self(-0.1597) * weight[5,5](0.1000) => -0.0160
activation[6] = self(0.3783) * weight[6,6](0.1000) => 0.0378
activation[7] = self(0.3783) * weight[7,7](0.1000) => 0.0378
activation[8] = self(0.3783) * weight[8,8](0.1000) => 0.0378
activation[9] = self(0.3783) * weight[9,9](0.1000) => 0.0378
activation[10] = self(0.3783) * weight[10,10](0.1000) => 0.0378
  b. Update inputs from other neurons
activation[1] = self(0.1111) + network_input[1]=1.0000 => 1.1111
activation[2] = self(0.1111) + network_input[2]=1.0000 => 1.1111
activation[3] = self(-0.0160) + output[1](1.1111) * 1.0000 => 1.0951
activation[3] = self(1.0951) + output[7](0.3783) * -0.5000 => 0.9060
activation[3] = self(0.9060) + output[8](0.3783) * -0.5000 => 0.7168
activation[4] = self(-0.0160) + output[1](1.1111) * 0.5000 => 0.5396
activation[4] = self(0.5396) + output[2](1.1111) * 0.5000 => 1.0951
activation[4] = self(1.0951) + output[6](0.3783) * -0.5000 => 0.9060
activation[4] = self(0.9060) + output[8](0.3783) * -0.5000 => 0.7168
activation[5] = self(-0.0160) + output[2](1.1111) * 1.0000 => 1.0951
activation[5] = self(1.0951) + output[6](0.3783) * -0.5000 => 0.9060
activation[5] = self(0.9060) + output[7](0.3783) * -0.5000 => 0.7168
activation[6] = self(0.0378) + output[3](0.0000) * 1.0000 => 0.0378
activation[7] = self(0.0378) + output[4](0.0000) * 1.0000 => 0.0378
activation[8] = self(0.0378) + output[5](0.0000) * 1.0000 => 0.0378
activation[9] = self(0.0378) + output[3](0.0000) * 0.7500 => 0.0378
activation[9] = self(0.0378) + output[4](0.0000) * 0.2500 => 0.0378
activation[10] = self(0.0378) + output[4](0.0000) * 0.2500 => 0.0378
activation[10] = self(0.0378) + output[5](0.0000) * 0.7500 => 0.0378
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.1111) > threshold(0.0000)? ==> 1.1111
neuronOutput[2]: activation(1.1111) > threshold(0.0000)? ==> 1.1111
neuronOutput[3]: activation(0.7168) > threshold(0.0000)? ==> 0.7168
neuronOutput[4]: activation(0.7168) > threshold(0.0000)? ==> 0.7168
neuronOutput[5]: activation(0.7168) > threshold(0.0000)? ==> 0.7168
neuronOutput[6]: activation(0.0378) > threshold(0.0000)? ==> 0.0378
neuronOutput[7]: activation(0.0378) > threshold(0.0000)? ==> 0.0378
neuronOutput[8]: activation(0.0378) > threshold(0.0000)? ==> 0.0378
neuronOutput[9]: activation(0.0378) > threshold(0.0000)? ==> 0.0378
neuronOutput[10]: activation(0.0378) > threshold(0.0000)? ==> 0.0378
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0378)
networkOutput[2] := neuronOutput[10](0.0378)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.1111) * output[3](0.7168) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.7168) * output[3](0.7168) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.1111) * output[4](0.7168) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.1111) * output[4](0.7168) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.7168) * output[4](0.7168) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.1111) * output[5](0.7168) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.7168) * output[5](0.7168) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=088 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.1111) * weight[1,1](0.1000) => 0.1111
activation[2] = self(1.1111) * weight[2,2](0.1000) => 0.1111
activation[3] = self(0.7168) * weight[3,3](0.1000) => 0.0717
activation[4] = self(0.7168) * weight[4,4](0.1000) => 0.0717
activation[5] = self(0.7168) * weight[5,5](0.1000) => 0.0717
activation[6] = self(0.0378) * weight[6,6](0.1000) => 0.0038
activation[7] = self(0.0378) * weight[7,7](0.1000) => 0.0038
activation[8] = self(0.0378) * weight[8,8](0.1000) => 0.0038
activation[9] = self(0.0378) * weight[9,9](0.1000) => 0.0038
activation[10] = self(0.0378) * weight[10,10](0.1000) => 0.0038
  b. Update inputs from other neurons
activation[1] = self(0.1111) + network_input[1]=0.0000 => 0.1111
activation[2] = self(0.1111) + network_input[2]=0.0000 => 0.1111
activation[3] = self(0.0717) + output[1](1.1111) * 1.0000 => 1.1828
activation[3] = self(1.1828) + output[7](0.0378) * -0.5000 => 1.1639
activation[3] = self(1.1639) + output[8](0.0378) * -0.5000 => 1.1450
activation[4] = self(0.0717) + output[1](1.1111) * 0.5000 => 0.6272
activation[4] = self(0.6272) + output[2](1.1111) * 0.5000 => 1.1828
activation[4] = self(1.1828) + output[6](0.0378) * -0.5000 => 1.1639
activation[4] = self(1.1639) + output[8](0.0378) * -0.5000 => 1.1450
activation[5] = self(0.0717) + output[2](1.1111) * 1.0000 => 1.1828
activation[5] = self(1.1828) + output[6](0.0378) * -0.5000 => 1.1639
activation[5] = self(1.1639) + output[7](0.0378) * -0.5000 => 1.1450
activation[6] = self(0.0038) + output[3](0.7168) * 1.0000 => 0.7206
activation[7] = self(0.0038) + output[4](0.7168) * 1.0000 => 0.7206
activation[8] = self(0.0038) + output[5](0.7168) * 1.0000 => 0.7206
activation[9] = self(0.0038) + output[3](0.7168) * 0.7500 => 0.5414
activation[9] = self(0.5414) + output[4](0.7168) * 0.2500 => 0.7206
activation[10] = self(0.0038) + output[4](0.7168) * 0.2500 => 0.1830
activation[10] = self(0.1830) + output[5](0.7168) * 0.7500 => 0.7206
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.1111) > threshold(0.0000)? ==> 0.1111
neuronOutput[2]: activation(0.1111) > threshold(0.0000)? ==> 0.1111
neuronOutput[3]: activation(1.1450) > threshold(0.0000)? ==> 1.1450
neuronOutput[4]: activation(1.1450) > threshold(0.0000)? ==> 1.1450
neuronOutput[5]: activation(1.1450) > threshold(0.0000)? ==> 1.1450
neuronOutput[6]: activation(0.7206) > threshold(0.0000)? ==> 0.7206
neuronOutput[7]: activation(0.7206) > threshold(0.0000)? ==> 0.7206
neuronOutput[8]: activation(0.7206) > threshold(0.0000)? ==> 0.7206
neuronOutput[9]: activation(0.7206) > threshold(0.0000)? ==> 0.7206
neuronOutput[10]: activation(0.7206) > threshold(0.0000)? ==> 0.7206
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.7206)
networkOutput[2] := neuronOutput[10](0.7206)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.1111) * output[3](1.1450) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](1.1450) * output[3](1.1450) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.1111) * output[4](1.1450) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.1111) * output[4](1.1450) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](1.1450) * output[4](1.1450) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.1111) * output[5](1.1450) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](1.1450) * output[5](1.1450) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=089 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.1111) * weight[1,1](0.1000) => 0.0111
activation[2] = self(0.1111) * weight[2,2](0.1000) => 0.0111
activation[3] = self(1.1450) * weight[3,3](0.1000) => 0.1145
activation[4] = self(1.1450) * weight[4,4](0.1000) => 0.1145
activation[5] = self(1.1450) * weight[5,5](0.1000) => 0.1145
activation[6] = self(0.7206) * weight[6,6](0.1000) => 0.0721
activation[7] = self(0.7206) * weight[7,7](0.1000) => 0.0721
activation[8] = self(0.7206) * weight[8,8](0.1000) => 0.0721
activation[9] = self(0.7206) * weight[9,9](0.1000) => 0.0721
activation[10] = self(0.7206) * weight[10,10](0.1000) => 0.0721
  b. Update inputs from other neurons
activation[1] = self(0.0111) + network_input[1]=1.0000 => 1.0111
activation[2] = self(0.0111) + network_input[2]=1.0000 => 1.0111
activation[3] = self(0.1145) + output[1](0.1111) * 1.0000 => 0.2256
activation[3] = self(0.2256) + output[7](0.7206) * -0.5000 => -0.1347
activation[3] = self(-0.1347) + output[8](0.7206) * -0.5000 => -0.4950
activation[4] = self(0.1145) + output[1](0.1111) * 0.5000 => 0.1701
activation[4] = self(0.1701) + output[2](0.1111) * 0.5000 => 0.2256
activation[4] = self(0.2256) + output[6](0.7206) * -0.5000 => -0.1347
activation[4] = self(-0.1347) + output[8](0.7206) * -0.5000 => -0.4950
activation[5] = self(0.1145) + output[2](0.1111) * 1.0000 => 0.2256
activation[5] = self(0.2256) + output[6](0.7206) * -0.5000 => -0.1347
activation[5] = self(-0.1347) + output[7](0.7206) * -0.5000 => -0.4950
activation[6] = self(0.0721) + output[3](1.1450) * 1.0000 => 1.2170
activation[7] = self(0.0721) + output[4](1.1450) * 1.0000 => 1.2170
activation[8] = self(0.0721) + output[5](1.1450) * 1.0000 => 1.2170
activation[9] = self(0.0721) + output[3](1.1450) * 0.7500 => 0.9308
activation[9] = self(0.9308) + output[4](1.1450) * 0.2500 => 1.2170
activation[10] = self(0.0721) + output[4](1.1450) * 0.2500 => 0.3583
activation[10] = self(0.3583) + output[5](1.1450) * 0.7500 => 1.2170
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0111) > threshold(0.0000)? ==> 1.0111
neuronOutput[2]: activation(1.0111) > threshold(0.0000)? ==> 1.0111
neuronOutput[3]: activation(-0.4950) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-0.4950) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-0.4950) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(1.2170) > threshold(0.0000)? ==> 1.2170
neuronOutput[7]: activation(1.2170) > threshold(0.0000)? ==> 1.2170
neuronOutput[8]: activation(1.2170) > threshold(0.0000)? ==> 1.2170
neuronOutput[9]: activation(1.2170) > threshold(0.0000)? ==> 1.2170
neuronOutput[10]: activation(1.2170) > threshold(0.0000)? ==> 1.2170
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](1.2170)
networkOutput[2] := neuronOutput[10](1.2170)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.0111) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.0111) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.0111) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.0111) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=090 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0111) * weight[1,1](0.1000) => 0.1011
activation[2] = self(1.0111) * weight[2,2](0.1000) => 0.1011
activation[3] = self(-0.4950) * weight[3,3](0.1000) => -0.0495
activation[4] = self(-0.4950) * weight[4,4](0.1000) => -0.0495
activation[5] = self(-0.4950) * weight[5,5](0.1000) => -0.0495
activation[6] = self(1.2170) * weight[6,6](0.1000) => 0.1217
activation[7] = self(1.2170) * weight[7,7](0.1000) => 0.1217
activation[8] = self(1.2170) * weight[8,8](0.1000) => 0.1217
activation[9] = self(1.2170) * weight[9,9](0.1000) => 0.1217
activation[10] = self(1.2170) * weight[10,10](0.1000) => 0.1217
  b. Update inputs from other neurons
activation[1] = self(0.1011) + network_input[1]=0.0000 => 0.1011
activation[2] = self(0.1011) + network_input[2]=0.0000 => 0.1011
activation[3] = self(-0.0495) + output[1](1.0111) * 1.0000 => 0.9616
activation[3] = self(0.9616) + output[7](1.2170) * -0.5000 => 0.3531
activation[3] = self(0.3531) + output[8](1.2170) * -0.5000 => -0.2554
activation[4] = self(-0.0495) + output[1](1.0111) * 0.5000 => 0.4561
activation[4] = self(0.4561) + output[2](1.0111) * 0.5000 => 0.9616
activation[4] = self(0.9616) + output[6](1.2170) * -0.5000 => 0.3531
activation[4] = self(0.3531) + output[8](1.2170) * -0.5000 => -0.2554
activation[5] = self(-0.0495) + output[2](1.0111) * 1.0000 => 0.9616
activation[5] = self(0.9616) + output[6](1.2170) * -0.5000 => 0.3531
activation[5] = self(0.3531) + output[7](1.2170) * -0.5000 => -0.2554
activation[6] = self(0.1217) + output[3](0.0000) * 1.0000 => 0.1217
activation[7] = self(0.1217) + output[4](0.0000) * 1.0000 => 0.1217
activation[8] = self(0.1217) + output[5](0.0000) * 1.0000 => 0.1217
activation[9] = self(0.1217) + output[3](0.0000) * 0.7500 => 0.1217
activation[9] = self(0.1217) + output[4](0.0000) * 0.2500 => 0.1217
activation[10] = self(0.1217) + output[4](0.0000) * 0.2500 => 0.1217
activation[10] = self(0.1217) + output[5](0.0000) * 0.7500 => 0.1217
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.1011) > threshold(0.0000)? ==> 0.1011
neuronOutput[2]: activation(0.1011) > threshold(0.0000)? ==> 0.1011
neuronOutput[3]: activation(-0.2554) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-0.2554) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-0.2554) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.1217) > threshold(0.0000)? ==> 0.1217
neuronOutput[7]: activation(0.1217) > threshold(0.0000)? ==> 0.1217
neuronOutput[8]: activation(0.1217) > threshold(0.0000)? ==> 0.1217
neuronOutput[9]: activation(0.1217) > threshold(0.0000)? ==> 0.1217
neuronOutput[10]: activation(0.1217) > threshold(0.0000)? ==> 0.1217
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.1217)
networkOutput[2] := neuronOutput[10](0.1217)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.1011) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.1011) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.1011) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.1011) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=091 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.1011) * weight[1,1](0.1000) => 0.0101
activation[2] = self(0.1011) * weight[2,2](0.1000) => 0.0101
activation[3] = self(-0.2554) * weight[3,3](0.1000) => -0.0255
activation[4] = self(-0.2554) * weight[4,4](0.1000) => -0.0255
activation[5] = self(-0.2554) * weight[5,5](0.1000) => -0.0255
activation[6] = self(0.1217) * weight[6,6](0.1000) => 0.0122
activation[7] = self(0.1217) * weight[7,7](0.1000) => 0.0122
activation[8] = self(0.1217) * weight[8,8](0.1000) => 0.0122
activation[9] = self(0.1217) * weight[9,9](0.1000) => 0.0122
activation[10] = self(0.1217) * weight[10,10](0.1000) => 0.0122
  b. Update inputs from other neurons
activation[1] = self(0.0101) + network_input[1]=0.0000 => 0.0101
activation[2] = self(0.0101) + network_input[2]=0.0000 => 0.0101
activation[3] = self(-0.0255) + output[1](0.1011) * 1.0000 => 0.0756
activation[3] = self(0.0756) + output[7](0.1217) * -0.5000 => 0.0147
activation[3] = self(0.0147) + output[8](0.1217) * -0.5000 => -0.0461
activation[4] = self(-0.0255) + output[1](0.1011) * 0.5000 => 0.0250
activation[4] = self(0.0250) + output[2](0.1011) * 0.5000 => 0.0756
activation[4] = self(0.0756) + output[6](0.1217) * -0.5000 => 0.0147
activation[4] = self(0.0147) + output[8](0.1217) * -0.5000 => -0.0461
activation[5] = self(-0.0255) + output[2](0.1011) * 1.0000 => 0.0756
activation[5] = self(0.0756) + output[6](0.1217) * -0.5000 => 0.0147
activation[5] = self(0.0147) + output[7](0.1217) * -0.5000 => -0.0461
activation[6] = self(0.0122) + output[3](0.0000) * 1.0000 => 0.0122
activation[7] = self(0.0122) + output[4](0.0000) * 1.0000 => 0.0122
activation[8] = self(0.0122) + output[5](0.0000) * 1.0000 => 0.0122
activation[9] = self(0.0122) + output[3](0.0000) * 0.7500 => 0.0122
activation[9] = self(0.0122) + output[4](0.0000) * 0.2500 => 0.0122
activation[10] = self(0.0122) + output[4](0.0000) * 0.2500 => 0.0122
activation[10] = self(0.0122) + output[5](0.0000) * 0.7500 => 0.0122
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0101) > threshold(0.0000)? ==> 0.0101
neuronOutput[2]: activation(0.0101) > threshold(0.0000)? ==> 0.0101
neuronOutput[3]: activation(-0.0461) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-0.0461) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-0.0461) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.0122) > threshold(0.0000)? ==> 0.0122
neuronOutput[7]: activation(0.0122) > threshold(0.0000)? ==> 0.0122
neuronOutput[8]: activation(0.0122) > threshold(0.0000)? ==> 0.0122
neuronOutput[9]: activation(0.0122) > threshold(0.0000)? ==> 0.0122
neuronOutput[10]: activation(0.0122) > threshold(0.0000)? ==> 0.0122
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0122)
networkOutput[2] := neuronOutput[10](0.0122)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.0101) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.0101) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.0101) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.0101) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=092 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0101) * weight[1,1](0.1000) => 0.0010
activation[2] = self(0.0101) * weight[2,2](0.1000) => 0.0010
activation[3] = self(-0.0461) * weight[3,3](0.1000) => -0.0046
activation[4] = self(-0.0461) * weight[4,4](0.1000) => -0.0046
activation[5] = self(-0.0461) * weight[5,5](0.1000) => -0.0046
activation[6] = self(0.0122) * weight[6,6](0.1000) => 0.0012
activation[7] = self(0.0122) * weight[7,7](0.1000) => 0.0012
activation[8] = self(0.0122) * weight[8,8](0.1000) => 0.0012
activation[9] = self(0.0122) * weight[9,9](0.1000) => 0.0012
activation[10] = self(0.0122) * weight[10,10](0.1000) => 0.0012
  b. Update inputs from other neurons
activation[1] = self(0.0010) + network_input[1]=1.0000 => 1.0010
activation[2] = self(0.0010) + network_input[2]=1.0000 => 1.0010
activation[3] = self(-0.0046) + output[1](0.0101) * 1.0000 => 0.0055
activation[3] = self(0.0055) + output[7](0.0122) * -0.5000 => -0.0006
activation[3] = self(-0.0006) + output[8](0.0122) * -0.5000 => -0.0067
activation[4] = self(-0.0046) + output[1](0.0101) * 0.5000 => 0.0004
activation[4] = self(0.0004) + output[2](0.0101) * 0.5000 => 0.0055
activation[4] = self(0.0055) + output[6](0.0122) * -0.5000 => -0.0006
activation[4] = self(-0.0006) + output[8](0.0122) * -0.5000 => -0.0067
activation[5] = self(-0.0046) + output[2](0.0101) * 1.0000 => 0.0055
activation[5] = self(0.0055) + output[6](0.0122) * -0.5000 => -0.0006
activation[5] = self(-0.0006) + output[7](0.0122) * -0.5000 => -0.0067
activation[6] = self(0.0012) + output[3](0.0000) * 1.0000 => 0.0012
activation[7] = self(0.0012) + output[4](0.0000) * 1.0000 => 0.0012
activation[8] = self(0.0012) + output[5](0.0000) * 1.0000 => 0.0012
activation[9] = self(0.0012) + output[3](0.0000) * 0.7500 => 0.0012
activation[9] = self(0.0012) + output[4](0.0000) * 0.2500 => 0.0012
activation[10] = self(0.0012) + output[4](0.0000) * 0.2500 => 0.0012
activation[10] = self(0.0012) + output[5](0.0000) * 0.7500 => 0.0012
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0010) > threshold(0.0000)? ==> 1.0010
neuronOutput[2]: activation(1.0010) > threshold(0.0000)? ==> 1.0010
neuronOutput[3]: activation(-0.0067) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-0.0067) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-0.0067) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.0012) > threshold(0.0000)? ==> 0.0012
neuronOutput[7]: activation(0.0012) > threshold(0.0000)? ==> 0.0012
neuronOutput[8]: activation(0.0012) > threshold(0.0000)? ==> 0.0012
neuronOutput[9]: activation(0.0012) > threshold(0.0000)? ==> 0.0012
neuronOutput[10]: activation(0.0012) > threshold(0.0000)? ==> 0.0012
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0012)
networkOutput[2] := neuronOutput[10](0.0012)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.0010) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.0010) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.0010) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.0010) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=093 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0010) * weight[1,1](0.1000) => 0.1001
activation[2] = self(1.0010) * weight[2,2](0.1000) => 0.1001
activation[3] = self(-0.0067) * weight[3,3](0.1000) => -0.0007
activation[4] = self(-0.0067) * weight[4,4](0.1000) => -0.0007
activation[5] = self(-0.0067) * weight[5,5](0.1000) => -0.0007
activation[6] = self(0.0012) * weight[6,6](0.1000) => 0.0001
activation[7] = self(0.0012) * weight[7,7](0.1000) => 0.0001
activation[8] = self(0.0012) * weight[8,8](0.1000) => 0.0001
activation[9] = self(0.0012) * weight[9,9](0.1000) => 0.0001
activation[10] = self(0.0012) * weight[10,10](0.1000) => 0.0001
  b. Update inputs from other neurons
activation[1] = self(0.1001) + network_input[1]=1.0000 => 1.1001
activation[2] = self(0.1001) + network_input[2]=1.0000 => 1.1001
activation[3] = self(-0.0007) + output[1](1.0010) * 1.0000 => 1.0003
activation[3] = self(1.0003) + output[7](0.0012) * -0.5000 => 0.9997
activation[3] = self(0.9997) + output[8](0.0012) * -0.5000 => 0.9991
activation[4] = self(-0.0007) + output[1](1.0010) * 0.5000 => 0.4998
activation[4] = self(0.4998) + output[2](1.0010) * 0.5000 => 1.0003
activation[4] = self(1.0003) + output[6](0.0012) * -0.5000 => 0.9997
activation[4] = self(0.9997) + output[8](0.0012) * -0.5000 => 0.9991
activation[5] = self(-0.0007) + output[2](1.0010) * 1.0000 => 1.0003
activation[5] = self(1.0003) + output[6](0.0012) * -0.5000 => 0.9997
activation[5] = self(0.9997) + output[7](0.0012) * -0.5000 => 0.9991
activation[6] = self(0.0001) + output[3](0.0000) * 1.0000 => 0.0001
activation[7] = self(0.0001) + output[4](0.0000) * 1.0000 => 0.0001
activation[8] = self(0.0001) + output[5](0.0000) * 1.0000 => 0.0001
activation[9] = self(0.0001) + output[3](0.0000) * 0.7500 => 0.0001
activation[9] = self(0.0001) + output[4](0.0000) * 0.2500 => 0.0001
activation[10] = self(0.0001) + output[4](0.0000) * 0.2500 => 0.0001
activation[10] = self(0.0001) + output[5](0.0000) * 0.7500 => 0.0001
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.1001) > threshold(0.0000)? ==> 1.1001
neuronOutput[2]: activation(1.1001) > threshold(0.0000)? ==> 1.1001
neuronOutput[3]: activation(0.9991) > threshold(0.0000)? ==> 0.9991
neuronOutput[4]: activation(0.9991) > threshold(0.0000)? ==> 0.9991
neuronOutput[5]: activation(0.9991) > threshold(0.0000)? ==> 0.9991
neuronOutput[6]: activation(0.0001) > threshold(0.0000)? ==> 0.0001
neuronOutput[7]: activation(0.0001) > threshold(0.0000)? ==> 0.0001
neuronOutput[8]: activation(0.0001) > threshold(0.0000)? ==> 0.0001
neuronOutput[9]: activation(0.0001) > threshold(0.0000)? ==> 0.0001
neuronOutput[10]: activation(0.0001) > threshold(0.0000)? ==> 0.0001
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0001)
networkOutput[2] := neuronOutput[10](0.0001)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.1001) * output[3](0.9991) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.9991) * output[3](0.9991) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.1001) * output[4](0.9991) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.1001) * output[4](0.9991) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.9991) * output[4](0.9991) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.1001) * output[5](0.9991) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.9991) * output[5](0.9991) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=094 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.1001) * weight[1,1](0.1000) => 0.1100
activation[2] = self(1.1001) * weight[2,2](0.1000) => 0.1100
activation[3] = self(0.9991) * weight[3,3](0.1000) => 0.0999
activation[4] = self(0.9991) * weight[4,4](0.1000) => 0.0999
activation[5] = self(0.9991) * weight[5,5](0.1000) => 0.0999
activation[6] = self(0.0001) * weight[6,6](0.1000) => 0.0000
activation[7] = self(0.0001) * weight[7,7](0.1000) => 0.0000
activation[8] = self(0.0001) * weight[8,8](0.1000) => 0.0000
activation[9] = self(0.0001) * weight[9,9](0.1000) => 0.0000
activation[10] = self(0.0001) * weight[10,10](0.1000) => 0.0000
  b. Update inputs from other neurons
activation[1] = self(0.1100) + network_input[1]=1.0000 => 1.1100
activation[2] = self(0.1100) + network_input[2]=1.0000 => 1.1100
activation[3] = self(0.0999) + output[1](1.1001) * 1.0000 => 1.2000
activation[3] = self(1.2000) + output[7](0.0001) * -0.5000 => 1.2000
activation[3] = self(1.2000) + output[8](0.0001) * -0.5000 => 1.1999
activation[4] = self(0.0999) + output[1](1.1001) * 0.5000 => 0.6500
activation[4] = self(0.6500) + output[2](1.1001) * 0.5000 => 1.2000
activation[4] = self(1.2000) + output[6](0.0001) * -0.5000 => 1.2000
activation[4] = self(1.2000) + output[8](0.0001) * -0.5000 => 1.1999
activation[5] = self(0.0999) + output[2](1.1001) * 1.0000 => 1.2000
activation[5] = self(1.2000) + output[6](0.0001) * -0.5000 => 1.2000
activation[5] = self(1.2000) + output[7](0.0001) * -0.5000 => 1.1999
activation[6] = self(0.0000) + output[3](0.9991) * 1.0000 => 0.9991
activation[7] = self(0.0000) + output[4](0.9991) * 1.0000 => 0.9991
activation[8] = self(0.0000) + output[5](0.9991) * 1.0000 => 0.9991
activation[9] = self(0.0000) + output[3](0.9991) * 0.7500 => 0.7494
activation[9] = self(0.7494) + output[4](0.9991) * 0.2500 => 0.9991
activation[10] = self(0.0000) + output[4](0.9991) * 0.2500 => 0.2498
activation[10] = self(0.2498) + output[5](0.9991) * 0.7500 => 0.9991
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.1100) > threshold(0.0000)? ==> 1.1100
neuronOutput[2]: activation(1.1100) > threshold(0.0000)? ==> 1.1100
neuronOutput[3]: activation(1.1999) > threshold(0.0000)? ==> 1.1999
neuronOutput[4]: activation(1.1999) > threshold(0.0000)? ==> 1.1999
neuronOutput[5]: activation(1.1999) > threshold(0.0000)? ==> 1.1999
neuronOutput[6]: activation(0.9991) > threshold(0.0000)? ==> 0.9991
neuronOutput[7]: activation(0.9991) > threshold(0.0000)? ==> 0.9991
neuronOutput[8]: activation(0.9991) > threshold(0.0000)? ==> 0.9991
neuronOutput[9]: activation(0.9991) > threshold(0.0000)? ==> 0.9991
neuronOutput[10]: activation(0.9991) > threshold(0.0000)? ==> 0.9991
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.9991)
networkOutput[2] := neuronOutput[10](0.9991)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.1100) * output[3](1.1999) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](1.1999) * output[3](1.1999) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.1100) * output[4](1.1999) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.1100) * output[4](1.1999) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](1.1999) * output[4](1.1999) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.1100) * output[5](1.1999) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](1.1999) * output[5](1.1999) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=095 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.1100) * weight[1,1](0.1000) => 0.1110
activation[2] = self(1.1100) * weight[2,2](0.1000) => 0.1110
activation[3] = self(1.1999) * weight[3,3](0.1000) => 0.1200
activation[4] = self(1.1999) * weight[4,4](0.1000) => 0.1200
activation[5] = self(1.1999) * weight[5,5](0.1000) => 0.1200
activation[6] = self(0.9991) * weight[6,6](0.1000) => 0.0999
activation[7] = self(0.9991) * weight[7,7](0.1000) => 0.0999
activation[8] = self(0.9991) * weight[8,8](0.1000) => 0.0999
activation[9] = self(0.9991) * weight[9,9](0.1000) => 0.0999
activation[10] = self(0.9991) * weight[10,10](0.1000) => 0.0999
  b. Update inputs from other neurons
activation[1] = self(0.1110) + network_input[1]=0.0000 => 0.1110
activation[2] = self(0.1110) + network_input[2]=0.0000 => 0.1110
activation[3] = self(0.1200) + output[1](1.1100) * 1.0000 => 1.2300
activation[3] = self(1.2300) + output[7](0.9991) * -0.5000 => 0.7304
activation[3] = self(0.7304) + output[8](0.9991) * -0.5000 => 0.2309
activation[4] = self(0.1200) + output[1](1.1100) * 0.5000 => 0.6750
activation[4] = self(0.6750) + output[2](1.1100) * 0.5000 => 1.2300
activation[4] = self(1.2300) + output[6](0.9991) * -0.5000 => 0.7304
activation[4] = self(0.7304) + output[8](0.9991) * -0.5000 => 0.2309
activation[5] = self(0.1200) + output[2](1.1100) * 1.0000 => 1.2300
activation[5] = self(1.2300) + output[6](0.9991) * -0.5000 => 0.7304
activation[5] = self(0.7304) + output[7](0.9991) * -0.5000 => 0.2309
activation[6] = self(0.0999) + output[3](1.1999) * 1.0000 => 1.2998
activation[7] = self(0.0999) + output[4](1.1999) * 1.0000 => 1.2998
activation[8] = self(0.0999) + output[5](1.1999) * 1.0000 => 1.2998
activation[9] = self(0.0999) + output[3](1.1999) * 0.7500 => 0.9998
activation[9] = self(0.9998) + output[4](1.1999) * 0.2500 => 1.2998
activation[10] = self(0.0999) + output[4](1.1999) * 0.2500 => 0.3999
activation[10] = self(0.3999) + output[5](1.1999) * 0.7500 => 1.2998
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.1110) > threshold(0.0000)? ==> 0.1110
neuronOutput[2]: activation(0.1110) > threshold(0.0000)? ==> 0.1110
neuronOutput[3]: activation(0.2309) > threshold(0.0000)? ==> 0.2309
neuronOutput[4]: activation(0.2309) > threshold(0.0000)? ==> 0.2309
neuronOutput[5]: activation(0.2309) > threshold(0.0000)? ==> 0.2309
neuronOutput[6]: activation(1.2998) > threshold(0.0000)? ==> 1.2998
neuronOutput[7]: activation(1.2998) > threshold(0.0000)? ==> 1.2998
neuronOutput[8]: activation(1.2998) > threshold(0.0000)? ==> 1.2998
neuronOutput[9]: activation(1.2998) > threshold(0.0000)? ==> 1.2998
neuronOutput[10]: activation(1.2998) > threshold(0.0000)? ==> 1.2998
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](1.2998)
networkOutput[2] := neuronOutput[10](1.2998)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.1110) * output[3](0.2309) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.2309) * output[3](0.2309) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.1110) * output[4](0.2309) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.1110) * output[4](0.2309) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.2309) * output[4](0.2309) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.1110) * output[5](0.2309) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.2309) * output[5](0.2309) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=096 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.1110) * weight[1,1](0.1000) => 0.0111
activation[2] = self(0.1110) * weight[2,2](0.1000) => 0.0111
activation[3] = self(0.2309) * weight[3,3](0.1000) => 0.0231
activation[4] = self(0.2309) * weight[4,4](0.1000) => 0.0231
activation[5] = self(0.2309) * weight[5,5](0.1000) => 0.0231
activation[6] = self(1.2998) * weight[6,6](0.1000) => 0.1300
activation[7] = self(1.2998) * weight[7,7](0.1000) => 0.1300
activation[8] = self(1.2998) * weight[8,8](0.1000) => 0.1300
activation[9] = self(1.2998) * weight[9,9](0.1000) => 0.1300
activation[10] = self(1.2998) * weight[10,10](0.1000) => 0.1300
  b. Update inputs from other neurons
activation[1] = self(0.0111) + network_input[1]=0.0000 => 0.0111
activation[2] = self(0.0111) + network_input[2]=0.0000 => 0.0111
activation[3] = self(0.0231) + output[1](0.1110) * 1.0000 => 0.1341
activation[3] = self(0.1341) + output[7](1.2998) * -0.5000 => -0.5158
activation[3] = self(-0.5158) + output[8](1.2998) * -0.5000 => -1.1657
activation[4] = self(0.0231) + output[1](0.1110) * 0.5000 => 0.0786
activation[4] = self(0.0786) + output[2](0.1110) * 0.5000 => 0.1341
activation[4] = self(0.1341) + output[6](1.2998) * -0.5000 => -0.5158
activation[4] = self(-0.5158) + output[8](1.2998) * -0.5000 => -1.1657
activation[5] = self(0.0231) + output[2](0.1110) * 1.0000 => 0.1341
activation[5] = self(0.1341) + output[6](1.2998) * -0.5000 => -0.5158
activation[5] = self(-0.5158) + output[7](1.2998) * -0.5000 => -1.1657
activation[6] = self(0.1300) + output[3](0.2309) * 1.0000 => 0.3608
activation[7] = self(0.1300) + output[4](0.2309) * 1.0000 => 0.3608
activation[8] = self(0.1300) + output[5](0.2309) * 1.0000 => 0.3608
activation[9] = self(0.1300) + output[3](0.2309) * 0.7500 => 0.3031
activation[9] = self(0.3031) + output[4](0.2309) * 0.2500 => 0.3608
activation[10] = self(0.1300) + output[4](0.2309) * 0.2500 => 0.1877
activation[10] = self(0.1877) + output[5](0.2309) * 0.7500 => 0.3608
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0111) > threshold(0.0000)? ==> 0.0111
neuronOutput[2]: activation(0.0111) > threshold(0.0000)? ==> 0.0111
neuronOutput[3]: activation(-1.1657) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-1.1657) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-1.1657) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.3608) > threshold(0.0000)? ==> 0.3608
neuronOutput[7]: activation(0.3608) > threshold(0.0000)? ==> 0.3608
neuronOutput[8]: activation(0.3608) > threshold(0.0000)? ==> 0.3608
neuronOutput[9]: activation(0.3608) > threshold(0.0000)? ==> 0.3608
neuronOutput[10]: activation(0.3608) > threshold(0.0000)? ==> 0.3608
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.3608)
networkOutput[2] := neuronOutput[10](0.3608)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.0111) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.0111) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.0111) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.0111) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=097 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0111) * weight[1,1](0.1000) => 0.0011
activation[2] = self(0.0111) * weight[2,2](0.1000) => 0.0011
activation[3] = self(-1.1657) * weight[3,3](0.1000) => -0.1166
activation[4] = self(-1.1657) * weight[4,4](0.1000) => -0.1166
activation[5] = self(-1.1657) * weight[5,5](0.1000) => -0.1166
activation[6] = self(0.3608) * weight[6,6](0.1000) => 0.0361
activation[7] = self(0.3608) * weight[7,7](0.1000) => 0.0361
activation[8] = self(0.3608) * weight[8,8](0.1000) => 0.0361
activation[9] = self(0.3608) * weight[9,9](0.1000) => 0.0361
activation[10] = self(0.3608) * weight[10,10](0.1000) => 0.0361
  b. Update inputs from other neurons
activation[1] = self(0.0011) + network_input[1]=1.0000 => 1.0011
activation[2] = self(0.0011) + network_input[2]=1.0000 => 1.0011
activation[3] = self(-0.1166) + output[1](0.0111) * 1.0000 => -0.1055
activation[3] = self(-0.1055) + output[7](0.3608) * -0.5000 => -0.2859
activation[3] = self(-0.2859) + output[8](0.3608) * -0.5000 => -0.4663
activation[4] = self(-0.1166) + output[1](0.0111) * 0.5000 => -0.1110
activation[4] = self(-0.1110) + output[2](0.0111) * 0.5000 => -0.1055
activation[4] = self(-0.1055) + output[6](0.3608) * -0.5000 => -0.2859
activation[4] = self(-0.2859) + output[8](0.3608) * -0.5000 => -0.4663
activation[5] = self(-0.1166) + output[2](0.0111) * 1.0000 => -0.1055
activation[5] = self(-0.1055) + output[6](0.3608) * -0.5000 => -0.2859
activation[5] = self(-0.2859) + output[7](0.3608) * -0.5000 => -0.4663
activation[6] = self(0.0361) + output[3](0.0000) * 1.0000 => 0.0361
activation[7] = self(0.0361) + output[4](0.0000) * 1.0000 => 0.0361
activation[8] = self(0.0361) + output[5](0.0000) * 1.0000 => 0.0361
activation[9] = self(0.0361) + output[3](0.0000) * 0.7500 => 0.0361
activation[9] = self(0.0361) + output[4](0.0000) * 0.2500 => 0.0361
activation[10] = self(0.0361) + output[4](0.0000) * 0.2500 => 0.0361
activation[10] = self(0.0361) + output[5](0.0000) * 0.7500 => 0.0361
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0011) > threshold(0.0000)? ==> 1.0011
neuronOutput[2]: activation(1.0011) > threshold(0.0000)? ==> 1.0011
neuronOutput[3]: activation(-0.4663) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-0.4663) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-0.4663) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.0361) > threshold(0.0000)? ==> 0.0361
neuronOutput[7]: activation(0.0361) > threshold(0.0000)? ==> 0.0361
neuronOutput[8]: activation(0.0361) > threshold(0.0000)? ==> 0.0361
neuronOutput[9]: activation(0.0361) > threshold(0.0000)? ==> 0.0361
neuronOutput[10]: activation(0.0361) > threshold(0.0000)? ==> 0.0361
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0361)
networkOutput[2] := neuronOutput[10](0.0361)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.0011) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.0011) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.0011) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.0011) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=098 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0011) * weight[1,1](0.1000) => 0.1001
activation[2] = self(1.0011) * weight[2,2](0.1000) => 0.1001
activation[3] = self(-0.4663) * weight[3,3](0.1000) => -0.0466
activation[4] = self(-0.4663) * weight[4,4](0.1000) => -0.0466
activation[5] = self(-0.4663) * weight[5,5](0.1000) => -0.0466
activation[6] = self(0.0361) * weight[6,6](0.1000) => 0.0036
activation[7] = self(0.0361) * weight[7,7](0.1000) => 0.0036
activation[8] = self(0.0361) * weight[8,8](0.1000) => 0.0036
activation[9] = self(0.0361) * weight[9,9](0.1000) => 0.0036
activation[10] = self(0.0361) * weight[10,10](0.1000) => 0.0036
  b. Update inputs from other neurons
activation[1] = self(0.1001) + network_input[1]=1.0000 => 1.1001
activation[2] = self(0.1001) + network_input[2]=1.0000 => 1.1001
activation[3] = self(-0.0466) + output[1](1.0011) * 1.0000 => 0.9545
activation[3] = self(0.9545) + output[7](0.0361) * -0.5000 => 0.9364
activation[3] = self(0.9364) + output[8](0.0361) * -0.5000 => 0.9184
activation[4] = self(-0.0466) + output[1](1.0011) * 0.5000 => 0.4539
activation[4] = self(0.4539) + output[2](1.0011) * 0.5000 => 0.9545
activation[4] = self(0.9545) + output[6](0.0361) * -0.5000 => 0.9364
activation[4] = self(0.9364) + output[8](0.0361) * -0.5000 => 0.9184
activation[5] = self(-0.0466) + output[2](1.0011) * 1.0000 => 0.9545
activation[5] = self(0.9545) + output[6](0.0361) * -0.5000 => 0.9364
activation[5] = self(0.9364) + output[7](0.0361) * -0.5000 => 0.9184
activation[6] = self(0.0036) + output[3](0.0000) * 1.0000 => 0.0036
activation[7] = self(0.0036) + output[4](0.0000) * 1.0000 => 0.0036
activation[8] = self(0.0036) + output[5](0.0000) * 1.0000 => 0.0036
activation[9] = self(0.0036) + output[3](0.0000) * 0.7500 => 0.0036
activation[9] = self(0.0036) + output[4](0.0000) * 0.2500 => 0.0036
activation[10] = self(0.0036) + output[4](0.0000) * 0.2500 => 0.0036
activation[10] = self(0.0036) + output[5](0.0000) * 0.7500 => 0.0036
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.1001) > threshold(0.0000)? ==> 1.1001
neuronOutput[2]: activation(1.1001) > threshold(0.0000)? ==> 1.1001
neuronOutput[3]: activation(0.9184) > threshold(0.0000)? ==> 0.9184
neuronOutput[4]: activation(0.9184) > threshold(0.0000)? ==> 0.9184
neuronOutput[5]: activation(0.9184) > threshold(0.0000)? ==> 0.9184
neuronOutput[6]: activation(0.0036) > threshold(0.0000)? ==> 0.0036
neuronOutput[7]: activation(0.0036) > threshold(0.0000)? ==> 0.0036
neuronOutput[8]: activation(0.0036) > threshold(0.0000)? ==> 0.0036
neuronOutput[9]: activation(0.0036) > threshold(0.0000)? ==> 0.0036
neuronOutput[10]: activation(0.0036) > threshold(0.0000)? ==> 0.0036
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0036)
networkOutput[2] := neuronOutput[10](0.0036)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.1001) * output[3](0.9184) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.9184) * output[3](0.9184) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.1001) * output[4](0.9184) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.1001) * output[4](0.9184) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.9184) * output[4](0.9184) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.1001) * output[5](0.9184) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.9184) * output[5](0.9184) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=099 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.1001) * weight[1,1](0.1000) => 0.1100
activation[2] = self(1.1001) * weight[2,2](0.1000) => 0.1100
activation[3] = self(0.9184) * weight[3,3](0.1000) => 0.0918
activation[4] = self(0.9184) * weight[4,4](0.1000) => 0.0918
activation[5] = self(0.9184) * weight[5,5](0.1000) => 0.0918
activation[6] = self(0.0036) * weight[6,6](0.1000) => 0.0004
activation[7] = self(0.0036) * weight[7,7](0.1000) => 0.0004
activation[8] = self(0.0036) * weight[8,8](0.1000) => 0.0004
activation[9] = self(0.0036) * weight[9,9](0.1000) => 0.0004
activation[10] = self(0.0036) * weight[10,10](0.1000) => 0.0004
  b. Update inputs from other neurons
activation[1] = self(0.1100) + network_input[1]=0.0000 => 0.1100
activation[2] = self(0.1100) + network_input[2]=0.0000 => 0.1100
activation[3] = self(0.0918) + output[1](1.1001) * 1.0000 => 1.1920
activation[3] = self(1.1920) + output[7](0.0036) * -0.5000 => 1.1901
activation[3] = self(1.1901) + output[8](0.0036) * -0.5000 => 1.1883
activation[4] = self(0.0918) + output[1](1.1001) * 0.5000 => 0.6419
activation[4] = self(0.6419) + output[2](1.1001) * 0.5000 => 1.1920
activation[4] = self(1.1920) + output[6](0.0036) * -0.5000 => 1.1901
activation[4] = self(1.1901) + output[8](0.0036) * -0.5000 => 1.1883
activation[5] = self(0.0918) + output[2](1.1001) * 1.0000 => 1.1920
activation[5] = self(1.1920) + output[6](0.0036) * -0.5000 => 1.1901
activation[5] = self(1.1901) + output[7](0.0036) * -0.5000 => 1.1883
activation[6] = self(0.0004) + output[3](0.9184) * 1.0000 => 0.9188
activation[7] = self(0.0004) + output[4](0.9184) * 1.0000 => 0.9188
activation[8] = self(0.0004) + output[5](0.9184) * 1.0000 => 0.9188
activation[9] = self(0.0004) + output[3](0.9184) * 0.7500 => 0.6892
activation[9] = self(0.6892) + output[4](0.9184) * 0.2500 => 0.9188
activation[10] = self(0.0004) + output[4](0.9184) * 0.2500 => 0.2300
activation[10] = self(0.2300) + output[5](0.9184) * 0.7500 => 0.9188
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.1100) > threshold(0.0000)? ==> 0.1100
neuronOutput[2]: activation(0.1100) > threshold(0.0000)? ==> 0.1100
neuronOutput[3]: activation(1.1883) > threshold(0.0000)? ==> 1.1883
neuronOutput[4]: activation(1.1883) > threshold(0.0000)? ==> 1.1883
neuronOutput[5]: activation(1.1883) > threshold(0.0000)? ==> 1.1883
neuronOutput[6]: activation(0.9188) > threshold(0.0000)? ==> 0.9188
neuronOutput[7]: activation(0.9188) > threshold(0.0000)? ==> 0.9188
neuronOutput[8]: activation(0.9188) > threshold(0.0000)? ==> 0.9188
neuronOutput[9]: activation(0.9188) > threshold(0.0000)? ==> 0.9188
neuronOutput[10]: activation(0.9188) > threshold(0.0000)? ==> 0.9188
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.9188)
networkOutput[2] := neuronOutput[10](0.9188)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.1100) * output[3](1.1883) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](1.1883) * output[3](1.1883) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.1100) * output[4](1.1883) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.1100) * output[4](1.1883) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](1.1883) * output[4](1.1883) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.1100) * output[5](1.1883) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](1.1883) * output[5](1.1883) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=100 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.1100) * weight[1,1](0.1000) => 0.0110
activation[2] = self(0.1100) * weight[2,2](0.1000) => 0.0110
activation[3] = self(1.1883) * weight[3,3](0.1000) => 0.1188
activation[4] = self(1.1883) * weight[4,4](0.1000) => 0.1188
activation[5] = self(1.1883) * weight[5,5](0.1000) => 0.1188
activation[6] = self(0.9188) * weight[6,6](0.1000) => 0.0919
activation[7] = self(0.9188) * weight[7,7](0.1000) => 0.0919
activation[8] = self(0.9188) * weight[8,8](0.1000) => 0.0919
activation[9] = self(0.9188) * weight[9,9](0.1000) => 0.0919
activation[10] = self(0.9188) * weight[10,10](0.1000) => 0.0919
  b. Update inputs from other neurons
activation[1] = self(0.0110) + network_input[1]=0.0000 => 0.0110
activation[2] = self(0.0110) + network_input[2]=0.0000 => 0.0110
activation[3] = self(0.1188) + output[1](0.1100) * 1.0000 => 0.2288
activation[3] = self(0.2288) + output[7](0.9188) * -0.5000 => -0.2305
activation[3] = self(-0.2305) + output[8](0.9188) * -0.5000 => -0.6899
activation[4] = self(0.1188) + output[1](0.1100) * 0.5000 => 0.1738
activation[4] = self(0.1738) + output[2](0.1100) * 0.5000 => 0.2288
activation[4] = self(0.2288) + output[6](0.9188) * -0.5000 => -0.2305
activation[4] = self(-0.2305) + output[8](0.9188) * -0.5000 => -0.6899
activation[5] = self(0.1188) + output[2](0.1100) * 1.0000 => 0.2288
activation[5] = self(0.2288) + output[6](0.9188) * -0.5000 => -0.2305
activation[5] = self(-0.2305) + output[7](0.9188) * -0.5000 => -0.6899
activation[6] = self(0.0919) + output[3](1.1883) * 1.0000 => 1.2802
activation[7] = self(0.0919) + output[4](1.1883) * 1.0000 => 1.2802
activation[8] = self(0.0919) + output[5](1.1883) * 1.0000 => 1.2802
activation[9] = self(0.0919) + output[3](1.1883) * 0.7500 => 0.9831
activation[9] = self(0.9831) + output[4](1.1883) * 0.2500 => 1.2802
activation[10] = self(0.0919) + output[4](1.1883) * 0.2500 => 0.3890
activation[10] = self(0.3890) + output[5](1.1883) * 0.7500 => 1.2802
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0110) > threshold(0.0000)? ==> 0.0110
neuronOutput[2]: activation(0.0110) > threshold(0.0000)? ==> 0.0110
neuronOutput[3]: activation(-0.6899) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-0.6899) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-0.6899) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(1.2802) > threshold(0.0000)? ==> 1.2802
neuronOutput[7]: activation(1.2802) > threshold(0.0000)? ==> 1.2802
neuronOutput[8]: activation(1.2802) > threshold(0.0000)? ==> 1.2802
neuronOutput[9]: activation(1.2802) > threshold(0.0000)? ==> 1.2802
neuronOutput[10]: activation(1.2802) > threshold(0.0000)? ==> 1.2802
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](1.2802)
networkOutput[2] := neuronOutput[10](1.2802)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.0110) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.0110) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.0110) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.0110) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=101 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0110) * weight[1,1](0.1000) => 0.0011
activation[2] = self(0.0110) * weight[2,2](0.1000) => 0.0011
activation[3] = self(-0.6899) * weight[3,3](0.1000) => -0.0690
activation[4] = self(-0.6899) * weight[4,4](0.1000) => -0.0690
activation[5] = self(-0.6899) * weight[5,5](0.1000) => -0.0690
activation[6] = self(1.2802) * weight[6,6](0.1000) => 0.1280
activation[7] = self(1.2802) * weight[7,7](0.1000) => 0.1280
activation[8] = self(1.2802) * weight[8,8](0.1000) => 0.1280
activation[9] = self(1.2802) * weight[9,9](0.1000) => 0.1280
activation[10] = self(1.2802) * weight[10,10](0.1000) => 0.1280
  b. Update inputs from other neurons
activation[1] = self(0.0011) + network_input[1]=0.0000 => 0.0011
activation[2] = self(0.0011) + network_input[2]=0.0000 => 0.0011
activation[3] = self(-0.0690) + output[1](0.0110) * 1.0000 => -0.0580
activation[3] = self(-0.0580) + output[7](1.2802) * -0.5000 => -0.6981
activation[3] = self(-0.6981) + output[8](1.2802) * -0.5000 => -1.3382
activation[4] = self(-0.0690) + output[1](0.0110) * 0.5000 => -0.0635
activation[4] = self(-0.0635) + output[2](0.0110) * 0.5000 => -0.0580
activation[4] = self(-0.0580) + output[6](1.2802) * -0.5000 => -0.6981
activation[4] = self(-0.6981) + output[8](1.2802) * -0.5000 => -1.3382
activation[5] = self(-0.0690) + output[2](0.0110) * 1.0000 => -0.0580
activation[5] = self(-0.0580) + output[6](1.2802) * -0.5000 => -0.6981
activation[5] = self(-0.6981) + output[7](1.2802) * -0.5000 => -1.3382
activation[6] = self(0.1280) + output[3](0.0000) * 1.0000 => 0.1280
activation[7] = self(0.1280) + output[4](0.0000) * 1.0000 => 0.1280
activation[8] = self(0.1280) + output[5](0.0000) * 1.0000 => 0.1280
activation[9] = self(0.1280) + output[3](0.0000) * 0.7500 => 0.1280
activation[9] = self(0.1280) + output[4](0.0000) * 0.2500 => 0.1280
activation[10] = self(0.1280) + output[4](0.0000) * 0.2500 => 0.1280
activation[10] = self(0.1280) + output[5](0.0000) * 0.7500 => 0.1280
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0011) > threshold(0.0000)? ==> 0.0011
neuronOutput[2]: activation(0.0011) > threshold(0.0000)? ==> 0.0011
neuronOutput[3]: activation(-1.3382) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-1.3382) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-1.3382) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.1280) > threshold(0.0000)? ==> 0.1280
neuronOutput[7]: activation(0.1280) > threshold(0.0000)? ==> 0.1280
neuronOutput[8]: activation(0.1280) > threshold(0.0000)? ==> 0.1280
neuronOutput[9]: activation(0.1280) > threshold(0.0000)? ==> 0.1280
neuronOutput[10]: activation(0.1280) > threshold(0.0000)? ==> 0.1280
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.1280)
networkOutput[2] := neuronOutput[10](0.1280)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.0011) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.0011) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.0011) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.0011) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=102 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0011) * weight[1,1](0.1000) => 0.0001
activation[2] = self(0.0011) * weight[2,2](0.1000) => 0.0001
activation[3] = self(-1.3382) * weight[3,3](0.1000) => -0.1338
activation[4] = self(-1.3382) * weight[4,4](0.1000) => -0.1338
activation[5] = self(-1.3382) * weight[5,5](0.1000) => -0.1338
activation[6] = self(0.1280) * weight[6,6](0.1000) => 0.0128
activation[7] = self(0.1280) * weight[7,7](0.1000) => 0.0128
activation[8] = self(0.1280) * weight[8,8](0.1000) => 0.0128
activation[9] = self(0.1280) * weight[9,9](0.1000) => 0.0128
activation[10] = self(0.1280) * weight[10,10](0.1000) => 0.0128
  b. Update inputs from other neurons
activation[1] = self(0.0001) + network_input[1]=1.0000 => 1.0001
activation[2] = self(0.0001) + network_input[2]=1.0000 => 1.0001
activation[3] = self(-0.1338) + output[1](0.0011) * 1.0000 => -0.1327
activation[3] = self(-0.1327) + output[7](0.1280) * -0.5000 => -0.1967
activation[3] = self(-0.1967) + output[8](0.1280) * -0.5000 => -0.2607
activation[4] = self(-0.1338) + output[1](0.0011) * 0.5000 => -0.1333
activation[4] = self(-0.1333) + output[2](0.0011) * 0.5000 => -0.1327
activation[4] = self(-0.1327) + output[6](0.1280) * -0.5000 => -0.1967
activation[4] = self(-0.1967) + output[8](0.1280) * -0.5000 => -0.2607
activation[5] = self(-0.1338) + output[2](0.0011) * 1.0000 => -0.1327
activation[5] = self(-0.1327) + output[6](0.1280) * -0.5000 => -0.1967
activation[5] = self(-0.1967) + output[7](0.1280) * -0.5000 => -0.2607
activation[6] = self(0.0128) + output[3](0.0000) * 1.0000 => 0.0128
activation[7] = self(0.0128) + output[4](0.0000) * 1.0000 => 0.0128
activation[8] = self(0.0128) + output[5](0.0000) * 1.0000 => 0.0128
activation[9] = self(0.0128) + output[3](0.0000) * 0.7500 => 0.0128
activation[9] = self(0.0128) + output[4](0.0000) * 0.2500 => 0.0128
activation[10] = self(0.0128) + output[4](0.0000) * 0.2500 => 0.0128
activation[10] = self(0.0128) + output[5](0.0000) * 0.7500 => 0.0128
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0001) > threshold(0.0000)? ==> 1.0001
neuronOutput[2]: activation(1.0001) > threshold(0.0000)? ==> 1.0001
neuronOutput[3]: activation(-0.2607) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-0.2607) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-0.2607) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.0128) > threshold(0.0000)? ==> 0.0128
neuronOutput[7]: activation(0.0128) > threshold(0.0000)? ==> 0.0128
neuronOutput[8]: activation(0.0128) > threshold(0.0000)? ==> 0.0128
neuronOutput[9]: activation(0.0128) > threshold(0.0000)? ==> 0.0128
neuronOutput[10]: activation(0.0128) > threshold(0.0000)? ==> 0.0128
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0128)
networkOutput[2] := neuronOutput[10](0.0128)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.0001) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.0001) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.0001) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.0001) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=103 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0001) * weight[1,1](0.1000) => 0.1000
activation[2] = self(1.0001) * weight[2,2](0.1000) => 0.1000
activation[3] = self(-0.2607) * weight[3,3](0.1000) => -0.0261
activation[4] = self(-0.2607) * weight[4,4](0.1000) => -0.0261
activation[5] = self(-0.2607) * weight[5,5](0.1000) => -0.0261
activation[6] = self(0.0128) * weight[6,6](0.1000) => 0.0013
activation[7] = self(0.0128) * weight[7,7](0.1000) => 0.0013
activation[8] = self(0.0128) * weight[8,8](0.1000) => 0.0013
activation[9] = self(0.0128) * weight[9,9](0.1000) => 0.0013
activation[10] = self(0.0128) * weight[10,10](0.1000) => 0.0013
  b. Update inputs from other neurons
activation[1] = self(0.1000) + network_input[1]=0.0000 => 0.1000
activation[2] = self(0.1000) + network_input[2]=0.0000 => 0.1000
activation[3] = self(-0.0261) + output[1](1.0001) * 1.0000 => 0.9740
activation[3] = self(0.9740) + output[7](0.0128) * -0.5000 => 0.9676
activation[3] = self(0.9676) + output[8](0.0128) * -0.5000 => 0.9612
activation[4] = self(-0.0261) + output[1](1.0001) * 0.5000 => 0.4740
activation[4] = self(0.4740) + output[2](1.0001) * 0.5000 => 0.9740
activation[4] = self(0.9740) + output[6](0.0128) * -0.5000 => 0.9676
activation[4] = self(0.9676) + output[8](0.0128) * -0.5000 => 0.9612
activation[5] = self(-0.0261) + output[2](1.0001) * 1.0000 => 0.9740
activation[5] = self(0.9740) + output[6](0.0128) * -0.5000 => 0.9676
activation[5] = self(0.9676) + output[7](0.0128) * -0.5000 => 0.9612
activation[6] = self(0.0013) + output[3](0.0000) * 1.0000 => 0.0013
activation[7] = self(0.0013) + output[4](0.0000) * 1.0000 => 0.0013
activation[8] = self(0.0013) + output[5](0.0000) * 1.0000 => 0.0013
activation[9] = self(0.0013) + output[3](0.0000) * 0.7500 => 0.0013
activation[9] = self(0.0013) + output[4](0.0000) * 0.2500 => 0.0013
activation[10] = self(0.0013) + output[4](0.0000) * 0.2500 => 0.0013
activation[10] = self(0.0013) + output[5](0.0000) * 0.7500 => 0.0013
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.1000) > threshold(0.0000)? ==> 0.1000
neuronOutput[2]: activation(0.1000) > threshold(0.0000)? ==> 0.1000
neuronOutput[3]: activation(0.9612) > threshold(0.0000)? ==> 0.9612
neuronOutput[4]: activation(0.9612) > threshold(0.0000)? ==> 0.9612
neuronOutput[5]: activation(0.9612) > threshold(0.0000)? ==> 0.9612
neuronOutput[6]: activation(0.0013) > threshold(0.0000)? ==> 0.0013
neuronOutput[7]: activation(0.0013) > threshold(0.0000)? ==> 0.0013
neuronOutput[8]: activation(0.0013) > threshold(0.0000)? ==> 0.0013
neuronOutput[9]: activation(0.0013) > threshold(0.0000)? ==> 0.0013
neuronOutput[10]: activation(0.0013) > threshold(0.0000)? ==> 0.0013
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0013)
networkOutput[2] := neuronOutput[10](0.0013)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.1000) * output[3](0.9612) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.9612) * output[3](0.9612) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.1000) * output[4](0.9612) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.1000) * output[4](0.9612) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.9612) * output[4](0.9612) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.1000) * output[5](0.9612) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.9612) * output[5](0.9612) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=104 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.1000) * weight[1,1](0.1000) => 0.0100
activation[2] = self(0.1000) * weight[2,2](0.1000) => 0.0100
activation[3] = self(0.9612) * weight[3,3](0.1000) => 0.0961
activation[4] = self(0.9612) * weight[4,4](0.1000) => 0.0961
activation[5] = self(0.9612) * weight[5,5](0.1000) => 0.0961
activation[6] = self(0.0013) * weight[6,6](0.1000) => 0.0001
activation[7] = self(0.0013) * weight[7,7](0.1000) => 0.0001
activation[8] = self(0.0013) * weight[8,8](0.1000) => 0.0001
activation[9] = self(0.0013) * weight[9,9](0.1000) => 0.0001
activation[10] = self(0.0013) * weight[10,10](0.1000) => 0.0001
  b. Update inputs from other neurons
activation[1] = self(0.0100) + network_input[1]=1.0000 => 1.0100
activation[2] = self(0.0100) + network_input[2]=1.0000 => 1.0100
activation[3] = self(0.0961) + output[1](0.1000) * 1.0000 => 0.1961
activation[3] = self(0.1961) + output[7](0.0013) * -0.5000 => 0.1955
activation[3] = self(0.1955) + output[8](0.0013) * -0.5000 => 0.1949
activation[4] = self(0.0961) + output[1](0.1000) * 0.5000 => 0.1461
activation[4] = self(0.1461) + output[2](0.1000) * 0.5000 => 0.1961
activation[4] = self(0.1961) + output[6](0.0013) * -0.5000 => 0.1955
activation[4] = self(0.1955) + output[8](0.0013) * -0.5000 => 0.1949
activation[5] = self(0.0961) + output[2](0.1000) * 1.0000 => 0.1961
activation[5] = self(0.1961) + output[6](0.0013) * -0.5000 => 0.1955
activation[5] = self(0.1955) + output[7](0.0013) * -0.5000 => 0.1949
activation[6] = self(0.0001) + output[3](0.9612) * 1.0000 => 0.9614
activation[7] = self(0.0001) + output[4](0.9612) * 1.0000 => 0.9614
activation[8] = self(0.0001) + output[5](0.9612) * 1.0000 => 0.9614
activation[9] = self(0.0001) + output[3](0.9612) * 0.7500 => 0.7211
activation[9] = self(0.7211) + output[4](0.9612) * 0.2500 => 0.9614
activation[10] = self(0.0001) + output[4](0.9612) * 0.2500 => 0.2404
activation[10] = self(0.2404) + output[5](0.9612) * 0.7500 => 0.9614
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0100) > threshold(0.0000)? ==> 1.0100
neuronOutput[2]: activation(1.0100) > threshold(0.0000)? ==> 1.0100
neuronOutput[3]: activation(0.1949) > threshold(0.0000)? ==> 0.1949
neuronOutput[4]: activation(0.1949) > threshold(0.0000)? ==> 0.1949
neuronOutput[5]: activation(0.1949) > threshold(0.0000)? ==> 0.1949
neuronOutput[6]: activation(0.9614) > threshold(0.0000)? ==> 0.9614
neuronOutput[7]: activation(0.9614) > threshold(0.0000)? ==> 0.9614
neuronOutput[8]: activation(0.9614) > threshold(0.0000)? ==> 0.9614
neuronOutput[9]: activation(0.9614) > threshold(0.0000)? ==> 0.9614
neuronOutput[10]: activation(0.9614) > threshold(0.0000)? ==> 0.9614
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.9614)
networkOutput[2] := neuronOutput[10](0.9614)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.0100) * output[3](0.1949) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.1949) * output[3](0.1949) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.0100) * output[4](0.1949) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.0100) * output[4](0.1949) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.1949) * output[4](0.1949) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.0100) * output[5](0.1949) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.1949) * output[5](0.1949) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=105 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0100) * weight[1,1](0.1000) => 0.1010
activation[2] = self(1.0100) * weight[2,2](0.1000) => 0.1010
activation[3] = self(0.1949) * weight[3,3](0.1000) => 0.0195
activation[4] = self(0.1949) * weight[4,4](0.1000) => 0.0195
activation[5] = self(0.1949) * weight[5,5](0.1000) => 0.0195
activation[6] = self(0.9614) * weight[6,6](0.1000) => 0.0961
activation[7] = self(0.9614) * weight[7,7](0.1000) => 0.0961
activation[8] = self(0.9614) * weight[8,8](0.1000) => 0.0961
activation[9] = self(0.9614) * weight[9,9](0.1000) => 0.0961
activation[10] = self(0.9614) * weight[10,10](0.1000) => 0.0961
  b. Update inputs from other neurons
activation[1] = self(0.1010) + network_input[1]=0.0000 => 0.1010
activation[2] = self(0.1010) + network_input[2]=0.0000 => 0.1010
activation[3] = self(0.0195) + output[1](1.0100) * 1.0000 => 1.0295
activation[3] = self(1.0295) + output[7](0.9614) * -0.5000 => 0.5488
activation[3] = self(0.5488) + output[8](0.9614) * -0.5000 => 0.0681
activation[4] = self(0.0195) + output[1](1.0100) * 0.5000 => 0.5245
activation[4] = self(0.5245) + output[2](1.0100) * 0.5000 => 1.0295
activation[4] = self(1.0295) + output[6](0.9614) * -0.5000 => 0.5488
activation[4] = self(0.5488) + output[8](0.9614) * -0.5000 => 0.0681
activation[5] = self(0.0195) + output[2](1.0100) * 1.0000 => 1.0295
activation[5] = self(1.0295) + output[6](0.9614) * -0.5000 => 0.5488
activation[5] = self(0.5488) + output[7](0.9614) * -0.5000 => 0.0681
activation[6] = self(0.0961) + output[3](0.1949) * 1.0000 => 0.2910
activation[7] = self(0.0961) + output[4](0.1949) * 1.0000 => 0.2910
activation[8] = self(0.0961) + output[5](0.1949) * 1.0000 => 0.2910
activation[9] = self(0.0961) + output[3](0.1949) * 0.7500 => 0.2423
activation[9] = self(0.2423) + output[4](0.1949) * 0.2500 => 0.2910
activation[10] = self(0.0961) + output[4](0.1949) * 0.2500 => 0.1448
activation[10] = self(0.1448) + output[5](0.1949) * 0.7500 => 0.2910
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.1010) > threshold(0.0000)? ==> 0.1010
neuronOutput[2]: activation(0.1010) > threshold(0.0000)? ==> 0.1010
neuronOutput[3]: activation(0.0681) > threshold(0.0000)? ==> 0.0681
neuronOutput[4]: activation(0.0681) > threshold(0.0000)? ==> 0.0681
neuronOutput[5]: activation(0.0681) > threshold(0.0000)? ==> 0.0681
neuronOutput[6]: activation(0.2910) > threshold(0.0000)? ==> 0.2910
neuronOutput[7]: activation(0.2910) > threshold(0.0000)? ==> 0.2910
neuronOutput[8]: activation(0.2910) > threshold(0.0000)? ==> 0.2910
neuronOutput[9]: activation(0.2910) > threshold(0.0000)? ==> 0.2910
neuronOutput[10]: activation(0.2910) > threshold(0.0000)? ==> 0.2910
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.2910)
networkOutput[2] := neuronOutput[10](0.2910)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.1010) * output[3](0.0681) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0681) * output[3](0.0681) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.1010) * output[4](0.0681) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.1010) * output[4](0.0681) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0681) * output[4](0.0681) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.1010) * output[5](0.0681) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0681) * output[5](0.0681) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=106 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.1010) * weight[1,1](0.1000) => 0.0101
activation[2] = self(0.1010) * weight[2,2](0.1000) => 0.0101
activation[3] = self(0.0681) * weight[3,3](0.1000) => 0.0068
activation[4] = self(0.0681) * weight[4,4](0.1000) => 0.0068
activation[5] = self(0.0681) * weight[5,5](0.1000) => 0.0068
activation[6] = self(0.2910) * weight[6,6](0.1000) => 0.0291
activation[7] = self(0.2910) * weight[7,7](0.1000) => 0.0291
activation[8] = self(0.2910) * weight[8,8](0.1000) => 0.0291
activation[9] = self(0.2910) * weight[9,9](0.1000) => 0.0291
activation[10] = self(0.2910) * weight[10,10](0.1000) => 0.0291
  b. Update inputs from other neurons
activation[1] = self(0.0101) + network_input[1]=1.0000 => 1.0101
activation[2] = self(0.0101) + network_input[2]=1.0000 => 1.0101
activation[3] = self(0.0068) + output[1](0.1010) * 1.0000 => 0.1078
activation[3] = self(0.1078) + output[7](0.2910) * -0.5000 => -0.0377
activation[3] = self(-0.0377) + output[8](0.2910) * -0.5000 => -0.1832
activation[4] = self(0.0068) + output[1](0.1010) * 0.5000 => 0.0573
activation[4] = self(0.0573) + output[2](0.1010) * 0.5000 => 0.1078
activation[4] = self(0.1078) + output[6](0.2910) * -0.5000 => -0.0377
activation[4] = self(-0.0377) + output[8](0.2910) * -0.5000 => -0.1832
activation[5] = self(0.0068) + output[2](0.1010) * 1.0000 => 0.1078
activation[5] = self(0.1078) + output[6](0.2910) * -0.5000 => -0.0377
activation[5] = self(-0.0377) + output[7](0.2910) * -0.5000 => -0.1832
activation[6] = self(0.0291) + output[3](0.0681) * 1.0000 => 0.0972
activation[7] = self(0.0291) + output[4](0.0681) * 1.0000 => 0.0972
activation[8] = self(0.0291) + output[5](0.0681) * 1.0000 => 0.0972
activation[9] = self(0.0291) + output[3](0.0681) * 0.7500 => 0.0802
activation[9] = self(0.0802) + output[4](0.0681) * 0.2500 => 0.0972
activation[10] = self(0.0291) + output[4](0.0681) * 0.2500 => 0.0461
activation[10] = self(0.0461) + output[5](0.0681) * 0.7500 => 0.0972
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0101) > threshold(0.0000)? ==> 1.0101
neuronOutput[2]: activation(1.0101) > threshold(0.0000)? ==> 1.0101
neuronOutput[3]: activation(-0.1832) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-0.1832) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-0.1832) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.0972) > threshold(0.0000)? ==> 0.0972
neuronOutput[7]: activation(0.0972) > threshold(0.0000)? ==> 0.0972
neuronOutput[8]: activation(0.0972) > threshold(0.0000)? ==> 0.0972
neuronOutput[9]: activation(0.0972) > threshold(0.0000)? ==> 0.0972
neuronOutput[10]: activation(0.0972) > threshold(0.0000)? ==> 0.0972
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0972)
networkOutput[2] := neuronOutput[10](0.0972)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.0101) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.0101) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.0101) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.0101) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=107 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0101) * weight[1,1](0.1000) => 0.1010
activation[2] = self(1.0101) * weight[2,2](0.1000) => 0.1010
activation[3] = self(-0.1832) * weight[3,3](0.1000) => -0.0183
activation[4] = self(-0.1832) * weight[4,4](0.1000) => -0.0183
activation[5] = self(-0.1832) * weight[5,5](0.1000) => -0.0183
activation[6] = self(0.0972) * weight[6,6](0.1000) => 0.0097
activation[7] = self(0.0972) * weight[7,7](0.1000) => 0.0097
activation[8] = self(0.0972) * weight[8,8](0.1000) => 0.0097
activation[9] = self(0.0972) * weight[9,9](0.1000) => 0.0097
activation[10] = self(0.0972) * weight[10,10](0.1000) => 0.0097
  b. Update inputs from other neurons
activation[1] = self(0.1010) + network_input[1]=1.0000 => 1.1010
activation[2] = self(0.1010) + network_input[2]=1.0000 => 1.1010
activation[3] = self(-0.0183) + output[1](1.0101) * 1.0000 => 0.9918
activation[3] = self(0.9918) + output[7](0.0972) * -0.5000 => 0.9432
activation[3] = self(0.9432) + output[8](0.0972) * -0.5000 => 0.8946
activation[4] = self(-0.0183) + output[1](1.0101) * 0.5000 => 0.4867
activation[4] = self(0.4867) + output[2](1.0101) * 0.5000 => 0.9918
activation[4] = self(0.9918) + output[6](0.0972) * -0.5000 => 0.9432
activation[4] = self(0.9432) + output[8](0.0972) * -0.5000 => 0.8946
activation[5] = self(-0.0183) + output[2](1.0101) * 1.0000 => 0.9918
activation[5] = self(0.9918) + output[6](0.0972) * -0.5000 => 0.9432
activation[5] = self(0.9432) + output[7](0.0972) * -0.5000 => 0.8946
activation[6] = self(0.0097) + output[3](0.0000) * 1.0000 => 0.0097
activation[7] = self(0.0097) + output[4](0.0000) * 1.0000 => 0.0097
activation[8] = self(0.0097) + output[5](0.0000) * 1.0000 => 0.0097
activation[9] = self(0.0097) + output[3](0.0000) * 0.7500 => 0.0097
activation[9] = self(0.0097) + output[4](0.0000) * 0.2500 => 0.0097
activation[10] = self(0.0097) + output[4](0.0000) * 0.2500 => 0.0097
activation[10] = self(0.0097) + output[5](0.0000) * 0.7500 => 0.0097
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.1010) > threshold(0.0000)? ==> 1.1010
neuronOutput[2]: activation(1.1010) > threshold(0.0000)? ==> 1.1010
neuronOutput[3]: activation(0.8946) > threshold(0.0000)? ==> 0.8946
neuronOutput[4]: activation(0.8946) > threshold(0.0000)? ==> 0.8946
neuronOutput[5]: activation(0.8946) > threshold(0.0000)? ==> 0.8946
neuronOutput[6]: activation(0.0097) > threshold(0.0000)? ==> 0.0097
neuronOutput[7]: activation(0.0097) > threshold(0.0000)? ==> 0.0097
neuronOutput[8]: activation(0.0097) > threshold(0.0000)? ==> 0.0097
neuronOutput[9]: activation(0.0097) > threshold(0.0000)? ==> 0.0097
neuronOutput[10]: activation(0.0097) > threshold(0.0000)? ==> 0.0097
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0097)
networkOutput[2] := neuronOutput[10](0.0097)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.1010) * output[3](0.8946) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.8946) * output[3](0.8946) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.1010) * output[4](0.8946) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.1010) * output[4](0.8946) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.8946) * output[4](0.8946) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.1010) * output[5](0.8946) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.8946) * output[5](0.8946) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=108 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.1010) * weight[1,1](0.1000) => 0.1101
activation[2] = self(1.1010) * weight[2,2](0.1000) => 0.1101
activation[3] = self(0.8946) * weight[3,3](0.1000) => 0.0895
activation[4] = self(0.8946) * weight[4,4](0.1000) => 0.0895
activation[5] = self(0.8946) * weight[5,5](0.1000) => 0.0895
activation[6] = self(0.0097) * weight[6,6](0.1000) => 0.0010
activation[7] = self(0.0097) * weight[7,7](0.1000) => 0.0010
activation[8] = self(0.0097) * weight[8,8](0.1000) => 0.0010
activation[9] = self(0.0097) * weight[9,9](0.1000) => 0.0010
activation[10] = self(0.0097) * weight[10,10](0.1000) => 0.0010
  b. Update inputs from other neurons
activation[1] = self(0.1101) + network_input[1]=1.0000 => 1.1101
activation[2] = self(0.1101) + network_input[2]=1.0000 => 1.1101
activation[3] = self(0.0895) + output[1](1.1010) * 1.0000 => 1.1905
activation[3] = self(1.1905) + output[7](0.0097) * -0.5000 => 1.1856
activation[3] = self(1.1856) + output[8](0.0097) * -0.5000 => 1.1807
activation[4] = self(0.0895) + output[1](1.1010) * 0.5000 => 0.6400
activation[4] = self(0.6400) + output[2](1.1010) * 0.5000 => 1.1905
activation[4] = self(1.1905) + output[6](0.0097) * -0.5000 => 1.1856
activation[4] = self(1.1856) + output[8](0.0097) * -0.5000 => 1.1807
activation[5] = self(0.0895) + output[2](1.1010) * 1.0000 => 1.1905
activation[5] = self(1.1905) + output[6](0.0097) * -0.5000 => 1.1856
activation[5] = self(1.1856) + output[7](0.0097) * -0.5000 => 1.1807
activation[6] = self(0.0010) + output[3](0.8946) * 1.0000 => 0.8955
activation[7] = self(0.0010) + output[4](0.8946) * 1.0000 => 0.8955
activation[8] = self(0.0010) + output[5](0.8946) * 1.0000 => 0.8955
activation[9] = self(0.0010) + output[3](0.8946) * 0.7500 => 0.6719
activation[9] = self(0.6719) + output[4](0.8946) * 0.2500 => 0.8955
activation[10] = self(0.0010) + output[4](0.8946) * 0.2500 => 0.2246
activation[10] = self(0.2246) + output[5](0.8946) * 0.7500 => 0.8955
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.1101) > threshold(0.0000)? ==> 1.1101
neuronOutput[2]: activation(1.1101) > threshold(0.0000)? ==> 1.1101
neuronOutput[3]: activation(1.1807) > threshold(0.0000)? ==> 1.1807
neuronOutput[4]: activation(1.1807) > threshold(0.0000)? ==> 1.1807
neuronOutput[5]: activation(1.1807) > threshold(0.0000)? ==> 1.1807
neuronOutput[6]: activation(0.8955) > threshold(0.0000)? ==> 0.8955
neuronOutput[7]: activation(0.8955) > threshold(0.0000)? ==> 0.8955
neuronOutput[8]: activation(0.8955) > threshold(0.0000)? ==> 0.8955
neuronOutput[9]: activation(0.8955) > threshold(0.0000)? ==> 0.8955
neuronOutput[10]: activation(0.8955) > threshold(0.0000)? ==> 0.8955
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.8955)
networkOutput[2] := neuronOutput[10](0.8955)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.1101) * output[3](1.1807) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](1.1807) * output[3](1.1807) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.1101) * output[4](1.1807) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.1101) * output[4](1.1807) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](1.1807) * output[4](1.1807) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.1101) * output[5](1.1807) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](1.1807) * output[5](1.1807) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=109 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.1101) * weight[1,1](0.1000) => 0.1110
activation[2] = self(1.1101) * weight[2,2](0.1000) => 0.1110
activation[3] = self(1.1807) * weight[3,3](0.1000) => 0.1181
activation[4] = self(1.1807) * weight[4,4](0.1000) => 0.1181
activation[5] = self(1.1807) * weight[5,5](0.1000) => 0.1181
activation[6] = self(0.8955) * weight[6,6](0.1000) => 0.0896
activation[7] = self(0.8955) * weight[7,7](0.1000) => 0.0896
activation[8] = self(0.8955) * weight[8,8](0.1000) => 0.0896
activation[9] = self(0.8955) * weight[9,9](0.1000) => 0.0896
activation[10] = self(0.8955) * weight[10,10](0.1000) => 0.0896
  b. Update inputs from other neurons
activation[1] = self(0.1110) + network_input[1]=0.0000 => 0.1110
activation[2] = self(0.1110) + network_input[2]=0.0000 => 0.1110
activation[3] = self(0.1181) + output[1](1.1101) * 1.0000 => 1.2282
activation[3] = self(1.2282) + output[7](0.8955) * -0.5000 => 0.7804
activation[3] = self(0.7804) + output[8](0.8955) * -0.5000 => 0.3326
activation[4] = self(0.1181) + output[1](1.1101) * 0.5000 => 0.6731
activation[4] = self(0.6731) + output[2](1.1101) * 0.5000 => 1.2282
activation[4] = self(1.2282) + output[6](0.8955) * -0.5000 => 0.7804
activation[4] = self(0.7804) + output[8](0.8955) * -0.5000 => 0.3326
activation[5] = self(0.1181) + output[2](1.1101) * 1.0000 => 1.2282
activation[5] = self(1.2282) + output[6](0.8955) * -0.5000 => 0.7804
activation[5] = self(0.7804) + output[7](0.8955) * -0.5000 => 0.3326
activation[6] = self(0.0896) + output[3](1.1807) * 1.0000 => 1.2703
activation[7] = self(0.0896) + output[4](1.1807) * 1.0000 => 1.2703
activation[8] = self(0.0896) + output[5](1.1807) * 1.0000 => 1.2703
activation[9] = self(0.0896) + output[3](1.1807) * 0.7500 => 0.9751
activation[9] = self(0.9751) + output[4](1.1807) * 0.2500 => 1.2703
activation[10] = self(0.0896) + output[4](1.1807) * 0.2500 => 0.3847
activation[10] = self(0.3847) + output[5](1.1807) * 0.7500 => 1.2703
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.1110) > threshold(0.0000)? ==> 0.1110
neuronOutput[2]: activation(0.1110) > threshold(0.0000)? ==> 0.1110
neuronOutput[3]: activation(0.3326) > threshold(0.0000)? ==> 0.3326
neuronOutput[4]: activation(0.3326) > threshold(0.0000)? ==> 0.3326
neuronOutput[5]: activation(0.3326) > threshold(0.0000)? ==> 0.3326
neuronOutput[6]: activation(1.2703) > threshold(0.0000)? ==> 1.2703
neuronOutput[7]: activation(1.2703) > threshold(0.0000)? ==> 1.2703
neuronOutput[8]: activation(1.2703) > threshold(0.0000)? ==> 1.2703
neuronOutput[9]: activation(1.2703) > threshold(0.0000)? ==> 1.2703
neuronOutput[10]: activation(1.2703) > threshold(0.0000)? ==> 1.2703
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](1.2703)
networkOutput[2] := neuronOutput[10](1.2703)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.1110) * output[3](0.3326) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.3326) * output[3](0.3326) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.1110) * output[4](0.3326) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.1110) * output[4](0.3326) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.3326) * output[4](0.3326) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.1110) * output[5](0.3326) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.3326) * output[5](0.3326) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=110 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.1110) * weight[1,1](0.1000) => 0.0111
activation[2] = self(0.1110) * weight[2,2](0.1000) => 0.0111
activation[3] = self(0.3326) * weight[3,3](0.1000) => 0.0333
activation[4] = self(0.3326) * weight[4,4](0.1000) => 0.0333
activation[5] = self(0.3326) * weight[5,5](0.1000) => 0.0333
activation[6] = self(1.2703) * weight[6,6](0.1000) => 0.1270
activation[7] = self(1.2703) * weight[7,7](0.1000) => 0.1270
activation[8] = self(1.2703) * weight[8,8](0.1000) => 0.1270
activation[9] = self(1.2703) * weight[9,9](0.1000) => 0.1270
activation[10] = self(1.2703) * weight[10,10](0.1000) => 0.1270
  b. Update inputs from other neurons
activation[1] = self(0.0111) + network_input[1]=1.0000 => 1.0111
activation[2] = self(0.0111) + network_input[2]=1.0000 => 1.0111
activation[3] = self(0.0333) + output[1](0.1110) * 1.0000 => 0.1443
activation[3] = self(0.1443) + output[7](1.2703) * -0.5000 => -0.4909
activation[3] = self(-0.4909) + output[8](1.2703) * -0.5000 => -1.1260
activation[4] = self(0.0333) + output[1](0.1110) * 0.5000 => 0.0888
activation[4] = self(0.0888) + output[2](0.1110) * 0.5000 => 0.1443
activation[4] = self(0.1443) + output[6](1.2703) * -0.5000 => -0.4909
activation[4] = self(-0.4909) + output[8](1.2703) * -0.5000 => -1.1260
activation[5] = self(0.0333) + output[2](0.1110) * 1.0000 => 0.1443
activation[5] = self(0.1443) + output[6](1.2703) * -0.5000 => -0.4909
activation[5] = self(-0.4909) + output[7](1.2703) * -0.5000 => -1.1260
activation[6] = self(0.1270) + output[3](0.3326) * 1.0000 => 0.4597
activation[7] = self(0.1270) + output[4](0.3326) * 1.0000 => 0.4597
activation[8] = self(0.1270) + output[5](0.3326) * 1.0000 => 0.4597
activation[9] = self(0.1270) + output[3](0.3326) * 0.7500 => 0.3765
activation[9] = self(0.3765) + output[4](0.3326) * 0.2500 => 0.4597
activation[10] = self(0.1270) + output[4](0.3326) * 0.2500 => 0.2102
activation[10] = self(0.2102) + output[5](0.3326) * 0.7500 => 0.4597
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0111) > threshold(0.0000)? ==> 1.0111
neuronOutput[2]: activation(1.0111) > threshold(0.0000)? ==> 1.0111
neuronOutput[3]: activation(-1.1260) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-1.1260) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-1.1260) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.4597) > threshold(0.0000)? ==> 0.4597
neuronOutput[7]: activation(0.4597) > threshold(0.0000)? ==> 0.4597
neuronOutput[8]: activation(0.4597) > threshold(0.0000)? ==> 0.4597
neuronOutput[9]: activation(0.4597) > threshold(0.0000)? ==> 0.4597
neuronOutput[10]: activation(0.4597) > threshold(0.0000)? ==> 0.4597
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.4597)
networkOutput[2] := neuronOutput[10](0.4597)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.0111) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.0111) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.0111) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.0111) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=111 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0111) * weight[1,1](0.1000) => 0.1011
activation[2] = self(1.0111) * weight[2,2](0.1000) => 0.1011
activation[3] = self(-1.1260) * weight[3,3](0.1000) => -0.1126
activation[4] = self(-1.1260) * weight[4,4](0.1000) => -0.1126
activation[5] = self(-1.1260) * weight[5,5](0.1000) => -0.1126
activation[6] = self(0.4597) * weight[6,6](0.1000) => 0.0460
activation[7] = self(0.4597) * weight[7,7](0.1000) => 0.0460
activation[8] = self(0.4597) * weight[8,8](0.1000) => 0.0460
activation[9] = self(0.4597) * weight[9,9](0.1000) => 0.0460
activation[10] = self(0.4597) * weight[10,10](0.1000) => 0.0460
  b. Update inputs from other neurons
activation[1] = self(0.1011) + network_input[1]=1.0000 => 1.1011
activation[2] = self(0.1011) + network_input[2]=1.0000 => 1.1011
activation[3] = self(-0.1126) + output[1](1.0111) * 1.0000 => 0.8985
activation[3] = self(0.8985) + output[7](0.4597) * -0.5000 => 0.6687
activation[3] = self(0.6687) + output[8](0.4597) * -0.5000 => 0.4388
activation[4] = self(-0.1126) + output[1](1.0111) * 0.5000 => 0.3929
activation[4] = self(0.3929) + output[2](1.0111) * 0.5000 => 0.8985
activation[4] = self(0.8985) + output[6](0.4597) * -0.5000 => 0.6687
activation[4] = self(0.6687) + output[8](0.4597) * -0.5000 => 0.4388
activation[5] = self(-0.1126) + output[2](1.0111) * 1.0000 => 0.8985
activation[5] = self(0.8985) + output[6](0.4597) * -0.5000 => 0.6687
activation[5] = self(0.6687) + output[7](0.4597) * -0.5000 => 0.4388
activation[6] = self(0.0460) + output[3](0.0000) * 1.0000 => 0.0460
activation[7] = self(0.0460) + output[4](0.0000) * 1.0000 => 0.0460
activation[8] = self(0.0460) + output[5](0.0000) * 1.0000 => 0.0460
activation[9] = self(0.0460) + output[3](0.0000) * 0.7500 => 0.0460
activation[9] = self(0.0460) + output[4](0.0000) * 0.2500 => 0.0460
activation[10] = self(0.0460) + output[4](0.0000) * 0.2500 => 0.0460
activation[10] = self(0.0460) + output[5](0.0000) * 0.7500 => 0.0460
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.1011) > threshold(0.0000)? ==> 1.1011
neuronOutput[2]: activation(1.1011) > threshold(0.0000)? ==> 1.1011
neuronOutput[3]: activation(0.4388) > threshold(0.0000)? ==> 0.4388
neuronOutput[4]: activation(0.4388) > threshold(0.0000)? ==> 0.4388
neuronOutput[5]: activation(0.4388) > threshold(0.0000)? ==> 0.4388
neuronOutput[6]: activation(0.0460) > threshold(0.0000)? ==> 0.0460
neuronOutput[7]: activation(0.0460) > threshold(0.0000)? ==> 0.0460
neuronOutput[8]: activation(0.0460) > threshold(0.0000)? ==> 0.0460
neuronOutput[9]: activation(0.0460) > threshold(0.0000)? ==> 0.0460
neuronOutput[10]: activation(0.0460) > threshold(0.0000)? ==> 0.0460
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0460)
networkOutput[2] := neuronOutput[10](0.0460)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.1011) * output[3](0.4388) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.4388) * output[3](0.4388) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.1011) * output[4](0.4388) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.1011) * output[4](0.4388) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.4388) * output[4](0.4388) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.1011) * output[5](0.4388) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.4388) * output[5](0.4388) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=112 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.1011) * weight[1,1](0.1000) => 0.1101
activation[2] = self(1.1011) * weight[2,2](0.1000) => 0.1101
activation[3] = self(0.4388) * weight[3,3](0.1000) => 0.0439
activation[4] = self(0.4388) * weight[4,4](0.1000) => 0.0439
activation[5] = self(0.4388) * weight[5,5](0.1000) => 0.0439
activation[6] = self(0.0460) * weight[6,6](0.1000) => 0.0046
activation[7] = self(0.0460) * weight[7,7](0.1000) => 0.0046
activation[8] = self(0.0460) * weight[8,8](0.1000) => 0.0046
activation[9] = self(0.0460) * weight[9,9](0.1000) => 0.0046
activation[10] = self(0.0460) * weight[10,10](0.1000) => 0.0046
  b. Update inputs from other neurons
activation[1] = self(0.1101) + network_input[1]=0.0000 => 0.1101
activation[2] = self(0.1101) + network_input[2]=0.0000 => 0.1101
activation[3] = self(0.0439) + output[1](1.1011) * 1.0000 => 1.1450
activation[3] = self(1.1450) + output[7](0.0460) * -0.5000 => 1.1220
activation[3] = self(1.1220) + output[8](0.0460) * -0.5000 => 1.0990
activation[4] = self(0.0439) + output[1](1.1011) * 0.5000 => 0.5944
activation[4] = self(0.5944) + output[2](1.1011) * 0.5000 => 1.1450
activation[4] = self(1.1450) + output[6](0.0460) * -0.5000 => 1.1220
activation[4] = self(1.1220) + output[8](0.0460) * -0.5000 => 1.0990
activation[5] = self(0.0439) + output[2](1.1011) * 1.0000 => 1.1450
activation[5] = self(1.1450) + output[6](0.0460) * -0.5000 => 1.1220
activation[5] = self(1.1220) + output[7](0.0460) * -0.5000 => 1.0990
activation[6] = self(0.0046) + output[3](0.4388) * 1.0000 => 0.4434
activation[7] = self(0.0046) + output[4](0.4388) * 1.0000 => 0.4434
activation[8] = self(0.0046) + output[5](0.4388) * 1.0000 => 0.4434
activation[9] = self(0.0046) + output[3](0.4388) * 0.7500 => 0.3337
activation[9] = self(0.3337) + output[4](0.4388) * 0.2500 => 0.4434
activation[10] = self(0.0046) + output[4](0.4388) * 0.2500 => 0.1143
activation[10] = self(0.1143) + output[5](0.4388) * 0.7500 => 0.4434
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.1101) > threshold(0.0000)? ==> 0.1101
neuronOutput[2]: activation(0.1101) > threshold(0.0000)? ==> 0.1101
neuronOutput[3]: activation(1.0990) > threshold(0.0000)? ==> 1.0990
neuronOutput[4]: activation(1.0990) > threshold(0.0000)? ==> 1.0990
neuronOutput[5]: activation(1.0990) > threshold(0.0000)? ==> 1.0990
neuronOutput[6]: activation(0.4434) > threshold(0.0000)? ==> 0.4434
neuronOutput[7]: activation(0.4434) > threshold(0.0000)? ==> 0.4434
neuronOutput[8]: activation(0.4434) > threshold(0.0000)? ==> 0.4434
neuronOutput[9]: activation(0.4434) > threshold(0.0000)? ==> 0.4434
neuronOutput[10]: activation(0.4434) > threshold(0.0000)? ==> 0.4434
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.4434)
networkOutput[2] := neuronOutput[10](0.4434)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.1101) * output[3](1.0990) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](1.0990) * output[3](1.0990) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.1101) * output[4](1.0990) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.1101) * output[4](1.0990) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](1.0990) * output[4](1.0990) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.1101) * output[5](1.0990) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](1.0990) * output[5](1.0990) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=113 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.1101) * weight[1,1](0.1000) => 0.0110
activation[2] = self(0.1101) * weight[2,2](0.1000) => 0.0110
activation[3] = self(1.0990) * weight[3,3](0.1000) => 0.1099
activation[4] = self(1.0990) * weight[4,4](0.1000) => 0.1099
activation[5] = self(1.0990) * weight[5,5](0.1000) => 0.1099
activation[6] = self(0.4434) * weight[6,6](0.1000) => 0.0443
activation[7] = self(0.4434) * weight[7,7](0.1000) => 0.0443
activation[8] = self(0.4434) * weight[8,8](0.1000) => 0.0443
activation[9] = self(0.4434) * weight[9,9](0.1000) => 0.0443
activation[10] = self(0.4434) * weight[10,10](0.1000) => 0.0443
  b. Update inputs from other neurons
activation[1] = self(0.0110) + network_input[1]=1.0000 => 1.0110
activation[2] = self(0.0110) + network_input[2]=1.0000 => 1.0110
activation[3] = self(0.1099) + output[1](0.1101) * 1.0000 => 0.2200
activation[3] = self(0.2200) + output[7](0.4434) * -0.5000 => -0.0017
activation[3] = self(-0.0017) + output[8](0.4434) * -0.5000 => -0.2234
activation[4] = self(0.1099) + output[1](0.1101) * 0.5000 => 0.1650
activation[4] = self(0.1650) + output[2](0.1101) * 0.5000 => 0.2200
activation[4] = self(0.2200) + output[6](0.4434) * -0.5000 => -0.0017
activation[4] = self(-0.0017) + output[8](0.4434) * -0.5000 => -0.2234
activation[5] = self(0.1099) + output[2](0.1101) * 1.0000 => 0.2200
activation[5] = self(0.2200) + output[6](0.4434) * -0.5000 => -0.0017
activation[5] = self(-0.0017) + output[7](0.4434) * -0.5000 => -0.2234
activation[6] = self(0.0443) + output[3](1.0990) * 1.0000 => 1.1434
activation[7] = self(0.0443) + output[4](1.0990) * 1.0000 => 1.1434
activation[8] = self(0.0443) + output[5](1.0990) * 1.0000 => 1.1434
activation[9] = self(0.0443) + output[3](1.0990) * 0.7500 => 0.8686
activation[9] = self(0.8686) + output[4](1.0990) * 0.2500 => 1.1434
activation[10] = self(0.0443) + output[4](1.0990) * 0.2500 => 0.3191
activation[10] = self(0.3191) + output[5](1.0990) * 0.7500 => 1.1434
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0110) > threshold(0.0000)? ==> 1.0110
neuronOutput[2]: activation(1.0110) > threshold(0.0000)? ==> 1.0110
neuronOutput[3]: activation(-0.2234) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-0.2234) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-0.2234) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(1.1434) > threshold(0.0000)? ==> 1.1434
neuronOutput[7]: activation(1.1434) > threshold(0.0000)? ==> 1.1434
neuronOutput[8]: activation(1.1434) > threshold(0.0000)? ==> 1.1434
neuronOutput[9]: activation(1.1434) > threshold(0.0000)? ==> 1.1434
neuronOutput[10]: activation(1.1434) > threshold(0.0000)? ==> 1.1434
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](1.1434)
networkOutput[2] := neuronOutput[10](1.1434)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.0110) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.0110) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.0110) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.0110) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=114 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0110) * weight[1,1](0.1000) => 0.1011
activation[2] = self(1.0110) * weight[2,2](0.1000) => 0.1011
activation[3] = self(-0.2234) * weight[3,3](0.1000) => -0.0223
activation[4] = self(-0.2234) * weight[4,4](0.1000) => -0.0223
activation[5] = self(-0.2234) * weight[5,5](0.1000) => -0.0223
activation[6] = self(1.1434) * weight[6,6](0.1000) => 0.1143
activation[7] = self(1.1434) * weight[7,7](0.1000) => 0.1143
activation[8] = self(1.1434) * weight[8,8](0.1000) => 0.1143
activation[9] = self(1.1434) * weight[9,9](0.1000) => 0.1143
activation[10] = self(1.1434) * weight[10,10](0.1000) => 0.1143
  b. Update inputs from other neurons
activation[1] = self(0.1011) + network_input[1]=0.0000 => 0.1011
activation[2] = self(0.1011) + network_input[2]=0.0000 => 0.1011
activation[3] = self(-0.0223) + output[1](1.0110) * 1.0000 => 0.9887
activation[3] = self(0.9887) + output[7](1.1434) * -0.5000 => 0.4170
activation[3] = self(0.4170) + output[8](1.1434) * -0.5000 => -0.1547
activation[4] = self(-0.0223) + output[1](1.0110) * 0.5000 => 0.4832
activation[4] = self(0.4832) + output[2](1.0110) * 0.5000 => 0.9887
activation[4] = self(0.9887) + output[6](1.1434) * -0.5000 => 0.4170
activation[4] = self(0.4170) + output[8](1.1434) * -0.5000 => -0.1547
activation[5] = self(-0.0223) + output[2](1.0110) * 1.0000 => 0.9887
activation[5] = self(0.9887) + output[6](1.1434) * -0.5000 => 0.4170
activation[5] = self(0.4170) + output[7](1.1434) * -0.5000 => -0.1547
activation[6] = self(0.1143) + output[3](0.0000) * 1.0000 => 0.1143
activation[7] = self(0.1143) + output[4](0.0000) * 1.0000 => 0.1143
activation[8] = self(0.1143) + output[5](0.0000) * 1.0000 => 0.1143
activation[9] = self(0.1143) + output[3](0.0000) * 0.7500 => 0.1143
activation[9] = self(0.1143) + output[4](0.0000) * 0.2500 => 0.1143
activation[10] = self(0.1143) + output[4](0.0000) * 0.2500 => 0.1143
activation[10] = self(0.1143) + output[5](0.0000) * 0.7500 => 0.1143
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.1011) > threshold(0.0000)? ==> 0.1011
neuronOutput[2]: activation(0.1011) > threshold(0.0000)? ==> 0.1011
neuronOutput[3]: activation(-0.1547) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-0.1547) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-0.1547) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.1143) > threshold(0.0000)? ==> 0.1143
neuronOutput[7]: activation(0.1143) > threshold(0.0000)? ==> 0.1143
neuronOutput[8]: activation(0.1143) > threshold(0.0000)? ==> 0.1143
neuronOutput[9]: activation(0.1143) > threshold(0.0000)? ==> 0.1143
neuronOutput[10]: activation(0.1143) > threshold(0.0000)? ==> 0.1143
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.1143)
networkOutput[2] := neuronOutput[10](0.1143)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.1011) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.1011) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.1011) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.1011) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=115 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.1011) * weight[1,1](0.1000) => 0.0101
activation[2] = self(0.1011) * weight[2,2](0.1000) => 0.0101
activation[3] = self(-0.1547) * weight[3,3](0.1000) => -0.0155
activation[4] = self(-0.1547) * weight[4,4](0.1000) => -0.0155
activation[5] = self(-0.1547) * weight[5,5](0.1000) => -0.0155
activation[6] = self(0.1143) * weight[6,6](0.1000) => 0.0114
activation[7] = self(0.1143) * weight[7,7](0.1000) => 0.0114
activation[8] = self(0.1143) * weight[8,8](0.1000) => 0.0114
activation[9] = self(0.1143) * weight[9,9](0.1000) => 0.0114
activation[10] = self(0.1143) * weight[10,10](0.1000) => 0.0114
  b. Update inputs from other neurons
activation[1] = self(0.0101) + network_input[1]=1.0000 => 1.0101
activation[2] = self(0.0101) + network_input[2]=1.0000 => 1.0101
activation[3] = self(-0.0155) + output[1](0.1011) * 1.0000 => 0.0856
activation[3] = self(0.0856) + output[7](0.1143) * -0.5000 => 0.0285
activation[3] = self(0.0285) + output[8](0.1143) * -0.5000 => -0.0287
activation[4] = self(-0.0155) + output[1](0.1011) * 0.5000 => 0.0351
activation[4] = self(0.0351) + output[2](0.1011) * 0.5000 => 0.0856
activation[4] = self(0.0856) + output[6](0.1143) * -0.5000 => 0.0285
activation[4] = self(0.0285) + output[8](0.1143) * -0.5000 => -0.0287
activation[5] = self(-0.0155) + output[2](0.1011) * 1.0000 => 0.0856
activation[5] = self(0.0856) + output[6](0.1143) * -0.5000 => 0.0285
activation[5] = self(0.0285) + output[7](0.1143) * -0.5000 => -0.0287
activation[6] = self(0.0114) + output[3](0.0000) * 1.0000 => 0.0114
activation[7] = self(0.0114) + output[4](0.0000) * 1.0000 => 0.0114
activation[8] = self(0.0114) + output[5](0.0000) * 1.0000 => 0.0114
activation[9] = self(0.0114) + output[3](0.0000) * 0.7500 => 0.0114
activation[9] = self(0.0114) + output[4](0.0000) * 0.2500 => 0.0114
activation[10] = self(0.0114) + output[4](0.0000) * 0.2500 => 0.0114
activation[10] = self(0.0114) + output[5](0.0000) * 0.7500 => 0.0114
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0101) > threshold(0.0000)? ==> 1.0101
neuronOutput[2]: activation(1.0101) > threshold(0.0000)? ==> 1.0101
neuronOutput[3]: activation(-0.0287) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-0.0287) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-0.0287) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.0114) > threshold(0.0000)? ==> 0.0114
neuronOutput[7]: activation(0.0114) > threshold(0.0000)? ==> 0.0114
neuronOutput[8]: activation(0.0114) > threshold(0.0000)? ==> 0.0114
neuronOutput[9]: activation(0.0114) > threshold(0.0000)? ==> 0.0114
neuronOutput[10]: activation(0.0114) > threshold(0.0000)? ==> 0.0114
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0114)
networkOutput[2] := neuronOutput[10](0.0114)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.0101) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.0101) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.0101) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.0101) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=116 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0101) * weight[1,1](0.1000) => 0.1010
activation[2] = self(1.0101) * weight[2,2](0.1000) => 0.1010
activation[3] = self(-0.0287) * weight[3,3](0.1000) => -0.0029
activation[4] = self(-0.0287) * weight[4,4](0.1000) => -0.0029
activation[5] = self(-0.0287) * weight[5,5](0.1000) => -0.0029
activation[6] = self(0.0114) * weight[6,6](0.1000) => 0.0011
activation[7] = self(0.0114) * weight[7,7](0.1000) => 0.0011
activation[8] = self(0.0114) * weight[8,8](0.1000) => 0.0011
activation[9] = self(0.0114) * weight[9,9](0.1000) => 0.0011
activation[10] = self(0.0114) * weight[10,10](0.1000) => 0.0011
  b. Update inputs from other neurons
activation[1] = self(0.1010) + network_input[1]=1.0000 => 1.1010
activation[2] = self(0.1010) + network_input[2]=1.0000 => 1.1010
activation[3] = self(-0.0029) + output[1](1.0101) * 1.0000 => 1.0072
activation[3] = self(1.0072) + output[7](0.0114) * -0.5000 => 1.0015
activation[3] = self(1.0015) + output[8](0.0114) * -0.5000 => 0.9958
activation[4] = self(-0.0029) + output[1](1.0101) * 0.5000 => 0.5022
activation[4] = self(0.5022) + output[2](1.0101) * 0.5000 => 1.0072
activation[4] = self(1.0072) + output[6](0.0114) * -0.5000 => 1.0015
activation[4] = self(1.0015) + output[8](0.0114) * -0.5000 => 0.9958
activation[5] = self(-0.0029) + output[2](1.0101) * 1.0000 => 1.0072
activation[5] = self(1.0072) + output[6](0.0114) * -0.5000 => 1.0015
activation[5] = self(1.0015) + output[7](0.0114) * -0.5000 => 0.9958
activation[6] = self(0.0011) + output[3](0.0000) * 1.0000 => 0.0011
activation[7] = self(0.0011) + output[4](0.0000) * 1.0000 => 0.0011
activation[8] = self(0.0011) + output[5](0.0000) * 1.0000 => 0.0011
activation[9] = self(0.0011) + output[3](0.0000) * 0.7500 => 0.0011
activation[9] = self(0.0011) + output[4](0.0000) * 0.2500 => 0.0011
activation[10] = self(0.0011) + output[4](0.0000) * 0.2500 => 0.0011
activation[10] = self(0.0011) + output[5](0.0000) * 0.7500 => 0.0011
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.1010) > threshold(0.0000)? ==> 1.1010
neuronOutput[2]: activation(1.1010) > threshold(0.0000)? ==> 1.1010
neuronOutput[3]: activation(0.9958) > threshold(0.0000)? ==> 0.9958
neuronOutput[4]: activation(0.9958) > threshold(0.0000)? ==> 0.9958
neuronOutput[5]: activation(0.9958) > threshold(0.0000)? ==> 0.9958
neuronOutput[6]: activation(0.0011) > threshold(0.0000)? ==> 0.0011
neuronOutput[7]: activation(0.0011) > threshold(0.0000)? ==> 0.0011
neuronOutput[8]: activation(0.0011) > threshold(0.0000)? ==> 0.0011
neuronOutput[9]: activation(0.0011) > threshold(0.0000)? ==> 0.0011
neuronOutput[10]: activation(0.0011) > threshold(0.0000)? ==> 0.0011
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0011)
networkOutput[2] := neuronOutput[10](0.0011)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.1010) * output[3](0.9958) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.9958) * output[3](0.9958) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.1010) * output[4](0.9958) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.1010) * output[4](0.9958) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.9958) * output[4](0.9958) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.1010) * output[5](0.9958) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.9958) * output[5](0.9958) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=117 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.1010) * weight[1,1](0.1000) => 0.1101
activation[2] = self(1.1010) * weight[2,2](0.1000) => 0.1101
activation[3] = self(0.9958) * weight[3,3](0.1000) => 0.0996
activation[4] = self(0.9958) * weight[4,4](0.1000) => 0.0996
activation[5] = self(0.9958) * weight[5,5](0.1000) => 0.0996
activation[6] = self(0.0011) * weight[6,6](0.1000) => 0.0001
activation[7] = self(0.0011) * weight[7,7](0.1000) => 0.0001
activation[8] = self(0.0011) * weight[8,8](0.1000) => 0.0001
activation[9] = self(0.0011) * weight[9,9](0.1000) => 0.0001
activation[10] = self(0.0011) * weight[10,10](0.1000) => 0.0001
  b. Update inputs from other neurons
activation[1] = self(0.1101) + network_input[1]=1.0000 => 1.1101
activation[2] = self(0.1101) + network_input[2]=1.0000 => 1.1101
activation[3] = self(0.0996) + output[1](1.1010) * 1.0000 => 1.2006
activation[3] = self(1.2006) + output[7](0.0011) * -0.5000 => 1.2000
activation[3] = self(1.2000) + output[8](0.0011) * -0.5000 => 1.1994
activation[4] = self(0.0996) + output[1](1.1010) * 0.5000 => 0.6501
activation[4] = self(0.6501) + output[2](1.1010) * 0.5000 => 1.2006
activation[4] = self(1.2006) + output[6](0.0011) * -0.5000 => 1.2000
activation[4] = self(1.2000) + output[8](0.0011) * -0.5000 => 1.1994
activation[5] = self(0.0996) + output[2](1.1010) * 1.0000 => 1.2006
activation[5] = self(1.2006) + output[6](0.0011) * -0.5000 => 1.2000
activation[5] = self(1.2000) + output[7](0.0011) * -0.5000 => 1.1994
activation[6] = self(0.0001) + output[3](0.9958) * 1.0000 => 0.9959
activation[7] = self(0.0001) + output[4](0.9958) * 1.0000 => 0.9959
activation[8] = self(0.0001) + output[5](0.9958) * 1.0000 => 0.9959
activation[9] = self(0.0001) + output[3](0.9958) * 0.7500 => 0.7470
activation[9] = self(0.7470) + output[4](0.9958) * 0.2500 => 0.9959
activation[10] = self(0.0001) + output[4](0.9958) * 0.2500 => 0.2491
activation[10] = self(0.2491) + output[5](0.9958) * 0.7500 => 0.9959
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.1101) > threshold(0.0000)? ==> 1.1101
neuronOutput[2]: activation(1.1101) > threshold(0.0000)? ==> 1.1101
neuronOutput[3]: activation(1.1994) > threshold(0.0000)? ==> 1.1994
neuronOutput[4]: activation(1.1994) > threshold(0.0000)? ==> 1.1994
neuronOutput[5]: activation(1.1994) > threshold(0.0000)? ==> 1.1994
neuronOutput[6]: activation(0.9959) > threshold(0.0000)? ==> 0.9959
neuronOutput[7]: activation(0.9959) > threshold(0.0000)? ==> 0.9959
neuronOutput[8]: activation(0.9959) > threshold(0.0000)? ==> 0.9959
neuronOutput[9]: activation(0.9959) > threshold(0.0000)? ==> 0.9959
neuronOutput[10]: activation(0.9959) > threshold(0.0000)? ==> 0.9959
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.9959)
networkOutput[2] := neuronOutput[10](0.9959)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.1101) * output[3](1.1994) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](1.1994) * output[3](1.1994) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.1101) * output[4](1.1994) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.1101) * output[4](1.1994) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](1.1994) * output[4](1.1994) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.1101) * output[5](1.1994) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](1.1994) * output[5](1.1994) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=118 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.1101) * weight[1,1](0.1000) => 0.1110
activation[2] = self(1.1101) * weight[2,2](0.1000) => 0.1110
activation[3] = self(1.1994) * weight[3,3](0.1000) => 0.1199
activation[4] = self(1.1994) * weight[4,4](0.1000) => 0.1199
activation[5] = self(1.1994) * weight[5,5](0.1000) => 0.1199
activation[6] = self(0.9959) * weight[6,6](0.1000) => 0.0996
activation[7] = self(0.9959) * weight[7,7](0.1000) => 0.0996
activation[8] = self(0.9959) * weight[8,8](0.1000) => 0.0996
activation[9] = self(0.9959) * weight[9,9](0.1000) => 0.0996
activation[10] = self(0.9959) * weight[10,10](0.1000) => 0.0996
  b. Update inputs from other neurons
activation[1] = self(0.1110) + network_input[1]=1.0000 => 1.1110
activation[2] = self(0.1110) + network_input[2]=1.0000 => 1.1110
activation[3] = self(0.1199) + output[1](1.1101) * 1.0000 => 1.2300
activation[3] = self(1.2300) + output[7](0.9959) * -0.5000 => 0.7321
activation[3] = self(0.7321) + output[8](0.9959) * -0.5000 => 0.2341
activation[4] = self(0.1199) + output[1](1.1101) * 0.5000 => 0.6750
activation[4] = self(0.6750) + output[2](1.1101) * 0.5000 => 1.2300
activation[4] = self(1.2300) + output[6](0.9959) * -0.5000 => 0.7321
activation[4] = self(0.7321) + output[8](0.9959) * -0.5000 => 0.2341
activation[5] = self(0.1199) + output[2](1.1101) * 1.0000 => 1.2300
activation[5] = self(1.2300) + output[6](0.9959) * -0.5000 => 0.7321
activation[5] = self(0.7321) + output[7](0.9959) * -0.5000 => 0.2341
activation[6] = self(0.0996) + output[3](1.1994) * 1.0000 => 1.2990
activation[7] = self(0.0996) + output[4](1.1994) * 1.0000 => 1.2990
activation[8] = self(0.0996) + output[5](1.1994) * 1.0000 => 1.2990
activation[9] = self(0.0996) + output[3](1.1994) * 0.7500 => 0.9992
activation[9] = self(0.9992) + output[4](1.1994) * 0.2500 => 1.2990
activation[10] = self(0.0996) + output[4](1.1994) * 0.2500 => 0.3995
activation[10] = self(0.3995) + output[5](1.1994) * 0.7500 => 1.2990
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.1110) > threshold(0.0000)? ==> 1.1110
neuronOutput[2]: activation(1.1110) > threshold(0.0000)? ==> 1.1110
neuronOutput[3]: activation(0.2341) > threshold(0.0000)? ==> 0.2341
neuronOutput[4]: activation(0.2341) > threshold(0.0000)? ==> 0.2341
neuronOutput[5]: activation(0.2341) > threshold(0.0000)? ==> 0.2341
neuronOutput[6]: activation(1.2990) > threshold(0.0000)? ==> 1.2990
neuronOutput[7]: activation(1.2990) > threshold(0.0000)? ==> 1.2990
neuronOutput[8]: activation(1.2990) > threshold(0.0000)? ==> 1.2990
neuronOutput[9]: activation(1.2990) > threshold(0.0000)? ==> 1.2990
neuronOutput[10]: activation(1.2990) > threshold(0.0000)? ==> 1.2990
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](1.2990)
networkOutput[2] := neuronOutput[10](1.2990)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.1110) * output[3](0.2341) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.2341) * output[3](0.2341) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.1110) * output[4](0.2341) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.1110) * output[4](0.2341) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.2341) * output[4](0.2341) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.1110) * output[5](0.2341) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.2341) * output[5](0.2341) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=119 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.1110) * weight[1,1](0.1000) => 0.1111
activation[2] = self(1.1110) * weight[2,2](0.1000) => 0.1111
activation[3] = self(0.2341) * weight[3,3](0.1000) => 0.0234
activation[4] = self(0.2341) * weight[4,4](0.1000) => 0.0234
activation[5] = self(0.2341) * weight[5,5](0.1000) => 0.0234
activation[6] = self(1.2990) * weight[6,6](0.1000) => 0.1299
activation[7] = self(1.2990) * weight[7,7](0.1000) => 0.1299
activation[8] = self(1.2990) * weight[8,8](0.1000) => 0.1299
activation[9] = self(1.2990) * weight[9,9](0.1000) => 0.1299
activation[10] = self(1.2990) * weight[10,10](0.1000) => 0.1299
  b. Update inputs from other neurons
activation[1] = self(0.1111) + network_input[1]=1.0000 => 1.1111
activation[2] = self(0.1111) + network_input[2]=1.0000 => 1.1111
activation[3] = self(0.0234) + output[1](1.1110) * 1.0000 => 1.1344
activation[3] = self(1.1344) + output[7](1.2990) * -0.5000 => 0.4849
activation[3] = self(0.4849) + output[8](1.2990) * -0.5000 => -0.1646
activation[4] = self(0.0234) + output[1](1.1110) * 0.5000 => 0.5789
activation[4] = self(0.5789) + output[2](1.1110) * 0.5000 => 1.1344
activation[4] = self(1.1344) + output[6](1.2990) * -0.5000 => 0.4849
activation[4] = self(0.4849) + output[8](1.2990) * -0.5000 => -0.1646
activation[5] = self(0.0234) + output[2](1.1110) * 1.0000 => 1.1344
activation[5] = self(1.1344) + output[6](1.2990) * -0.5000 => 0.4849
activation[5] = self(0.4849) + output[7](1.2990) * -0.5000 => -0.1646
activation[6] = self(0.1299) + output[3](0.2341) * 1.0000 => 0.3640
activation[7] = self(0.1299) + output[4](0.2341) * 1.0000 => 0.3640
activation[8] = self(0.1299) + output[5](0.2341) * 1.0000 => 0.3640
activation[9] = self(0.1299) + output[3](0.2341) * 0.7500 => 0.3055
activation[9] = self(0.3055) + output[4](0.2341) * 0.2500 => 0.3640
activation[10] = self(0.1299) + output[4](0.2341) * 0.2500 => 0.1884
activation[10] = self(0.1884) + output[5](0.2341) * 0.7500 => 0.3640
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.1111) > threshold(0.0000)? ==> 1.1111
neuronOutput[2]: activation(1.1111) > threshold(0.0000)? ==> 1.1111
neuronOutput[3]: activation(-0.1646) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-0.1646) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-0.1646) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.3640) > threshold(0.0000)? ==> 0.3640
neuronOutput[7]: activation(0.3640) > threshold(0.0000)? ==> 0.3640
neuronOutput[8]: activation(0.3640) > threshold(0.0000)? ==> 0.3640
neuronOutput[9]: activation(0.3640) > threshold(0.0000)? ==> 0.3640
neuronOutput[10]: activation(0.3640) > threshold(0.0000)? ==> 0.3640
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.3640)
networkOutput[2] := neuronOutput[10](0.3640)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.1111) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.1111) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.1111) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.1111) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=120 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.1111) * weight[1,1](0.1000) => 0.1111
activation[2] = self(1.1111) * weight[2,2](0.1000) => 0.1111
activation[3] = self(-0.1646) * weight[3,3](0.1000) => -0.0165
activation[4] = self(-0.1646) * weight[4,4](0.1000) => -0.0165
activation[5] = self(-0.1646) * weight[5,5](0.1000) => -0.0165
activation[6] = self(0.3640) * weight[6,6](0.1000) => 0.0364
activation[7] = self(0.3640) * weight[7,7](0.1000) => 0.0364
activation[8] = self(0.3640) * weight[8,8](0.1000) => 0.0364
activation[9] = self(0.3640) * weight[9,9](0.1000) => 0.0364
activation[10] = self(0.3640) * weight[10,10](0.1000) => 0.0364
  b. Update inputs from other neurons
activation[1] = self(0.1111) + network_input[1]=1.0000 => 1.1111
activation[2] = self(0.1111) + network_input[2]=1.0000 => 1.1111
activation[3] = self(-0.0165) + output[1](1.1111) * 1.0000 => 1.0946
activation[3] = self(1.0946) + output[7](0.3640) * -0.5000 => 0.9126
activation[3] = self(0.9126) + output[8](0.3640) * -0.5000 => 0.7306
activation[4] = self(-0.0165) + output[1](1.1111) * 0.5000 => 0.5391
activation[4] = self(0.5391) + output[2](1.1111) * 0.5000 => 1.0946
activation[4] = self(1.0946) + output[6](0.3640) * -0.5000 => 0.9126
activation[4] = self(0.9126) + output[8](0.3640) * -0.5000 => 0.7306
activation[5] = self(-0.0165) + output[2](1.1111) * 1.0000 => 1.0946
activation[5] = self(1.0946) + output[6](0.3640) * -0.5000 => 0.9126
activation[5] = self(0.9126) + output[7](0.3640) * -0.5000 => 0.7306
activation[6] = self(0.0364) + output[3](0.0000) * 1.0000 => 0.0364
activation[7] = self(0.0364) + output[4](0.0000) * 1.0000 => 0.0364
activation[8] = self(0.0364) + output[5](0.0000) * 1.0000 => 0.0364
activation[9] = self(0.0364) + output[3](0.0000) * 0.7500 => 0.0364
activation[9] = self(0.0364) + output[4](0.0000) * 0.2500 => 0.0364
activation[10] = self(0.0364) + output[4](0.0000) * 0.2500 => 0.0364
activation[10] = self(0.0364) + output[5](0.0000) * 0.7500 => 0.0364
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.1111) > threshold(0.0000)? ==> 1.1111
neuronOutput[2]: activation(1.1111) > threshold(0.0000)? ==> 1.1111
neuronOutput[3]: activation(0.7306) > threshold(0.0000)? ==> 0.7306
neuronOutput[4]: activation(0.7306) > threshold(0.0000)? ==> 0.7306
neuronOutput[5]: activation(0.7306) > threshold(0.0000)? ==> 0.7306
neuronOutput[6]: activation(0.0364) > threshold(0.0000)? ==> 0.0364
neuronOutput[7]: activation(0.0364) > threshold(0.0000)? ==> 0.0364
neuronOutput[8]: activation(0.0364) > threshold(0.0000)? ==> 0.0364
neuronOutput[9]: activation(0.0364) > threshold(0.0000)? ==> 0.0364
neuronOutput[10]: activation(0.0364) > threshold(0.0000)? ==> 0.0364
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0364)
networkOutput[2] := neuronOutput[10](0.0364)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.1111) * output[3](0.7306) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.7306) * output[3](0.7306) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.1111) * output[4](0.7306) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.1111) * output[4](0.7306) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.7306) * output[4](0.7306) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.1111) * output[5](0.7306) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.7306) * output[5](0.7306) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=121 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.1111) * weight[1,1](0.1000) => 0.1111
activation[2] = self(1.1111) * weight[2,2](0.1000) => 0.1111
activation[3] = self(0.7306) * weight[3,3](0.1000) => 0.0731
activation[4] = self(0.7306) * weight[4,4](0.1000) => 0.0731
activation[5] = self(0.7306) * weight[5,5](0.1000) => 0.0731
activation[6] = self(0.0364) * weight[6,6](0.1000) => 0.0036
activation[7] = self(0.0364) * weight[7,7](0.1000) => 0.0036
activation[8] = self(0.0364) * weight[8,8](0.1000) => 0.0036
activation[9] = self(0.0364) * weight[9,9](0.1000) => 0.0036
activation[10] = self(0.0364) * weight[10,10](0.1000) => 0.0036
  b. Update inputs from other neurons
activation[1] = self(0.1111) + network_input[1]=1.0000 => 1.1111
activation[2] = self(0.1111) + network_input[2]=1.0000 => 1.1111
activation[3] = self(0.0731) + output[1](1.1111) * 1.0000 => 1.1842
activation[3] = self(1.1842) + output[7](0.0364) * -0.5000 => 1.1660
activation[3] = self(1.1660) + output[8](0.0364) * -0.5000 => 1.1478
activation[4] = self(0.0731) + output[1](1.1111) * 0.5000 => 0.6286
activation[4] = self(0.6286) + output[2](1.1111) * 0.5000 => 1.1842
activation[4] = self(1.1842) + output[6](0.0364) * -0.5000 => 1.1660
activation[4] = self(1.1660) + output[8](0.0364) * -0.5000 => 1.1478
activation[5] = self(0.0731) + output[2](1.1111) * 1.0000 => 1.1842
activation[5] = self(1.1842) + output[6](0.0364) * -0.5000 => 1.1660
activation[5] = self(1.1660) + output[7](0.0364) * -0.5000 => 1.1478
activation[6] = self(0.0036) + output[3](0.7306) * 1.0000 => 0.7342
activation[7] = self(0.0036) + output[4](0.7306) * 1.0000 => 0.7342
activation[8] = self(0.0036) + output[5](0.7306) * 1.0000 => 0.7342
activation[9] = self(0.0036) + output[3](0.7306) * 0.7500 => 0.5516
activation[9] = self(0.5516) + output[4](0.7306) * 0.2500 => 0.7342
activation[10] = self(0.0036) + output[4](0.7306) * 0.2500 => 0.1863
activation[10] = self(0.1863) + output[5](0.7306) * 0.7500 => 0.7342
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.1111) > threshold(0.0000)? ==> 1.1111
neuronOutput[2]: activation(1.1111) > threshold(0.0000)? ==> 1.1111
neuronOutput[3]: activation(1.1478) > threshold(0.0000)? ==> 1.1478
neuronOutput[4]: activation(1.1478) > threshold(0.0000)? ==> 1.1478
neuronOutput[5]: activation(1.1478) > threshold(0.0000)? ==> 1.1478
neuronOutput[6]: activation(0.7342) > threshold(0.0000)? ==> 0.7342
neuronOutput[7]: activation(0.7342) > threshold(0.0000)? ==> 0.7342
neuronOutput[8]: activation(0.7342) > threshold(0.0000)? ==> 0.7342
neuronOutput[9]: activation(0.7342) > threshold(0.0000)? ==> 0.7342
neuronOutput[10]: activation(0.7342) > threshold(0.0000)? ==> 0.7342
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.7342)
networkOutput[2] := neuronOutput[10](0.7342)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.1111) * output[3](1.1478) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](1.1478) * output[3](1.1478) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.1111) * output[4](1.1478) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.1111) * output[4](1.1478) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](1.1478) * output[4](1.1478) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.1111) * output[5](1.1478) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](1.1478) * output[5](1.1478) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=122 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.1111) * weight[1,1](0.1000) => 0.1111
activation[2] = self(1.1111) * weight[2,2](0.1000) => 0.1111
activation[3] = self(1.1478) * weight[3,3](0.1000) => 0.1148
activation[4] = self(1.1478) * weight[4,4](0.1000) => 0.1148
activation[5] = self(1.1478) * weight[5,5](0.1000) => 0.1148
activation[6] = self(0.7342) * weight[6,6](0.1000) => 0.0734
activation[7] = self(0.7342) * weight[7,7](0.1000) => 0.0734
activation[8] = self(0.7342) * weight[8,8](0.1000) => 0.0734
activation[9] = self(0.7342) * weight[9,9](0.1000) => 0.0734
activation[10] = self(0.7342) * weight[10,10](0.1000) => 0.0734
  b. Update inputs from other neurons
activation[1] = self(0.1111) + network_input[1]=0.0000 => 0.1111
activation[2] = self(0.1111) + network_input[2]=0.0000 => 0.1111
activation[3] = self(0.1148) + output[1](1.1111) * 1.0000 => 1.2259
activation[3] = self(1.2259) + output[7](0.7342) * -0.5000 => 0.8588
activation[3] = self(0.8588) + output[8](0.7342) * -0.5000 => 0.4916
activation[4] = self(0.1148) + output[1](1.1111) * 0.5000 => 0.6703
activation[4] = self(0.6703) + output[2](1.1111) * 0.5000 => 1.2259
activation[4] = self(1.2259) + output[6](0.7342) * -0.5000 => 0.8588
activation[4] = self(0.8588) + output[8](0.7342) * -0.5000 => 0.4916
activation[5] = self(0.1148) + output[2](1.1111) * 1.0000 => 1.2259
activation[5] = self(1.2259) + output[6](0.7342) * -0.5000 => 0.8588
activation[5] = self(0.8588) + output[7](0.7342) * -0.5000 => 0.4916
activation[6] = self(0.0734) + output[3](1.1478) * 1.0000 => 1.2212
activation[7] = self(0.0734) + output[4](1.1478) * 1.0000 => 1.2212
activation[8] = self(0.0734) + output[5](1.1478) * 1.0000 => 1.2212
activation[9] = self(0.0734) + output[3](1.1478) * 0.7500 => 0.9343
activation[9] = self(0.9343) + output[4](1.1478) * 0.2500 => 1.2212
activation[10] = self(0.0734) + output[4](1.1478) * 0.2500 => 0.3604
activation[10] = self(0.3604) + output[5](1.1478) * 0.7500 => 1.2212
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.1111) > threshold(0.0000)? ==> 0.1111
neuronOutput[2]: activation(0.1111) > threshold(0.0000)? ==> 0.1111
neuronOutput[3]: activation(0.4916) > threshold(0.0000)? ==> 0.4916
neuronOutput[4]: activation(0.4916) > threshold(0.0000)? ==> 0.4916
neuronOutput[5]: activation(0.4916) > threshold(0.0000)? ==> 0.4916
neuronOutput[6]: activation(1.2212) > threshold(0.0000)? ==> 1.2212
neuronOutput[7]: activation(1.2212) > threshold(0.0000)? ==> 1.2212
neuronOutput[8]: activation(1.2212) > threshold(0.0000)? ==> 1.2212
neuronOutput[9]: activation(1.2212) > threshold(0.0000)? ==> 1.2212
neuronOutput[10]: activation(1.2212) > threshold(0.0000)? ==> 1.2212
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](1.2212)
networkOutput[2] := neuronOutput[10](1.2212)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.1111) * output[3](0.4916) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.4916) * output[3](0.4916) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.1111) * output[4](0.4916) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.1111) * output[4](0.4916) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.4916) * output[4](0.4916) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.1111) * output[5](0.4916) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.4916) * output[5](0.4916) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=123 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.1111) * weight[1,1](0.1000) => 0.0111
activation[2] = self(0.1111) * weight[2,2](0.1000) => 0.0111
activation[3] = self(0.4916) * weight[3,3](0.1000) => 0.0492
activation[4] = self(0.4916) * weight[4,4](0.1000) => 0.0492
activation[5] = self(0.4916) * weight[5,5](0.1000) => 0.0492
activation[6] = self(1.2212) * weight[6,6](0.1000) => 0.1221
activation[7] = self(1.2212) * weight[7,7](0.1000) => 0.1221
activation[8] = self(1.2212) * weight[8,8](0.1000) => 0.1221
activation[9] = self(1.2212) * weight[9,9](0.1000) => 0.1221
activation[10] = self(1.2212) * weight[10,10](0.1000) => 0.1221
  b. Update inputs from other neurons
activation[1] = self(0.0111) + network_input[1]=1.0000 => 1.0111
activation[2] = self(0.0111) + network_input[2]=1.0000 => 1.0111
activation[3] = self(0.0492) + output[1](0.1111) * 1.0000 => 0.1603
activation[3] = self(0.1603) + output[7](1.2212) * -0.5000 => -0.4503
activation[3] = self(-0.4503) + output[8](1.2212) * -0.5000 => -1.0609
activation[4] = self(0.0492) + output[1](0.1111) * 0.5000 => 0.1047
activation[4] = self(0.1047) + output[2](0.1111) * 0.5000 => 0.1603
activation[4] = self(0.1603) + output[6](1.2212) * -0.5000 => -0.4503
activation[4] = self(-0.4503) + output[8](1.2212) * -0.5000 => -1.0609
activation[5] = self(0.0492) + output[2](0.1111) * 1.0000 => 0.1603
activation[5] = self(0.1603) + output[6](1.2212) * -0.5000 => -0.4503
activation[5] = self(-0.4503) + output[7](1.2212) * -0.5000 => -1.0609
activation[6] = self(0.1221) + output[3](0.4916) * 1.0000 => 0.6138
activation[7] = self(0.1221) + output[4](0.4916) * 1.0000 => 0.6138
activation[8] = self(0.1221) + output[5](0.4916) * 1.0000 => 0.6138
activation[9] = self(0.1221) + output[3](0.4916) * 0.7500 => 0.4908
activation[9] = self(0.4908) + output[4](0.4916) * 0.2500 => 0.6138
activation[10] = self(0.1221) + output[4](0.4916) * 0.2500 => 0.2450
activation[10] = self(0.2450) + output[5](0.4916) * 0.7500 => 0.6138
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0111) > threshold(0.0000)? ==> 1.0111
neuronOutput[2]: activation(1.0111) > threshold(0.0000)? ==> 1.0111
neuronOutput[3]: activation(-1.0609) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-1.0609) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-1.0609) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.6138) > threshold(0.0000)? ==> 0.6138
neuronOutput[7]: activation(0.6138) > threshold(0.0000)? ==> 0.6138
neuronOutput[8]: activation(0.6138) > threshold(0.0000)? ==> 0.6138
neuronOutput[9]: activation(0.6138) > threshold(0.0000)? ==> 0.6138
neuronOutput[10]: activation(0.6138) > threshold(0.0000)? ==> 0.6138
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.6138)
networkOutput[2] := neuronOutput[10](0.6138)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.0111) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.0111) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.0111) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.0111) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=124 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(1.0111) * weight[1,1](0.1000) => 0.1011
activation[2] = self(1.0111) * weight[2,2](0.1000) => 0.1011
activation[3] = self(-1.0609) * weight[3,3](0.1000) => -0.1061
activation[4] = self(-1.0609) * weight[4,4](0.1000) => -0.1061
activation[5] = self(-1.0609) * weight[5,5](0.1000) => -0.1061
activation[6] = self(0.6138) * weight[6,6](0.1000) => 0.0614
activation[7] = self(0.6138) * weight[7,7](0.1000) => 0.0614
activation[8] = self(0.6138) * weight[8,8](0.1000) => 0.0614
activation[9] = self(0.6138) * weight[9,9](0.1000) => 0.0614
activation[10] = self(0.6138) * weight[10,10](0.1000) => 0.0614
  b. Update inputs from other neurons
activation[1] = self(0.1011) + network_input[1]=0.0000 => 0.1011
activation[2] = self(0.1011) + network_input[2]=0.0000 => 0.1011
activation[3] = self(-0.1061) + output[1](1.0111) * 1.0000 => 0.9050
activation[3] = self(0.9050) + output[7](0.6138) * -0.5000 => 0.5981
activation[3] = self(0.5981) + output[8](0.6138) * -0.5000 => 0.2913
activation[4] = self(-0.1061) + output[1](1.0111) * 0.5000 => 0.3995
activation[4] = self(0.3995) + output[2](1.0111) * 0.5000 => 0.9050
activation[4] = self(0.9050) + output[6](0.6138) * -0.5000 => 0.5981
activation[4] = self(0.5981) + output[8](0.6138) * -0.5000 => 0.2913
activation[5] = self(-0.1061) + output[2](1.0111) * 1.0000 => 0.9050
activation[5] = self(0.9050) + output[6](0.6138) * -0.5000 => 0.5981
activation[5] = self(0.5981) + output[7](0.6138) * -0.5000 => 0.2913
activation[6] = self(0.0614) + output[3](0.0000) * 1.0000 => 0.0614
activation[7] = self(0.0614) + output[4](0.0000) * 1.0000 => 0.0614
activation[8] = self(0.0614) + output[5](0.0000) * 1.0000 => 0.0614
activation[9] = self(0.0614) + output[3](0.0000) * 0.7500 => 0.0614
activation[9] = self(0.0614) + output[4](0.0000) * 0.2500 => 0.0614
activation[10] = self(0.0614) + output[4](0.0000) * 0.2500 => 0.0614
activation[10] = self(0.0614) + output[5](0.0000) * 0.7500 => 0.0614
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.1011) > threshold(0.0000)? ==> 0.1011
neuronOutput[2]: activation(0.1011) > threshold(0.0000)? ==> 0.1011
neuronOutput[3]: activation(0.2913) > threshold(0.0000)? ==> 0.2913
neuronOutput[4]: activation(0.2913) > threshold(0.0000)? ==> 0.2913
neuronOutput[5]: activation(0.2913) > threshold(0.0000)? ==> 0.2913
neuronOutput[6]: activation(0.0614) > threshold(0.0000)? ==> 0.0614
neuronOutput[7]: activation(0.0614) > threshold(0.0000)? ==> 0.0614
neuronOutput[8]: activation(0.0614) > threshold(0.0000)? ==> 0.0614
neuronOutput[9]: activation(0.0614) > threshold(0.0000)? ==> 0.0614
neuronOutput[10]: activation(0.0614) > threshold(0.0000)? ==> 0.0614
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0614)
networkOutput[2] := neuronOutput[10](0.0614)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.1011) * output[3](0.2913) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.2913) * output[3](0.2913) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.1011) * output[4](0.2913) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.1011) * output[4](0.2913) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.2913) * output[4](0.2913) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.1011) * output[5](0.2913) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.2913) * output[5](0.2913) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=125 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.1011) * weight[1,1](0.1000) => 0.0101
activation[2] = self(0.1011) * weight[2,2](0.1000) => 0.0101
activation[3] = self(0.2913) * weight[3,3](0.1000) => 0.0291
activation[4] = self(0.2913) * weight[4,4](0.1000) => 0.0291
activation[5] = self(0.2913) * weight[5,5](0.1000) => 0.0291
activation[6] = self(0.0614) * weight[6,6](0.1000) => 0.0061
activation[7] = self(0.0614) * weight[7,7](0.1000) => 0.0061
activation[8] = self(0.0614) * weight[8,8](0.1000) => 0.0061
activation[9] = self(0.0614) * weight[9,9](0.1000) => 0.0061
activation[10] = self(0.0614) * weight[10,10](0.1000) => 0.0061
  b. Update inputs from other neurons
activation[1] = self(0.0101) + network_input[1]=0.0000 => 0.0101
activation[2] = self(0.0101) + network_input[2]=0.0000 => 0.0101
activation[3] = self(0.0291) + output[1](0.1011) * 1.0000 => 0.1302
activation[3] = self(0.1302) + output[7](0.0614) * -0.5000 => 0.0995
activation[3] = self(0.0995) + output[8](0.0614) * -0.5000 => 0.0689
activation[4] = self(0.0291) + output[1](0.1011) * 0.5000 => 0.0797
activation[4] = self(0.0797) + output[2](0.1011) * 0.5000 => 0.1302
activation[4] = self(0.1302) + output[6](0.0614) * -0.5000 => 0.0995
activation[4] = self(0.0995) + output[8](0.0614) * -0.5000 => 0.0689
activation[5] = self(0.0291) + output[2](0.1011) * 1.0000 => 0.1302
activation[5] = self(0.1302) + output[6](0.0614) * -0.5000 => 0.0995
activation[5] = self(0.0995) + output[7](0.0614) * -0.5000 => 0.0689
activation[6] = self(0.0061) + output[3](0.2913) * 1.0000 => 0.2974
activation[7] = self(0.0061) + output[4](0.2913) * 1.0000 => 0.2974
activation[8] = self(0.0061) + output[5](0.2913) * 1.0000 => 0.2974
activation[9] = self(0.0061) + output[3](0.2913) * 0.7500 => 0.2246
activation[9] = self(0.2246) + output[4](0.2913) * 0.2500 => 0.2974
activation[10] = self(0.0061) + output[4](0.2913) * 0.2500 => 0.0790
activation[10] = self(0.0790) + output[5](0.2913) * 0.7500 => 0.2974
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0101) > threshold(0.0000)? ==> 0.0101
neuronOutput[2]: activation(0.0101) > threshold(0.0000)? ==> 0.0101
neuronOutput[3]: activation(0.0689) > threshold(0.0000)? ==> 0.0689
neuronOutput[4]: activation(0.0689) > threshold(0.0000)? ==> 0.0689
neuronOutput[5]: activation(0.0689) > threshold(0.0000)? ==> 0.0689
neuronOutput[6]: activation(0.2974) > threshold(0.0000)? ==> 0.2974
neuronOutput[7]: activation(0.2974) > threshold(0.0000)? ==> 0.2974
neuronOutput[8]: activation(0.2974) > threshold(0.0000)? ==> 0.2974
neuronOutput[9]: activation(0.2974) > threshold(0.0000)? ==> 0.2974
neuronOutput[10]: activation(0.2974) > threshold(0.0000)? ==> 0.2974
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.2974)
networkOutput[2] := neuronOutput[10](0.2974)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.0101) * output[3](0.0689) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0689) * output[3](0.0689) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.0101) * output[4](0.0689) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.0101) * output[4](0.0689) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0689) * output[4](0.0689) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.0101) * output[5](0.0689) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0689) * output[5](0.0689) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=126 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0101) * weight[1,1](0.1000) => 0.0010
activation[2] = self(0.0101) * weight[2,2](0.1000) => 0.0010
activation[3] = self(0.0689) * weight[3,3](0.1000) => 0.0069
activation[4] = self(0.0689) * weight[4,4](0.1000) => 0.0069
activation[5] = self(0.0689) * weight[5,5](0.1000) => 0.0069
activation[6] = self(0.2974) * weight[6,6](0.1000) => 0.0297
activation[7] = self(0.2974) * weight[7,7](0.1000) => 0.0297
activation[8] = self(0.2974) * weight[8,8](0.1000) => 0.0297
activation[9] = self(0.2974) * weight[9,9](0.1000) => 0.0297
activation[10] = self(0.2974) * weight[10,10](0.1000) => 0.0297
  b. Update inputs from other neurons
activation[1] = self(0.0010) + network_input[1]=0.0000 => 0.0010
activation[2] = self(0.0010) + network_input[2]=0.0000 => 0.0010
activation[3] = self(0.0069) + output[1](0.0101) * 1.0000 => 0.0170
activation[3] = self(0.0170) + output[7](0.2974) * -0.5000 => -0.1317
activation[3] = self(-0.1317) + output[8](0.2974) * -0.5000 => -0.2804
activation[4] = self(0.0069) + output[1](0.0101) * 0.5000 => 0.0119
activation[4] = self(0.0119) + output[2](0.0101) * 0.5000 => 0.0170
activation[4] = self(0.0170) + output[6](0.2974) * -0.5000 => -0.1317
activation[4] = self(-0.1317) + output[8](0.2974) * -0.5000 => -0.2804
activation[5] = self(0.0069) + output[2](0.0101) * 1.0000 => 0.0170
activation[5] = self(0.0170) + output[6](0.2974) * -0.5000 => -0.1317
activation[5] = self(-0.1317) + output[7](0.2974) * -0.5000 => -0.2804
activation[6] = self(0.0297) + output[3](0.0689) * 1.0000 => 0.0986
activation[7] = self(0.0297) + output[4](0.0689) * 1.0000 => 0.0986
activation[8] = self(0.0297) + output[5](0.0689) * 1.0000 => 0.0986
activation[9] = self(0.0297) + output[3](0.0689) * 0.7500 => 0.0814
activation[9] = self(0.0814) + output[4](0.0689) * 0.2500 => 0.0986
activation[10] = self(0.0297) + output[4](0.0689) * 0.2500 => 0.0470
activation[10] = self(0.0470) + output[5](0.0689) * 0.7500 => 0.0986
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(0.0010) > threshold(0.0000)? ==> 0.0010
neuronOutput[2]: activation(0.0010) > threshold(0.0000)? ==> 0.0010
neuronOutput[3]: activation(-0.2804) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-0.2804) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-0.2804) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.0986) > threshold(0.0000)? ==> 0.0986
neuronOutput[7]: activation(0.0986) > threshold(0.0000)? ==> 0.0986
neuronOutput[8]: activation(0.0986) > threshold(0.0000)? ==> 0.0986
neuronOutput[9]: activation(0.0986) > threshold(0.0000)? ==> 0.0986
neuronOutput[10]: activation(0.0986) > threshold(0.0000)? ==> 0.0986
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0986)
networkOutput[2] := neuronOutput[10](0.0986)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](0.0010) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](0.0010) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](0.0010) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](0.0010) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
-- t=127 --
***** cycleNetwork
1. networkActivation()
  a. Update autapses
activation[1] = self(0.0010) * weight[1,1](0.1000) => 0.0001
activation[2] = self(0.0010) * weight[2,2](0.1000) => 0.0001
activation[3] = self(-0.2804) * weight[3,3](0.1000) => -0.0280
activation[4] = self(-0.2804) * weight[4,4](0.1000) => -0.0280
activation[5] = self(-0.2804) * weight[5,5](0.1000) => -0.0280
activation[6] = self(0.0986) * weight[6,6](0.1000) => 0.0099
activation[7] = self(0.0986) * weight[7,7](0.1000) => 0.0099
activation[8] = self(0.0986) * weight[8,8](0.1000) => 0.0099
activation[9] = self(0.0986) * weight[9,9](0.1000) => 0.0099
activation[10] = self(0.0986) * weight[10,10](0.1000) => 0.0099
  b. Update inputs from other neurons
activation[1] = self(0.0001) + network_input[1]=1.0000 => 1.0001
activation[2] = self(0.0001) + network_input[2]=1.0000 => 1.0001
activation[3] = self(-0.0280) + output[1](0.0010) * 1.0000 => -0.0270
activation[3] = self(-0.0270) + output[7](0.0986) * -0.5000 => -0.0763
activation[3] = self(-0.0763) + output[8](0.0986) * -0.5000 => -0.1256
activation[4] = self(-0.0280) + output[1](0.0010) * 0.5000 => -0.0275
activation[4] = self(-0.0275) + output[2](0.0010) * 0.5000 => -0.0270
activation[4] = self(-0.0270) + output[6](0.0986) * -0.5000 => -0.0763
activation[4] = self(-0.0763) + output[8](0.0986) * -0.5000 => -0.1256
activation[5] = self(-0.0280) + output[2](0.0010) * 1.0000 => -0.0270
activation[5] = self(-0.0270) + output[6](0.0986) * -0.5000 => -0.0763
activation[5] = self(-0.0763) + output[7](0.0986) * -0.5000 => -0.1256
activation[6] = self(0.0099) + output[3](0.0000) * 1.0000 => 0.0099
activation[7] = self(0.0099) + output[4](0.0000) * 1.0000 => 0.0099
activation[8] = self(0.0099) + output[5](0.0000) * 1.0000 => 0.0099
activation[9] = self(0.0099) + output[3](0.0000) * 0.7500 => 0.0099
activation[9] = self(0.0099) + output[4](0.0000) * 0.2500 => 0.0099
activation[10] = self(0.0099) + output[4](0.0000) * 0.2500 => 0.0099
activation[10] = self(0.0099) + output[5](0.0000) * 0.7500 => 0.0099
2. copyNeuronActivationsToNeuronOutputs() (WARN) does nothing
3. thresholdNeuronOutputs()
neuronOutput[1]: activation(1.0001) > threshold(0.0000)? ==> 1.0001
neuronOutput[2]: activation(1.0001) > threshold(0.0000)? ==> 1.0001
neuronOutput[3]: activation(-0.1256) > threshold(0.0000)? ==> 0.0000
neuronOutput[4]: activation(-0.1256) > threshold(0.0000)? ==> 0.0000
neuronOutput[5]: activation(-0.1256) > threshold(0.0000)? ==> 0.0000
neuronOutput[6]: activation(0.0099) > threshold(0.0000)? ==> 0.0099
neuronOutput[7]: activation(0.0099) > threshold(0.0000)? ==> 0.0099
neuronOutput[8]: activation(0.0099) > threshold(0.0000)? ==> 0.0099
neuronOutput[9]: activation(0.0099) > threshold(0.0000)? ==> 0.0099
neuronOutput[10]: activation(0.0099) > threshold(0.0000)? ==> 0.0099
4. setNetworkOutput()
networkOutput[1] := neuronOutput[9](0.0099)
networkOutput[2] := neuronOutput[10](0.0099)
***** cycleNetworkNormalizeHebbianLearning()
1. hebbianExcitatoryWeightUpdate()
weight[1,3](1.0000) += learning_rate(0.2500) * output[1](1.0001) * output[3](0.0000) * mask(0)  => 1.0000
weight[3,3](0.1000) += learning_rate(0.2500) * output[3](0.0000) * output[3](0.0000) * mask(0)  => 0.1000
weight[1,4](0.5000) += learning_rate(0.2500) * output[1](1.0001) * output[4](0.0000) * mask(0)  => 0.5000
weight[2,4](0.5000) += learning_rate(0.2500) * output[2](1.0001) * output[4](0.0000) * mask(0)  => 0.5000
weight[4,4](0.1000) += learning_rate(0.2500) * output[4](0.0000) * output[4](0.0000) * mask(0)  => 0.1000
weight[2,5](1.0000) += learning_rate(0.2500) * output[2](1.0001) * output[5](0.0000) * mask(0)  => 1.0000
weight[5,5](0.1000) += learning_rate(0.2500) * output[5](0.0000) * output[5](0.0000) * mask(0)  => 0.1000
2. normalizeNonDiagonalExcitatoryNeuronWeights()
weight[1,3] = weight_total[3](1.0000) * weight[1,3](1.0000) / sum(1.0000) => 1.0000
weight[1,4] = weight_total[4](1.0000) * weight[1,4](0.5000) / sum(1.0000) => 0.5000
weight[2,4] = weight_total[4](1.0000) * weight[2,4](0.5000) / sum(1.0000) => 0.5000
weight[2,5] = weight_total[5](1.0000) * weight[2,5](1.0000) / sum(1.0000) => 1.0000
weight[3,6] = weight_total[6](1.0000) * weight[3,6](1.0000) / sum(1.0000) => 1.0000
weight[4,7] = weight_total[7](1.0000) * weight[4,7](1.0000) / sum(1.0000) => 1.0000
weight[5,8] = weight_total[8](1.0000) * weight[5,8](1.0000) / sum(1.0000) => 1.0000
weight[3,9] = weight_total[9](1.0000) * weight[3,9](0.7500) / sum(1.0000) => 0.7500
weight[4,9] = weight_total[9](1.0000) * weight[4,9](0.2500) / sum(1.0000) => 0.2500
weight[4,10] = weight_total[10](1.0000) * weight[4,10](0.2500) / sum(1.0000) => 0.2500
weight[5,10] = weight_total[10](1.0000) * weight[5,10](0.7500) / sum(1.0000) => 0.7500
